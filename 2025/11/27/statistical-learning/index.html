<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>模式识别与统计学习 | WENTAO's Blog</title><meta name="author" content="文韬"><meta name="copyright" content="文韬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第一章 统计学习方法概述 ❝ 机器学习是一门人工智能的科学, 该领域的主要研究对象是人工智能, 特别是如何在经验学习中改善具体算法的性能。 ❞ —— 机器学习是对能通过经验自动改进的计算机算法的研究。  统计学习 概念：计算机基于数据构建概率统计模型，运用模型对数据进行预测和分析。 研究对象：数据、基本假设。 目的：对数据（特别是未知数据）进行预测和分析。  🤔 为什么需要机器学习？  数据量太">
<meta property="og:type" content="article">
<meta property="og:title" content="模式识别与统计学习">
<meta property="og:url" content="http://example.com/2025/11/27/statistical-learning/index.html">
<meta property="og:site_name" content="WENTAO&#39;s Blog">
<meta property="og:description" content="第一章 统计学习方法概述 ❝ 机器学习是一门人工智能的科学, 该领域的主要研究对象是人工智能, 特别是如何在经验学习中改善具体算法的性能。 ❞ —— 机器学习是对能通过经验自动改进的计算机算法的研究。  统计学习 概念：计算机基于数据构建概率统计模型，运用模型对数据进行预测和分析。 研究对象：数据、基本假设。 目的：对数据（特别是未知数据）进行预测和分析。  🤔 为什么需要机器学习？  数据量太">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/logo.jpg">
<meta property="article:published_time" content="2025-11-27T07:03:00.000Z">
<meta property="article:modified_time" content="2025-12-04T12:15:09.306Z">
<meta property="article:author" content="文韬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/logo.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "模式识别与统计学习",
  "url": "http://example.com/2025/11/27/statistical-learning/",
  "image": "http://example.com/images/logo.jpg",
  "datePublished": "2025-11-27T07:03:00.000Z",
  "dateModified": "2025-12-04T12:15:09.306Z",
  "author": [
    {
      "@type": "Person",
      "name": "文韬",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/11/27/statistical-learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.3-b2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '模式识别与统计学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css"><style>
  /* ====== 终极字体方案 ====== */
  body, 
  #site-title, 
  #site-subtitle, 
  .post-title, 
  .page-title,
  #page-header #site-title,
  h1, h2, h3, h4, h5, h6,
  #footer,
  #nav,
  .menus_item { 
      font-family: 'LXGW WenKai Screen', sans-serif !important; 
  }

  /* 首页大标题防闪烁 */
  #page-header:not(.not-home-page) #site-title {
      font-size: 0 !important;
  }
  #page-header:not(.not-home-page) #site-title::before {
      content: "夢が叶ったの？";
      font-size: 5rem !important;
      font-family: 'LXGW WenKai Screen', sans-serif !important;
      text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
      display: block;
  }

  /* 其他样式保持不变 */
  hr { border: none !important; background: transparent !important; margin: 2rem auto !important; }
  hr::before { display: none !important; }

  #web_bg {
      position: fixed !important; top: 0; left: 0; width: 100%; height: 100%; min-height: 100vh; z-index: -999;
      background: url("/images/index.jpg") no-repeat center center / cover !important;
      background-attachment: scroll !important;
      will-change: transform; transform: translate3d(0,0,0);
  }
  
  #page-header, #page-header.not-home-page, #page-header.full_page, #page-header.post-bg {
      background-color: transparent !important; background-image: none !important;
  }
  #page-header::before, #page-header.not-home-page::before, #page-header.post-bg::before {
      background-color: rgba(0,0,0,0.25) !important; background-image: none !important; opacity: 1 !important;
  }
  
  .avatar-img img:hover { transform: none !important; }
  #footer { background: rgba(255,255,255, 0.95) !important; color: #000 !important; }
  #footer a { color: #333 !important; }
  #footer::before { background: transparent !important; }
  #local-search .search-dialog hr { display: none !important; }
</style>
<meta name="generator" content="Hexo 8.1.1"></head><body><div id="web_bg" style="background-image: url(/images/index.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/ecust/"><i class="fa-fw fas fa-university"></i><span> 华理</span></a></div><div class="menus_item"><a class="site-page" href="/categories/%E7%95%99%E5%AD%A6/"><i class="fa-fw fas fa-plane"></i><span> 留学</span></a></div><div class="menus_item"><a class="site-page" href="/categories/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-feather-alt"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">WENTAO's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">模式识别与统计学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/ecust/"><i class="fa-fw fas fa-university"></i><span> 华理</span></a></div><div class="menus_item"><a class="site-page" href="/categories/%E7%95%99%E5%AD%A6/"><i class="fa-fw fas fa-plane"></i><span> 留学</span></a></div><div class="menus_item"><a class="site-page" href="/categories/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-feather-alt"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">模式识别与统计学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-27T07:03:00.000Z" title="发表于 2025-11-27 15:03:00">2025-11-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-04T12:15:09.306Z" title="更新于 2025-12-04 20:15:09">2025-12-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8D%8E%E7%90%86/">华理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8D%8E%E7%90%86/%E5%A4%A7%E4%B8%89%E4%B8%8A/">大三上</a></span></div><div class="meta-secondline"></div></div></div><article class="container post-content" id="article-container"><h1 id="第一章-统计学习方法概述"><a href="#第一章-统计学习方法概述" class="headerlink" title="第一章 统计学习方法概述"></a>第一章 统计学习方法概述</h1><blockquote>
<p>❝ 机器学习是一门人工智能的科学, 该领域的主要研究对象是人工智能, 特别是如何在经验学习中改善具体算法的性能。 ❞</p>
<p>—— 机器学习是对能通过经验自动改进的计算机算法的研究。</p>
</blockquote>
<h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><ul>
<li><strong>概念</strong>：计算机基于数据构建概率统计<strong>模型</strong>，运用<strong>模型</strong>对数据进行预测和分析。</li>
<li><strong>研究对象</strong>：数据、基本假设。</li>
<li><strong>目的</strong>：对数据（特别是未知数据）进行预测和分析。</li>
</ul>
<p><strong>🤔 为什么需要机器学习？</strong></p>
<ol>
<li><strong>数据量太大</strong>：人脑无法处理海量维度。</li>
<li><strong>知识盲区</strong>：人类无法解释专业知识，或专业知识本身不存在。</li>
</ol>
<hr>
<h3 id="发展历程与三要素"><a href="#发展历程与三要素" class="headerlink" title="发展历程与三要素"></a>发展历程与三要素</h3><p>模式识别与机器学习发展历程：<br><img src="2025-11-27-15-37-49.png" alt=""></p>
<h3 id="核心：统计学三要素"><a href="#核心：统计学三要素" class="headerlink" title="核心：统计学三要素"></a>核心：统计学三要素</h3><script type="math/tex; mode=display">\text{方法} = \text{模型} + \text{策略} + \text{算法}</script><p><strong>1. 模型 (Model)</strong></p>
<ul>
<li><strong>非概率模型</strong>：<ul>
<li>假设空间：决策函数 $F=\{f \mid Y=f(X)\}$</li>
<li>参数空间：$F=\{f \mid Y=f_\theta(X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
<li><strong>概率模型</strong>：<ul>
<li>假设空间：条件概率 $F=\{P \mid P(Y|X)\}$</li>
<li>参数空间：$F=\{P \mid P_\theta(Y|X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
</ul>
<p><strong>2. 策略 (Strategy)</strong></p>
<ul>
<li><strong>损失函数</strong>：度量<strong>一次</strong>预测的好坏。</li>
<li><strong>风险函数</strong>：度量平均意义下的好坏（损失函数的期望）。<script type="math/tex; mode=display">R(f)=E_P[L(Y,f(X))]=\int L(y,f(x))P(x,y)dxdy</script></li>
</ul>
<p><strong>3. 算法 (Algorithm)</strong></p>
<ul>
<li>求解得到最优模型（通常指最优化问题的求解算法）。</li>
</ul>
<hr>
<h3 id="分类体系"><a href="#分类体系" class="headerlink" title="分类体系"></a>分类体系</h3><ul>
<li><strong>学习分类</strong>：监督学习、无监督学习、强化学习 （具体概念）</li>
<li><strong>模型分类</strong>：(非)概率、(非)线性、(非)参数、生成/判别</li>
</ul>
<hr>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><strong>⚡️ 快速记忆口诀：</strong></p>
<blockquote>
<p><span style="background-color:#fff5f5; color:#ff5050; padding:2px 6px; border-radius:4px;">先看预测，再看结果</span></p>
<ul>
<li>预测为<strong>真</strong> $\rightarrow$ <strong>P</strong> (Positive)；预测为<strong>假</strong> $\rightarrow$ <strong>N</strong> (Negative)</li>
<li>预测<strong>正确</strong> $\rightarrow$ <strong>T</strong> (True)；预测<strong>错误</strong> $\rightarrow$ <strong>F</strong> (False)</li>
</ul>
</blockquote>
<p><strong>核心指标：</strong></p>
<ul>
<li><strong>精确率 (Precision)</strong>：$P=\frac{TP}{TP+FP}$ <em>（你认为是对的里面，有多少真是对的？）</em></li>
<li><strong>召回率 (Recall)</strong>：$R=\frac{TP}{TP+FN}$ <em>（把精确率的 FP 换成 FN）</em></li>
<li><strong>F1值</strong>：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}$</li>
</ul>
<hr>
<h2 id="过拟合-Overfitting"><a href="#过拟合-Overfitting" class="headerlink" title="过拟合 (Overfitting)"></a>过拟合 (Overfitting)</h2><p>在<strong>训练数据</strong>上表现很好（把噪声也学进去了），对于<strong>未知数据</strong>表现很差。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>正则化</li>
<li>增大数据集</li>
<li>早停机制</li>
<li>减少模型参数</li>
</ul>
<h2 id="欠拟合-Underfitting"><a href="#欠拟合-Underfitting" class="headerlink" title="欠拟合 (Underfitting)"></a>欠拟合 (Underfitting)</h2><p><strong>模型过于简单</strong>，未能学习到数据中的基本规律，导致在<strong>训练集和未知数据</strong>上表现均不理想。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>增加模型复杂度</li>
<li>增加特征数量</li>
<li>减少或移除正则化参数</li>
<li>延长训练时间/充分训练  </li>
</ul>
<h1 id="第二章-线性感知机"><a href="#第二章-线性感知机" class="headerlink" title="第二章 线性感知机"></a>第二章 线性感知机</h1><h2 id="1-超平面-Hyperplane"><a href="#1-超平面-Hyperplane" class="headerlink" title="1. 超平面 (Hyperplane)"></a>1. 超平面 (Hyperplane)</h2><h3 id="怎么理解？"><a href="#怎么理解？" class="headerlink" title="怎么理解？"></a>怎么理解？</h3><blockquote>
<p><strong>定义</strong>：在 $n$ 维空间中，将空间一分为二的 $n-1$ 维子空间。</p>
</blockquote>
<ul>
<li><strong>超平面方程</strong>：<script type="math/tex; mode=display">w \cdot x + b = 0</script></li>
</ul>
<h3 id="点到超平面的几何距离"><a href="#点到超平面的几何距离" class="headerlink" title="点到超平面的几何距离"></a>点到超平面的几何距离</h3><script type="math/tex; mode=display">d = \frac{|w \cdot x + b|}{\|w\|}</script><hr>
<h2 id="2-感知机模型-Perceptron"><a href="#2-感知机模型-Perceptron" class="headerlink" title="2. 感知机模型 (Perceptron)"></a>2. 感知机模型 (Perceptron)</h2><blockquote>
<p><strong>关键词</strong>：<code>线性二分模型、判别模型</code></p>
</blockquote>
<h3 id="1-模型"><a href="#1-模型" class="headerlink" title="(1) 模型"></a>(1) 模型</h3><ul>
<li><strong>输入空间</strong>：$x \in R^n$</li>
<li><strong>输出空间</strong>：$y \in \{+1, -1\}$</li>
<li><p><strong>假设空间</strong>：</p>
<script type="math/tex; mode=display">f(x) = sign(w \cdot x + b)</script><p><strong>符号函数 (Sign Function)</strong>：</p>
<script type="math/tex; mode=display">
sign(x) =
\begin{cases}
+1, & x \geq 0 \\
-1, & x < 0
\end{cases}</script></li>
<li><p><strong>模型参数</strong>：</p>
<ul>
<li>$w$：权值向量</li>
<li>$b$：偏置</li>
<li>$w \cdot x$：内积</li>
</ul>
</li>
</ul>
<h3 id="2-策略"><a href="#2-策略" class="headerlink" title="(2) 策略"></a>(2) 策略</h3><p>感知机的目标是<strong>最小化误分类点到超平面的总距离。</strong></p>
<ul>
<li><strong>损失函数</strong>（误分类点集合 $M$）：<script type="math/tex; mode=display">
  L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script>  <em>(注：由于 $\frac{1}{|w|}$ 不影响梯度方向，为了计算简便，在损失函数中省略)</em></li>
</ul>
<h3 id="3-算法"><a href="#3-算法" class="headerlink" title="(3) 算法"></a>(3) 算法</h3><p>转化为求解最优化问题：</p>
<script type="math/tex; mode=display">
\min_{w, b} L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script><hr>
<h2 id="3-算法实现形式"><a href="#3-算法实现形式" class="headerlink" title="3. 算法实现形式"></a>3. 算法实现形式</h2><h3 id="1-梯度下降法"><a href="#1-梯度下降法" class="headerlink" title="(1) 梯度下降法"></a>(1) 梯度下降法</h3><ul>
<li><strong>输入</strong>：训练数据集 $T = {(x_1, y_1), \dots, (x_N, y_N)}$，学习率 $\eta$ ($0 &lt; \eta \le 1$)</li>
<li><strong>输出</strong>：$w, b$</li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$w_0 \leftarrow 0, b_0 \leftarrow 0$，$\eta$看题目</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件 $y_i (w \cdot x_i + b) \le 0$，则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
w &\leftarrow w + \eta y_i x_i \\
b &\leftarrow b + \eta y_i
\end{aligned}</script></li>
<li><strong>循环</strong>：转至 (2)，直至训练集中没有误分类点。</li>
</ol>
<blockquote>
<p><strong>特点</strong>：直观，但如果数据量大或维度高，计算量大。</p>
</blockquote>
<h3 id="2-对偶形式"><a href="#2-对偶形式" class="headerlink" title="(2) 对偶形式"></a>(2) 对偶形式</h3><ul>
<li><strong>核心思想</strong>：将 $w$ 表示为实例 $x_i$ 和标记 $y_i$ 的线性组合。</li>
<li><strong>输出</strong>：$\alpha, b$</li>
<li><strong>假设空间变为</strong>：<script type="math/tex; mode=display">f(x) = sign\left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right)</script>  <em>(其中 $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_N)^T$)</em></li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$\alpha \leftarrow 0, b \leftarrow 0$</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件：<script type="math/tex; mode=display">
y_i \left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right) \le 0</script>则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
\alpha_i &\leftarrow \alpha_i + \eta \\
b &\leftarrow b + \eta y_i
\end{aligned}</script>（$b$和梯度下降一样）</li>
<li><strong>循环</strong>：转至 (2) 直到无误分类数据。</li>
</ol>
<h4 id="💡-重点：如何计算判别式："><a href="#💡-重点：如何计算判别式：" class="headerlink" title="💡 重点：如何计算判别式："></a>💡 重点：如何计算判别式：</h4><p>在对偶形式中，样本之间的内积计算非常频繁，通常预先计算 <strong>Gram 矩阵</strong> 并存储：</p>
<script type="math/tex; mode=display">
G = [x_i \cdot x_j]_{N \times N} =
\begin{bmatrix}
x_1 \cdot x_1 & x_1 \cdot x_2 & \cdots & x_1 \cdot x_N \\
x_2 \cdot x_1 & x_2 \cdot x_2 & \cdots & x_2 \cdot x_N \\
\vdots & \vdots & \ddots & \vdots \\
x_N \cdot x_1 & x_N \cdot x_2 & \cdots & x_N \cdot x_N \\
\end{bmatrix}</script><p><strong>注意：$x_i$ 有几个，$\alpha_i$ 就有几个 $=0$（初始化）</strong><br><strong>做题技巧（表格法迭代）</strong>：<br><img src="2025-11-30-18-07-41.png" alt=""></p>
<hr>
<h2 id="4-感知机算法收敛性定理"><a href="#4-感知机算法收敛性定理" class="headerlink" title="4. 感知机算法收敛性定理"></a>4. 感知机算法收敛性定理</h2><ul>
<li><p><strong>收敛定理 (Novikoff)</strong>：<br>  误分类次数 $k$ 有上界。这意味着，<strong>只要训练数据集是线性可分的，感知机算法一定能收敛</strong>（在有限步内找到解）。</p>
</li>
<li><p><strong>局限性</strong>：</p>
<ol>
<li><strong>解不唯一</strong>：解依赖于初值的选择，也依赖于误分类点的选择顺序。</li>
<li><strong>震荡</strong>：如果数据集<strong>线性不可分</strong>，算法会一直震荡，无法做到完全正确分类。</li>
</ol>
</li>
<li><p><strong>下一步引申</strong>：<br>  为了得到<strong>唯一</strong>且<strong>最优</strong>的分离超平面，我们需要引入更强的约束 —— 这就是 <strong>SVM (支持向量机)</strong> 的由来。  </p>
</li>
</ul>
<h1 id="第三章-K-近邻"><a href="#第三章-K-近邻" class="headerlink" title="第三章 $K$近邻"></a>第三章 $K$近邻</h1><h2 id="1-算法简介"><a href="#1-算法简介" class="headerlink" title="1. 算法简介"></a>1. 算法简介</h2><blockquote>
<p><strong>关键词</strong>：<code>分类模型、回归模型、适用于(数值型、标称型)数据、1-NN训练错误率=0</code></p>
</blockquote>
<h3 id="算法特点及优缺点"><a href="#算法特点及优缺点" class="headerlink" title="算法特点及优缺点"></a>算法特点及优缺点</h3><h4 id="懒惰学习-Lazy-Learning"><a href="#懒惰学习-Lazy-Learning" class="headerlink" title="懒惰学习 (Lazy Learning)"></a>懒惰学习 (Lazy Learning)</h4><blockquote>
<p><strong>定义</strong>：<strong>无实质性模型训练</strong>，利用训练集数据对特征空间进行划分。测试时，要计算输入实例距每个样本的距离。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优点 (Pros)</th>
<th style="text-align:left">缺点 (Cons)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>精确度高</strong></td>
<td style="text-align:left"><strong>时间复杂度高</strong>（花时间）</td>
</tr>
<tr>
<td style="text-align:left"><strong>对异常值不敏感</strong></td>
<td style="text-align:left"><strong>空间复杂度高</strong>（耗内存）</td>
</tr>
<tr>
<td style="text-align:left"><strong>无数据输入假定</strong>（无训练阶段）</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
</div>
<h3 id="K-值的选择"><a href="#K-值的选择" class="headerlink" title="$K$ 值的选择"></a>$K$ 值的选择</h3><ul>
<li><p><strong>$K$ 值较小</strong> (模型复杂)</p>
<ul>
<li>近似误差（<em>偏差</em>）减小 $\rightarrow$ 估计误差（<em>方差</em>）增大（对噪声敏感）</li>
<li><strong>噪声敏感</strong></li>
<li>整体模型变得复杂，容易发生<strong>过拟合</strong></li>
</ul>
</li>
<li><p><strong>$K$ 值较大</strong> (模型简单)</p>
<ul>
<li>近似误差增大 $\rightarrow$ 估计误差减小</li>
<li>整体的模型变得简单，但容易发生<strong>欠拟合</strong></li>
</ul>
</li>
<li><p><strong>通过交叉验证选择 $K$ 值</strong></p>
<ul>
<li>模型准确率随 $K$ 值的变化<strong>非单调</strong></li>
<li>$K$ 一般低于训练集样本容量的平方根：$k &lt; \sqrt{N}$</li>
<li>经验上，$k$ 值一般不超过 20：$k &lt; 20$</li>
</ul>
</li>
</ul>
<h3 id="kNN-算法的应用挑战"><a href="#kNN-算法的应用挑战" class="headerlink" title="$kNN$ 算法的应用挑战"></a>$kNN$ 算法的应用挑战</h3><ul>
<li><strong>距离函数确定</strong><ul>
<li>距离度量方式众多</li>
</ul>
</li>
<li><strong>$k$ 值确定</strong><ul>
<li>$k$ 值对模型精度的影响<strong>非单调</strong></li>
<li>边界易出错 (软分类)</li>
</ul>
</li>
<li><strong>特征的选择</strong><ul>
<li>不同特征的影响程度不同</li>
</ul>
</li>
<li><strong>复杂度</strong><ul>
<li>需要计算测试例 $x’$ 与所有训练数据的距离</li>
<li>计算复杂度正比于训练样本容量</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-距离度量"><a href="#2-距离度量" class="headerlink" title="2. 距离度量"></a>2. 距离度量</h2><h3 id="L-p-距离"><a href="#L-p-距离" class="headerlink" title="$L_p$ 距离"></a>$L_p$ 距离</h3><ul>
<li><p><strong>欧氏距离 ($L_2$)</strong>：</p>
<script type="math/tex; mode=display">L_2(x_i, x_j) = \left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^2\right)^{\frac{1}{2}}</script></li>
<li><p><strong>曼哈顿距离 ($L_1$)</strong>：</p>
<script type="math/tex; mode=display">L_1(x_i, x_j) = \sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|</script></li>
<li><p><strong>切比雪夫距离 ($L_{\infty}$)</strong>：（取同维度坐标差最大者）</p>
<script type="math/tex; mode=display">L_{\infty}(x_i, x_j) = \max_{l} |x_i^{(l)} - x_j^{(l)}|</script></li>
</ul>
<h3 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h3><ul>
<li>了解即可</li>
</ul>
<h3 id="距离度量的性质"><a href="#距离度量的性质" class="headerlink" title="距离度量的性质"></a>距离度量的性质</h3><ul>
<li><strong>非负性 (non-negativity)</strong>:<script type="math/tex; mode=display">D(a, b) \ge 0</script></li>
<li><strong>自反性 (reflexivity)</strong>:<script type="math/tex; mode=display">D(a, b) = 0 \iff a = b</script></li>
<li><strong>对称性 (symmetry)</strong>:<script type="math/tex; mode=display">D(a, b) = D(b, a)</script></li>
<li><strong>三角不等式 (triangle inequality)</strong>:<script type="math/tex; mode=display">D(a, b) + D(b, c) \ge D(a, c)</script></li>
</ul>
<hr>
<h2 id="3-分类决策规则"><a href="#3-分类决策规则" class="headerlink" title="3. 分类决策规则"></a>3. 分类决策规则</h2><ul>
<li><strong>核心规则</strong>：多数表决规则。</li>
<li><strong>理论意义</strong>：等价于 $0-1$ 损失函数下的<strong>经验风险最小化</strong>。</li>
<li><strong>误分类概率</strong>：在一个领域中，与领域样本特征不同的总数 $\div$ 领域样本总数。</li>
</ul>
<hr>
<h2 id="4-kd-树"><a href="#4-kd-树" class="headerlink" title="4. $kd$ 树"></a>4. $kd$ 树</h2><h3 id="搜索方法"><a href="#搜索方法" class="headerlink" title="搜索方法"></a>搜索方法</h3><p><img src="2025-12-01-15-48-55.png" alt=""></p>
<p><em>(注：$kd$ 树是二叉树)</em>  </p>
<ol>
<li>先在 $x$ 轴找到中位数，垂直划分。</li>
<li>再在划分的两块区域里找 $y$ 轴中位数，继续划分。</li>
<li>直至划分完毕。</li>
</ol>
<h3 id="kd-树计算复杂度"><a href="#kd-树计算复杂度" class="headerlink" title="$kd$ 树计算复杂度"></a>$kd$ 树计算复杂度</h3><ul>
<li><strong>一般情况</strong>：时间复杂度为 $O(\log N)$</li>
<li><strong>最坏情况</strong>：时间复杂度 $O(N)$</li>
<li><strong>适用场景</strong>：更适用于<strong>训练实例维度远大于空间维度</strong>时的搜索。  </li>
</ul>
<h1 id="第四章-贝叶斯方法"><a href="#第四章-贝叶斯方法" class="headerlink" title="第四章 贝叶斯方法"></a>第四章 贝叶斯方法</h1><h2 id="1-概率统计基础知识"><a href="#1-概率统计基础知识" class="headerlink" title="1. 概率统计基础知识"></a>1. 概率统计基础知识</h2><h3 id="pmf-amp-pdf"><a href="#pmf-amp-pdf" class="headerlink" title="$pmf$ &amp; $pdf$"></a>$pmf$ &amp; $pdf$</h3><p><strong>概率质量函数（Probability mass function）</strong><br>定义：$P(X=x_i)$，即<strong>离散随机变量</strong>在各特定取值上的概率。</p>
<ul>
<li>$\sum_{i} P(X=x_i) = 1$</li>
<li>$P(X=x_i \cap X=x_j) = 0 \text{ if } i \neq j$</li>
<li>$P(X=x_i \cup X=x_j) = P(X=x_i) + P(X=x_j) \text{ if } i \neq j$</li>
<li>$P(X=x_1 \cup X=x_2 \cup \dots \cup X=x_k) = 1$</li>
</ul>
<p><strong>概率密度函数（Probability density function）</strong><br>定义：描述了<strong>连续随机变量</strong> $x$ 的概率密度。</p>
<ul>
<li>$f(x) \ge 0, \forall x$</li>
<li>$\int_{-\infty}^{+\infty} f(x) = 1$</li>
<li>真实概率通过 pdf 的积分得到</li>
<li>e.g. $X$ 取 0-1 的概率为：<script type="math/tex; mode=display">P(0 \le X \le 1) = \int_{0}^{1} f(x) dx</script></li>
</ul>
<h3 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h3><p>描述两个或多个随机变量<strong>同时</strong>发生的概率。</p>
<ul>
<li><strong>离散型</strong>：$P(X=x, Y=y)$<ul>
<li>性质：$\sum_x \sum_y P(X=x, Y=y) = 1$</li>
</ul>
</li>
<li><strong>连续型</strong>：$f(x, y)$<ul>
<li>性质：$\iint f(x, y) dx dy = 1$</li>
</ul>
</li>
<li><strong>边缘分布 (Marginalization)</strong>：从联合分布中求出单个变量的分布。<script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x, Y=y)</script></li>
<li><strong>独立性</strong>：若 $X$ 与 $Y$ 相互独立，则：<script type="math/tex; mode=display">P(X, Y) = P(X)P(Y)</script></li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>定义：在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。</p>
<script type="math/tex; mode=display">P(A|B) = \frac{P(A, B)}{P(B)}</script><p>（其中 $P(A, B)$ 是联合概率，$P(B)$ 是边缘概率）</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>将联合概率分解为条件概率的乘积。<br>对于两个变量：</p>
<script type="math/tex; mode=display">P(A, B) = P(A|B)P(B) = P(B|A)P(A)</script><p>推广到 $n$ 个变量：</p>
<script type="math/tex; mode=display">P(X_1, X_2, \dots, X_n) = P(X_1) \prod_{i=2}^{n} P(X_i | X_1, \dots, X_{i-1})</script><h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>如果事件组 $B_1, B_2, \dots, B_n$ 构成样本空间的一个划分（互斥且完备），则对于任意事件 $A$：</p>
<script type="math/tex; mode=display">P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i)</script><ul>
<li><strong>直观理解</strong>：求 $A$ 发生的概率，等于把 $A$ 在各种不同情况（$B_i$）下发生的概率加权求和。</li>
</ul>
<h3 id="均值与方差"><a href="#均值与方差" class="headerlink" title="均值与方差"></a>均值与方差</h3><ul>
<li><strong>期望 (Expectation / Mean)</strong> $\mu$：<ul>
<li>离散：$E[X] = \sum x_i P(x_i)$</li>
<li>连续：$E[X] = \int_{-\infty}^{+\infty} x f(x) dx$</li>
<li>性质：$E[aX + b] = aE[X] + b$</li>
</ul>
</li>
<li><strong>方差 (Variance)</strong> $\sigma^2$：衡量数据的离散程度。<script type="math/tex; mode=display">Var(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2</script></li>
</ul>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>定义：衡量两个变量 $X$ 和 $Y$ 总体误差的期望，反映它们之间变化的<strong>方向</strong>。</p>
<script type="math/tex; mode=display">Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)]</script><ul>
<li><strong>$Cov &gt; 0$</strong>：正相关</li>
<li><strong>$Cov &lt; 0$</strong>：负相关</li>
<li><strong>$Cov = 0$</strong>：不相关（线性无关）</li>
</ul>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p><strong>皮尔逊相关系数 (Correlation Coefficient)</strong> $\rho$：<br>将协方差归一化，消除了量纲的影响，衡量线性相关的强度。</p>
<script type="math/tex; mode=display">\rho_{X,Y} = \frac{Cov(X, Y)}{\sigma_x \sigma_y}</script><ul>
<li>取值范围：$[-1, 1]$。<br>（$1$ 为完全正相关，$-1$ 为完全负相关，$0$ 为无线性相关）</li>
</ul>
<h2 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2. 朴素贝叶斯"></a>2. 朴素贝叶斯</h2><h3 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h3><ul>
<li><strong><code>贝叶斯公式 (Bayes&#39; Theorem)</code></strong>：<script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)P(A)}{P(B)}</script><ul>
<li>$P(A|B)$: 后验概率 (Posterior)</li>
<li>$P(B|A)$: 似然 (Likelihood)</li>
<li>$P(A)$: 先验概率 (Prior)</li>
<li>$P(B)$: 证据 (Evidence)</li>
</ul>
</li>
<li><strong>贝叶斯决策</strong>：<ul>
<li>已知<strong>类条件概率密度</strong>参数表达式和<strong>先验概率</strong></li>
<li>利用<strong>贝叶斯公式</strong>转换成<strong>后验概率</strong></li>
<li>根据<strong>后验概率</strong>大小进行决策分类</li>
</ul>
</li>
</ul>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>基于贝叶斯定理与特征条件独立假设的分类算法。</p>
<ul>
<li><strong>“朴素” (Naive) 的含义</strong>：假设样本的各个特征之间<strong>相互独立</strong>。<script type="math/tex; mode=display">P(x_1, x_2, \dots, x_n | y) = \prod_{i=1}^{n} P(x_i | y)</script></li>
</ul>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>给定输入特征向量 $\mathbf{x} = (x_1, x_2, \dots, x_n)$，预测类别 $y$。<br>我们只需最大化分子部分<code>(这个公式考试一定要写)</code>：</p>
<script type="math/tex; mode=display">\hat{y} = \arg\max_{y} P(y) \prod_{i=1}^{n} P(x_i | y)</script><ul>
<li>$P(y)$：先验概率，即类别的频率。</li>
<li>$P(x_i|y)$：条件概率（似然），即在某类中观察到该特征的概率。</li>
</ul>
<h3 id="朴素贝叶斯参数估计"><a href="#朴素贝叶斯参数估计" class="headerlink" title="朴素贝叶斯参数估计"></a>朴素贝叶斯参数估计</h3><ol>
<li><strong>极大似然估计 (MLE)</strong><br>直接使用频率来估计概率。<ul>
<li>$P(y = c_k) = \frac{N_{c_k}}{N}$</li>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik}}{N_{c_k}}$</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i}{N}$</li>
</ul>
</li>
<li><strong>贝叶斯估计 (Bayes)</strong><br>引入平滑项（如拉普拉斯平滑）解决零概率问题。<ul>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}$</li>
<li>其中 $\lambda \ge 0$ 为平滑系数（常取 1），$S_i$ 为特征 $x_i$ 可能取值的个数。</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i+\alpha}{N+\alpha+\beta}$ ($\hat{\theta}$ ~ $Beta(\alpha,\beta)$)</li>
</ul>
</li>
<li><strong>拉普拉斯平滑 (Laplace Smoothing)</strong><br>解决零概率问题（即某个特征在训练集中未出现导致概率为0）。<script type="math/tex; mode=display">P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}</script><ul>
<li>$\lambda$: 平滑系数（通常取 1）。</li>
<li>$S_i$: 特征 $x_i$ 可能取值的个数。</li>
</ul>
</li>
</ol>
<h1 id="第五章-决策树"><a href="#第五章-决策树" class="headerlink" title="第五章 决策树"></a>第五章 决策树</h1><h2 id="1-决策树模型"><a href="#1-决策树模型" class="headerlink" title="1. 决策树模型"></a>1. 决策树模型</h2><h3 id="决策树的表示"><a href="#决策树的表示" class="headerlink" title="决策树的表示"></a>决策树的表示</h3><ul>
<li>一种描述对实例分类的 <strong>树形结构</strong>，包含：<ul>
<li><strong>根结点 (Root Node)</strong>：最上面的结点，是整个决策树的开始。</li>
<li><strong>内部结点 (Internal Node)</strong>：代表一个<strong>问题</strong>或者<strong>决策</strong>，通常对应待<strong>分类对象的属性</strong>。</li>
<li><strong>叶结点 (Leaf Node)</strong>：代表一种可能的<strong>分类结果</strong>。</li>
<li><strong>有向边</strong>：连接各个结点。</li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>可解释性强</strong>：推理过程容易理解，可以表示成 <code>If-Then</code> 形式。</li>
<li><strong>依赖属性取值</strong>：推理过程完全依赖于属性变量的取值特点。</li>
<li><strong>特征筛选</strong>：可自动忽略对目标变量<strong>没有贡献的属性变量</strong>，为判断属性重要性、减少变量数目提供参考。</li>
</ul>
<h3 id="学习目的"><a href="#学习目的" class="headerlink" title="学习目的"></a>学习目的</h3><ul>
<li><strong>理想的决策树</strong>（通常有以下3种目标）：<ol>
<li>叶结点数最少</li>
<li>叶结点深度最小</li>
<li>叶结点数最少且叶结点深度最小</li>
</ol>
</li>
<li><strong>现实困境</strong>：找到这种绝对最优的决策树是 <strong>NP难题</strong>。</li>
<li><strong>实际目标</strong>：找到 <strong>尽可能</strong> 趋向于最优的决策树。</li>
</ul>
<hr>
<h2 id="2-特征选择与信息熵"><a href="#2-特征选择与信息熵" class="headerlink" title="2. 特征选择与信息熵"></a>2. 特征选择与信息熵</h2><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h3><ul>
<li><strong>定义</strong>：度量样本集合 <strong>纯度 (Purity)</strong> 最常用的一种指标，代表随机变量<strong>不确定性的度量</strong>。<blockquote>
<p><strong>规律</strong>：熵越大，随机变量的不确定性就越大。</p>
</blockquote>
</li>
<li><strong>计算公式</strong>：<br>  设 $X$ 是一个取有限个值的离散随机变量，其概率分布为 $P(X=x_i) = p_i, \quad i=1,2,\dots,n$，则随机变量 $X$ 的熵 $H(X)$ 定义为：<script type="math/tex; mode=display">H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i</script>  <strong>注</strong>：若 $p_i=0$，则定义 $0 \log 0 = 0$。</li>
</ul>
<h3 id="条件熵-Conditional-Entropy"><a href="#条件熵-Conditional-Entropy" class="headerlink" title="条件熵 (Conditional Entropy)"></a>条件熵 (Conditional Entropy)</h3><ul>
<li><strong>定义</strong>：表示在已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。</li>
<li><strong>计算公式</strong>：<script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)</script>  其中 $p_i = P(X=x_i)$。</li>
</ul>
<h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益 (Information Gain)"></a>信息增益 (Information Gain)</h3><ul>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差。<script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A)</script></li>
<li><strong>物理意义</strong>：由于特征 $A$ 而使得对数据集 $D$ 的分类不确定性减少的程度。<strong>信息增益大的特征具有更强的分类能力。</strong></li>
</ul>
<h3 id="信息增益比-Information-Gain-Ratio"><a href="#信息增益比-Information-Gain-Ratio" class="headerlink" title="信息增益比 (Information Gain Ratio)"></a>信息增益比 (Information Gain Ratio)</h3><ul>
<li><strong>背景</strong>：信息增益倾向于选择 <strong>取值较多</strong> 的特征（例如唯一的 ID 号），这往往不是我们想要的。</li>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>  其中，<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>   $n$ 是特征 $A$ 取值的个数。</li>
</ul>
<hr>
<h2 id="3-经典算法"><a href="#3-经典算法" class="headerlink" title="3. 经典算法"></a>3. 经典算法</h2><h3 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="$ID3$ 算法"></a>$ID3$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益</code>  </li>
</ul>
<p><strong>具体计算流程</strong>：（设$C_1+C_2=D$）</p>
<ol>
<li><p><strong>计算不考虑特征的信息熵</strong>：</p>
<script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k= -\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}</script><p>其中 $|D|$ 为样本总数，$|C_k|$ 为属于第 $k$ 类的样本数(eg: <em>买/不买</em>)。</p>
</li>
<li><p><strong>计算条件熵</strong>：</p>
<ul>
<li>先选择一个 <strong>特征 $A_i$</strong>，将数据集 $D$ 根据特征 $A_i$ 的<strong>不同取值</strong>划分为 $n$ 个子集 $D_1, D_2, \dots, D_n$。</li>
<li>计算特征 $A_i$ 下不同取值 $D_i$ 的信息熵 $H(D_i)$：<script type="math/tex; mode=display">H(D_i)= -\sum_{k=1}^{K} \frac{|C_k|}{|D_i|} \log_2 \frac{|C_k|}{|D_i|}</script>其中，$C_k$ 是在集合 $D_i$ 里的第 $k$ 种类别的数量 (eg: <em>买/不买</em>)。</li>
<li>计算条件熵 $H(D|A_i)$：<script type="math/tex; mode=display">H(D|A_i) = \sum_{i=1}^{n} p_i H(D_i)= \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)</script></li>
</ul>
</li>
<li><p><strong>计算信息增益</strong>：</p>
<script type="math/tex; mode=display">g(D, A_i) = H(D) - H(D|A_i)</script></li>
<li><p><strong>特征选择</strong>：<br>选取 <strong>信息增益最大</strong> 的特征$A_i$作为当前的分裂节点（根节点或内部节点）。</p>
</li>
<li><p><strong>递归构建</strong>：<br>对子节点重复以上步骤，以此类推，直至分类完全（或满足停止条件）。</p>
</li>
</ol>
<h3 id="C4-5-算法"><a href="#C4-5-算法" class="headerlink" title="$C4.5$ 算法"></a>$C4.5$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益比</code> （除<strong>选择特征的标准不同</strong>外，其余步骤与 $ID3$ <strong>一致</strong>）。</li>
<li>计算完<strong>信息增益</strong>后，计算<strong>信息增益比</strong>：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>其中，<strong>特征固有值</strong>：<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>这里 $|D|$ 是样本总数，注意和 $H(D_i)$ 的计算区分。</li>
</ul>
<h3 id="过拟合-Overfitting-1"><a href="#过拟合-Overfitting-1" class="headerlink" title="过拟合 (Overfitting)"></a>过拟合 (Overfitting)</h3><ul>
<li><strong>现象</strong>：决策树算法增长树的每一个分支的深度，直到恰好能对训练样例比较完美地分类。</li>
<li><strong>原因</strong>：当数据中 <strong>有噪声</strong> 或 <strong>训练样例数量太少</strong>，以至于不能产生目标函数的有代表性的采样时，简单算法产生的树会过拟合训练样例。</li>
</ul>
<hr>
<h2 id="4-CART-算法-Classification-And-Regression-Tree"><a href="#4-CART-算法-Classification-And-Regression-Tree" class="headerlink" title="4. $CART$算法 (Classification And Regression Tree)"></a>4. $CART$算法 (Classification And Regression Tree)</h2><h3 id="基尼指数-Gini-Index"><a href="#基尼指数-Gini-Index" class="headerlink" title="基尼指数 (Gini Index)"></a>基尼指数 (Gini Index)</h3><ul>
<li><strong>定义</strong>：度量数据纯度的指标。<strong>基尼指数越小，模型的不确定性越小，纯度越高</strong>。</li>
<li><p><strong>计算公式</strong>：<br>  假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$，则概率分布的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K} p_k^2</script><p>  对于给定的样本集合 $D$，其基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D) = 1 - \sum_{k=1}^{K} \left(\frac{|C_k|}{|D|}\right)^2</script><p>  其中 $C_k$ 是 $D$ 中属于第 $k$ 类的样本子集。</p>
</li>
<li><p><strong>特征下的基尼指数</strong>：（一分为二的划分问题）<br>  如果特征 $A$ 的取值将集合 $D$ 划分成 $D_1$ 和 $D_2$ 两部分（<strong>注意：CART 生成的是二叉树</strong>），则在特征 $A$ 的条件下，集合 $D$ 的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D, A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)</script><p>  <img src="2025-12-04-20-06-28.png" alt=""><br>  <img src="2025-12-04-20-06-41.png" alt=""><br>  <img src="2025-12-04-20-06-57.png" alt=""></p>
</li>
<li><strong>选择标准</strong>：选择使得 <strong>基尼指数最小</strong> 的<strong>特征</strong>及其<strong>划分点</strong>作为最优特征和最优切分点。<img src="2025-12-04-20-08-35.png" alt=""><img src="2025-12-04-20-10-08.png" alt=""></li>
</ul>
<h3 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h3><ul>
<li><strong>回归树</strong>：使用 <strong>平方误差最小化</strong> 准则。</li>
<li><strong>分类树</strong>：使用 <strong>基尼指数最小化</strong> 准则。</li>
<li><strong>具体生成流程 (分类树)</strong>：<em>（上一部分的图片例子可以帮助理解）</em><ol>
<li>对训练数据集 $D$ 的每个特征 $A$，以及该特征的每个可能取值 $a$，根据 $A=a$ 与 $A \neq a$ 将 $D$ 分割为 $D_1$ 和 $D_2$。</li>
<li>计算该切分下的 $Gini(D, A)$。</li>
<li>在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择 <strong>基尼指数最小</strong> 的特征及其切分点作为最优特征与最优切分点。</li>
<li>生成两个子结点，将训练数据集依特征分配到两个子结点中。</li>
<li>对子结点递归地调用上述步骤，直到满足停止条件（如结点样本数小于阈值、基尼指数小于阈值或没有更多特征）。</li>
</ol>
</li>
</ul>
<h3 id="决策树剪枝-Pruning"><a href="#决策树剪枝-Pruning" class="headerlink" title="决策树剪枝 (Pruning)"></a>决策树剪枝 (Pruning)</h3><ul>
<li><strong>目的</strong>：防止过拟合。</li>
<li><strong>手段</strong>：剪枝，限定叶节点最小样本数，强制决策树最大深度，交叉验证（$CART$剪枝）。</li>
</ul>
<h4 id="预剪枝-Pre-Pruning"><a href="#预剪枝-Pre-Pruning" class="headerlink" title="预剪枝 (Pre-Pruning)"></a>预剪枝 (Pre-Pruning)</h4><ul>
<li>在决策树生成过程中，对每个结点在 <strong>划分前先进行估计</strong>。</li>
<li>若当前结点的划分不能带来决策树 <strong>泛化性能提升</strong>，则停止划分，并标记当前结点为叶结点。</li>
</ul>
<h4 id="后剪枝-Post-Pruning"><a href="#后剪枝-Post-Pruning" class="headerlink" title="后剪枝 (Post-Pruning)"></a>后剪枝 (Post-Pruning)</h4><ul>
<li>从训练集先生成一棵 <strong>完整的决策树</strong>。</li>
<li>自底而上地考察非叶结点，若将该结点对应的子树替换为叶结点能带来决策树 <strong>泛化性能提升</strong>，则 <strong>将该子树替换为叶结点</strong>。</li>
<li>CART 常用 <strong>CCP (Cost-Complexity Pruning, 代价复杂度剪枝)</strong> 方法。</li>
</ul>
<blockquote>
<p><strong>对比</strong>：一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能优于预剪枝决策树。但训练时间开销比未剪枝决策树和预剪枝决策树要大得多。</p>
</blockquote>
<hr>
<h2 id="5-随机森林-Random-Forest"><a href="#5-随机森林-Random-Forest" class="headerlink" title="5. 随机森林 (Random Forest)"></a>5. 随机森林 (Random Forest)</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>一种基于 <strong>Bagging</strong>（装袋法）的集成学习方法。</li>
<li>通过构建多棵决策树，并将它们的预测结果进行汇总（分类问题用 <strong>多数投票</strong>，回归问题用 <strong>平均值</strong>）来输出最终结果。</li>
<li><strong>“随机”的含义</strong>：体现在 <strong>样本选择的随机性</strong> 和 <strong>特征选择的随机性</strong>。</li>
</ul>
<h3 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h3><ol>
<li><p><strong>Bootstrap 采样 (样本随机)</strong>：</p>
<ul>
<li>对于 $N$ 个样本的训练集，有放回地随机抽取 $N$ 次，得到一个新的训练集。</li>
<li>未被抽到的样本称为 <strong>袋外数据 (Out-of-Bag, OOB)</strong>，可用于验证模型泛化能力。</li>
<li>重复上述步骤 $k$ 次，生成 $k$ 个独立的训练集。</li>
</ul>
</li>
<li><p><strong>特征随机选择 (特征随机)</strong>：</p>
<ul>
<li>在构建每棵树的每个分裂结点时，不是从所有 $M$ 个特征中选择最优特征，而是随机从 $M$ 个特征中选取 $m$ 个特征 ($m \ll M$) 作为一个子集。</li>
<li>从这 $m$ 个特征中选择最优的分裂属性。</li>
</ul>
</li>
<li><p><strong>生成决策树</strong>：</p>
<ul>
<li>利用上述采样和特征子集，完全生长决策树（通常 <strong>不剪枝</strong>）。</li>
</ul>
</li>
<li><p><strong>集成输出</strong>：</p>
<ul>
<li><strong>分类</strong>：所有树投票，票数最多的类别为最终结果。</li>
<li><strong>回归</strong>：所有树预测值的简单算术平均。</li>
</ul>
</li>
</ol>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>准确率高</strong>：在许多数据集上表现良好，抗过拟合能力强（因为引入了双重随机性）。</li>
<li><strong>并行处理</strong>：每棵树互不依赖，可以并行训练，速度快。</li>
<li><strong>高维数据友好</strong>：能够处理具有成千上万个特征的输入矩阵，而无需进行特征降维。</li>
<li><strong>自带评估</strong>：可以使用 OOB 数据进行内部评估，无需额外的验证集。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">文韬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/11/27/statistical-learning/">http://example.com/2025/11/27/statistical-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">WENTAO's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/images/logo.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/11/26/lecture-report/" title="专业前沿讲座报告"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">专业前沿讲座报告</div></div><div class="info-2"><div class="info-item-1"> 课程信息  课程名称： 专业前沿讲座 学期： 2025—2026学年第 1 学期 作者： 宋文韬 (智机试验2305 / 23013361)   摘要当前，人工智能与机器人技术正经历高速发展，多智能体协同、复杂系统优化等成为研究热点。文章结合《专业前沿讲座》课程内容及相关文献研读，梳理与分析了多智能体感知决策、大规模全局优化、工业互联网信息融合与安全以及机器学习在科学探索中的前沿应用。通过对这些技术在原理、实例及前沿研究上的综合分析，本报告总结了当前人工智能技术发展的内在规律与主流趋势，进一步形成了对未来研究方向与科研方法的初步认识与思考。 关键词： 人工智能与机器人、多智能体、全局优化、工业互联网、机器学习  1. 引言当今世界，新一轮科技革命和产业变革正在深入发展，人工智能与机器人技术已成为国际竞争的新高地，也是推动经济社会发展的核心引擎。从自动驾驶车队的协同作业到智慧工厂的精准制造，从复杂系统的调度优化到基础科学领域的突破性发现，智能科学技术正在以前所未有的速度重塑着我们对世界的认知与改造能力。作为信息学院的学生，置身于这一时代的浪潮中，不仅需要掌握扎实的工程基础，更需要...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">文韬</div><div class="author-info-description">在这里记录我的日常</div><div class="card-info-social-icons"><a class="social-icon" href="https://www.douyin.com/user/self?from_tab_name=main" target="_blank" title="抖音"><i class="fab fa-tiktok" style="color: #000000;"></i></a><a class="social-icon" href="https://space.bilibili.com/384204746?spm_id_from=333.1007.0.0" target="_blank" title="Bilibili"><i class="fab fa-bilibili" style="color: #fb7299;"></i></a><a class="social-icon" href="https://github.com/WENTAO2297" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="toc-text">第一章 统计学习方法概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0"><span class="toc-text">统计学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%E4%B8%8E%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="toc-text">发展历程与三要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="toc-text">核心：统计学三要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E4%BD%93%E7%B3%BB"><span class="toc-text">分类体系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting"><span class="toc-text">过拟合 (Overfitting)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88-Underfitting"><span class="toc-text">欠拟合 (Underfitting)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">第二章 线性感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%B6%85%E5%B9%B3%E9%9D%A2-Hyperplane"><span class="toc-text">1. 超平面 (Hyperplane)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%EF%BC%9F"><span class="toc-text">怎么理解？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%82%B9%E5%88%B0%E8%B6%85%E5%B9%B3%E9%9D%A2%E7%9A%84%E5%87%A0%E4%BD%95%E8%B7%9D%E7%A6%BB"><span class="toc-text">点到超平面的几何距离</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B-Perceptron"><span class="toc-text">2. 感知机模型 (Perceptron)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B"><span class="toc-text">(1) 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%AD%96%E7%95%A5"><span class="toc-text">(2) 策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%AE%97%E6%B3%95"><span class="toc-text">(3) 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%BD%A2%E5%BC%8F"><span class="toc-text">3. 算法实现形式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">(1) 梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F"><span class="toc-text">(2) 对偶形式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%92%A1-%E9%87%8D%E7%82%B9%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%88%A4%E5%88%AB%E5%BC%8F%EF%BC%9A"><span class="toc-text">💡 重点：如何计算判别式：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E6%94%B6%E6%95%9B%E6%80%A7%E5%AE%9A%E7%90%86"><span class="toc-text">4. 感知机算法收敛性定理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-K-%E8%BF%91%E9%82%BB"><span class="toc-text">第三章 $K$近邻</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">1. 算法简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">算法特点及优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%87%92%E6%83%B0%E5%AD%A6%E4%B9%A0-Lazy-Learning"><span class="toc-text">懒惰学习 (Lazy Learning)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-text">$K$ 值的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kNN-%E7%AE%97%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8%E6%8C%91%E6%88%98"><span class="toc-text">$kNN$ 算法的应用挑战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F"><span class="toc-text">2. 距离度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L-p-%E8%B7%9D%E7%A6%BB"><span class="toc-text">$L_p$ 距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB"><span class="toc-text">马氏距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="toc-text">距离度量的性质</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99"><span class="toc-text">3. 分类决策规则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-kd-%E6%A0%91"><span class="toc-text">4. $kd$ 树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="toc-text">搜索方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kd-%E6%A0%91%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">$kd$ 树计算复杂度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95"><span class="toc-text">第四章 贝叶斯方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-text">1. 概率统计基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pmf-amp-pdf"><span class="toc-text">$pmf$ &amp; $pdf$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="toc-text">联合概率分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="toc-text">条件概率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="toc-text">全概率公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-text">均值与方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="toc-text">协方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-text">相关性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-text">2. 朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-text">贝叶斯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-text">朴素贝叶斯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-text">朴素贝叶斯参数估计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">第五章 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. 决策树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-text">决策树的表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E7%9A%84"><span class="toc-text">学习目的</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-text">2. 特征选择与信息熵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%86%B5%EF%BC%88Entropy%EF%BC%89"><span class="toc-text">熵（Entropy）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5-Conditional-Entropy"><span class="toc-text">条件熵 (Conditional Entropy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-Information-Gain"><span class="toc-text">信息增益 (Information Gain)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%AF%94-Information-Gain-Ratio"><span class="toc-text">信息增益比 (Information Gain Ratio)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95"><span class="toc-text">3. 经典算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3-%E7%AE%97%E6%B3%95"><span class="toc-text">$ID3$ 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5-%E7%AE%97%E6%B3%95"><span class="toc-text">$C4.5$ 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting-1"><span class="toc-text">过拟合 (Overfitting)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-CART-%E7%AE%97%E6%B3%95-Classification-And-Regression-Tree"><span class="toc-text">4. $CART$算法 (Classification And Regression Tree)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0-Gini-Index"><span class="toc-text">基尼指数 (Gini Index)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90"><span class="toc-text">决策树生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D-Pruning"><span class="toc-text">决策树剪枝 (Pruning)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D-Pre-Pruning"><span class="toc-text">预剪枝 (Pre-Pruning)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D-Post-Pruning"><span class="toc-text">后剪枝 (Post-Pruning)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-Random-Forest"><span class="toc-text">5. 随机森林 (Random Forest)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-text">构建流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-1"><span class="toc-text">优点</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/27/statistical-learning/" title="模式识别与统计学习">模式识别与统计学习</a><time datetime="2025-11-27T07:03:00.000Z" title="发表于 2025-11-27 15:03:00">2025-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/26/lecture-report/" title="专业前沿讲座报告">专业前沿讲座报告</a><time datetime="2025-11-26T12:52:00.000Z" title="发表于 2025-11-26 20:52:00">2025-11-26</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 文韬</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.3-b2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3-b2"></script><script src="/js/main.js?v=5.5.3-b2"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'WENTAO2297/WENTAO2297.github.io',
      'data-repo-id': 'R_kgDOQbrGPw',
      'data-category-id': 'DIC_kwDOQbrGP84Cye9K',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><link rel="preload" href="/images/index.jpg" as="image"><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3-b2"></script></div></div></body></html>