<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>嵌入式系统原理</title>
    <url>/2025/12/21/STM32/</url>
    <content><![CDATA[<h1 id="第一章-导论"><a href="#第一章-导论" class="headerlink" title="第一章 导论"></a>第一章 导论</h1><blockquote>
<p>嵌入式系统是一切非PC和大型计算机系统。</p>
</blockquote>
<h1 id="第二章-Cortex-M3-微处理器"><a href="#第二章-Cortex-M3-微处理器" class="headerlink" title="第二章 Cortex-M3 微处理器"></a>第二章 Cortex-M3 微处理器</h1><h2 id="1-内核结构"><a href="#1-内核结构" class="headerlink" title="1. 内核结构"></a>1. 内核结构</h2><ul>
<li>核心架构：Cortex-M3 是基于 ARMv7-M 架构的 32 位处理器内核，采用高性能的<strong>哈佛结构</strong>，寻址能力为4GB。</li>
</ul>
<p><img src="2025-12-21-21-55-13.png" alt=""></p>
<ul>
<li>设计模式：ARM 公司设计内核，芯片制造商（如 ATMEL、NXP、TI）在此基础上添加不同的外设、存储器和 I/O，形成各类微控制器 (MCU)。</li>
</ul>
<h2 id="2-流水线技术"><a href="#2-流水线技术" class="headerlink" title="2. 流水线技术"></a>2. 流水线技术</h2><ul>
<li>定义：将指令分解为多步并重叠执行，以实现并行处理。</li>
</ul>
<p><img src="2025-12-21-22-00-52.png" alt=""></p>
<ul>
<li>流水线技术三大指标：吞吐率、加速比、效率</li>
</ul>
<h3 id="（1）吞吐率"><a href="#（1）吞吐率" class="headerlink" title="（1）吞吐率"></a>（1）吞吐率</h3><ul>
<li>单位时间内完成的指令条数<br><img src="2025-12-21-22-21-51.png" alt=""></li>
<li>$流水时间 = 三个阶段总时间 + （指令条数 -1）\times 时间最长的阶段$</li>
</ul>
<p><strong>例题</strong>：<br><img src="2025-12-21-22-24-19.png" alt=""></p>
<h3 id="（2）加速比"><a href="#（2）加速比" class="headerlink" title="（2）加速比"></a>（2）加速比</h3><ul>
<li>不使用流水线时间和使用流水线时间之比</li>
</ul>
<p>若流水线<strong>各段时间均为 $\Delta t$</strong>，流水线<strong>级数 $m$</strong>，<strong>指令数为 $n$</strong>：<br>分子：$nm\Delta t$<br>分母：$m\Delta t + (n-1)\Delta t$ <em>（流水线计算公式）</em><br>则加速比为：</p>
<script type="math/tex; mode=display">S_p = \frac{nm\Delta t}{m\Delta t + (n-1)\Delta t}</script><p>注意：<strong>平衡的流水线效率更优</strong></p>
<h2 id="3-系统总线"><a href="#3-系统总线" class="headerlink" title="3. 系统总线"></a>3. 系统总线</h2><ul>
<li><strong>总线</strong>：计算机中，各个部件之间传送信息的公共通路</li>
<li><strong>分类</strong>：数据总线、地址总线、控制总线</li>
</ul>
<h3 id="AMBA"><a href="#AMBA" class="headerlink" title="AMBA"></a>AMBA</h3><ul>
<li>AMBA 是一种<strong>高级微控制器总线架构</strong><br>主要包含以下两种不同的标准：</li>
</ul>
<ol>
<li>AHB（Advanced High-performance Bus）：高级高性能总线<ul>
<li>多个主机和多个从机的连接<br><img src="2025-12-21-23-56-03.png" alt=""></li>
<li>AHB-lite：单个主机和多个从机</li>
</ul>
</li>
<li>APB（Advanced Peripheral Bus）：高级外设总线<ul>
<li>低成本、低功耗、结构简单<br><img src="2025-12-21-23-54-52.png" alt=""></li>
</ul>
</li>
</ol>
<h3 id="Cortex-M3-总线结构"><a href="#Cortex-M3-总线结构" class="headerlink" title="Cortex-M3 总线结构"></a>Cortex-M3 总线结构</h3><p><img src="2025-12-21-23-52-52.png" alt=""></p>
<h2 id="4-寄存器与储存结构"><a href="#4-寄存器与储存结构" class="headerlink" title="4. 寄存器与储存结构"></a>4. 寄存器与储存结构</h2><p><img src="2025-12-22-13-55-29.png" alt=""></p>
<ul>
<li><strong>堆栈</strong>：先进后出，由一块连续内存、一个栈顶指针组成</li>
<li><strong>储存结构</strong>：Crtex-M3与外设<strong>统一编址</strong>，这种方式称为<strong>储存器地址映射</strong></li>
</ul>
<h2 id="5-工作模态"><a href="#5-工作模态" class="headerlink" title="5. 工作模态"></a>5. 工作模态</h2><h3 id="（1）两种模式与特权等级"><a href="#（1）两种模式与特权等级" class="headerlink" title="（1）两种模式与特权等级"></a>（1）两种模式与特权等级</h3><p><img src="2025-12-22-14-04-48.png" alt=""></p>
<h3 id="（2）三种模态"><a href="#（2）三种模态" class="headerlink" title="（2）三种模态"></a>（2）三种模态</h3><p><img src="2025-12-22-14-06-26.png" alt=""></p>
<h2 id="6-中断与异常"><a href="#6-中断与异常" class="headerlink" title="6. 中断与异常"></a>6. 中断与异常</h2><script type="math/tex; mode=display">
保存现场（压栈）\rightarrow 取向量（查找 ISR 地址）\rightarrow 更新寄存器（跳入 ISR 执行）</script><h3 id="（1）中断悬起"><a href="#（1）中断悬起" class="headerlink" title="（1）中断悬起"></a>（1）中断悬起</h3><ul>
<li>如果中断发生时，正在处理更高优先级异常/中断被屏蔽，此时<strong>中断被悬起</strong><br><img src="2025-12-22-14-12-58.png" alt=""></li>
</ul>
<h3 id="（2）中断活跃"><a href="#（2）中断活跃" class="headerlink" title="（2）中断活跃"></a>（2）中断活跃</h3><p><img src="2025-12-22-14-15-04.png" alt=""></p>
<h3 id="（3）中断请求信号保持"><a href="#（3）中断请求信号保持" class="headerlink" title="（3）中断请求信号保持"></a>（3）中断请求信号保持</h3><p><img src="2025-12-22-14-16-00.png" alt=""></p>
<h2 id="7-调试与跟踪"><a href="#7-调试与跟踪" class="headerlink" title="7. 调试与跟踪"></a>7. 调试与跟踪</h2><ul>
<li>基于<strong>CoreSight</strong>架构实现以下两种模式<br><img src="2025-12-22-14-29-07.png" alt=""></li>
<li>调试访问端口：DAP</li>
<li>调试接口：DP（一端连接 DAP，一端连接到调试器）</li>
<li>跟踪接口：CoreSight 架构还可以用于<strong>数据跟踪</strong></li>
</ul>
<h2 id="8-Cortex-M3-实用功能"><a href="#8-Cortex-M3-实用功能" class="headerlink" title="8. Cortex-M3 实用功能"></a>8. Cortex-M3 实用功能</h2><h3 id="（1）SysTick-定时器"><a href="#（1）SysTick-定时器" class="headerlink" title="（1）SysTick 定时器"></a>（1）SysTick 定时器</h3><p>SysTick 是集成在内核中的 24 位系统节拍定时器，对实时操作系统（RTOS）至关重要</p>
<ul>
<li><strong>基本机制</strong>：它是一个 24 位的倒计数定时器，当计数减到 0 时，会自动从 RELOAD 寄存器中重装定时初值 </li>
<li><strong>运行状态</strong>：只要不清除 SysTick 控制及状态寄存器中的使能位，定时器就会持续运行 </li>
<li><strong>核心寄存器</strong>：<ul>
<li><strong>CTRL (0xE000E010)</strong>：控制和状态寄存器，用于设置时钟源、使能中断及查询计数标志 </li>
<li><strong>RELOAD (0xE000E014)</strong>：重装载值寄存器，存储定时器溢出后自动加载的初值 </li>
<li><strong>CURRENT (0xE000E018)</strong>：当前值寄存器，反映当前的计数值 </li>
<li><strong>CALIB (0xE000E01C)</strong>：校准值寄存器，提供硬件参考时钟信息 </li>
</ul>
</li>
<li><strong>主要用途</strong>：常用于产生操作系统所需的“滴答”中断，或作为基本的定时/计数工具 </li>
</ul>
<h3 id="（2）电源管理"><a href="#（2）电源管理" class="headerlink" title="（2）电源管理"></a>（2）电源管理</h3><p>Cortex-M3 在内核级别提供了功耗优化方案，支持两种主要的睡眠模式 </p>
<ul>
<li><strong>睡眠模式 (Sleep)</strong>：由内核的 <code>SLEEPING</code> 信号指示 </li>
<li><strong>深度睡眠模式 (Deep Sleep)</strong>：由内核的 <code>SLEEPDEEP</code> 信号指示，用于更大幅度降低功耗 </li>
<li><strong>唤醒机制</strong>：在睡眠状态下，系统时钟可以停止，但通常保持 FCLK 运行，以确保处理器能被 SysTick 异常或外部中断唤醒 </li>
<li><strong>状态判定</strong>：开发者可以通过读取 NVIC（嵌套向量中断控制器）的相关系统控制寄存器来判定当前的睡眠模式及上下文 </li>
</ul>
<h3 id="（3）复位序列"><a href="#（3）复位序列" class="headerlink" title="（3）复位序列"></a>（3）复位序列</h3><p>复位是处理器通电或重启后的起始动作，Cortex-M3 有一套固定的操作流程来定栈和定位代码入口 </p>
<ul>
<li><strong>读取 MSP</strong>：处理器首先从地址 <code>0x00000000</code> 处读取 32 位整数，作为 <strong>主堆栈指针 (MSP)</strong> 的初始值，从而完成“定栈” </li>
<li><strong>读取 PC</strong>：随后从地址 <code>0x00000004</code> 处读取 32 位整数，作为 <strong>程序计数器 (PC)</strong> 的初始值，即复位向量，决定代码执行的入口地址 </li>
<li><strong>堆栈模型</strong>：在 Cortex-M3 中，堆栈是<strong>向下生长</strong>的（即从高地址向低地址方向压栈）</li>
<li><strong>内存布局示例</strong>：例如复位向量指向地址 <code>0x00000101</code>，则处理器将从该处的启动引导代码开始运行</li>
</ul>
<h1 id="第三章-STM32-最小系统及开发环境"><a href="#第三章-STM32-最小系统及开发环境" class="headerlink" title="第三章 STM32 最小系统及开发环境"></a>第三章 STM32 最小系统及开发环境</h1><h2 id="1-系统组成"><a href="#1-系统组成" class="headerlink" title="1. 系统组成"></a>1. 系统组成</h2><ul>
<li><strong>最小系统</strong>：用最少的元件组成微控制器可以工作的系统。<h3 id="（1）微控制器"><a href="#（1）微控制器" class="headerlink" title="（1）微控制器"></a>（1）微控制器</h3></li>
<li>核心芯片工作电压通常为 <strong>2.0V～3.6V</strong><br><img src="2025-12-23-14-58-40.png" alt=""></li>
</ul>
<h3 id="（2）电源电路"><a href="#（2）电源电路" class="headerlink" title="（2）电源电路"></a>（2）电源电路</h3><p><img src="2025-12-23-14-55-05.png" alt=""></p>
<ul>
<li>常用 AMS1117 芯片将 5V 电压降至 3.3V 供 VDD 使用。</li>
<li>STM32 内部的电压调节器将外部 3.3V 的电压转化为 1.8V 提供给 Cortex-M3、内存以及外设使用。</li>
<li>C1、C2 是输入电容，防止断电后电压倒置。</li>
<li>C3、C4 是输出滤波电容，抑制自激振章 &amp; 稳定输出电压。</li>
</ul>
<h3 id="（3）时钟电路"><a href="#（3）时钟电路" class="headerlink" title="（3）时钟电路"></a>（3）时钟电路</h3><ul>
<li>时钟频率越高，单片机运行速度越快，功耗越大。<br><img src="2025-12-23-15-01-33.png" alt=""></li>
<li><strong>高速外部时钟 (HSE)</strong>：通常外接 8MHz 晶振，通过 PLL 倍频最高可达 72MHz 作为系统主时钟。</li>
<li><strong>低速外部时钟 (LSE)</strong>：外接 32.768kHz 晶振，主要用于 RTC 精准计时。</li>
</ul>
<h3 id="（4）复位电路"><a href="#（4）复位电路" class="headerlink" title="（4）复位电路"></a>（4）复位电路</h3><ul>
<li><strong>上电自动复位</strong><ul>
<li>依靠 RC 电路产生的 ~1ms 延迟。</li>
<li>上电瞬间电容两端电压不能突变，Reset 出现短暂低电平。之后芯片复位，进入充电时间：<br>$t=1.1R\times C$（电阻、电容）</li>
</ul>
</li>
</ul>
<ul>
<li><strong>手动按键复位</strong><ul>
<li>按键按下时，Reset 短暂接地，产生低电平</li>
</ul>
</li>
</ul>
<h3 id="（5）调试和下载电路"><a href="#（5）调试和下载电路" class="headerlink" title="（5）调试和下载电路"></a>（5）调试和下载电路</h3><p><strong>调试接口</strong>：JTAG（5引脚）、SWD（2引脚，更常用）<br><strong>Cortex-M3 的三种启动方式，主要依靠不同电平组合：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">BOOT0</th>
<th style="text-align:center">BOOT1</th>
<th style="text-align:left">启动模式</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>X</strong></td>
<td style="text-align:left">用户闪存 (Flash)</td>
<td style="text-align:left">最常用的正常运行模式（从 <code>0x08000000</code> 启动）</td>
</tr>
<tr>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:left">系统存储器</td>
<td style="text-align:left">用于串口下载程序（ISP）</td>
</tr>
<tr>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:left">内置 SRAM</td>
<td style="text-align:left">用于在内存中调试代码</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-开发环境-amp-基础配置"><a href="#2-开发环境-amp-基础配置" class="headerlink" title="2. 开发环境 &amp; 基础配置"></a>2. 开发环境 &amp; 基础配置</h2><p><strong>HAL库开发</strong>： STM32CubeMX+Keil 5</p>
<p><img src="2025-12-23-15-36-48.png" alt=""></p>
<ul>
<li>下载工具：<br> J-Link：通用型，支持多种内核及 IDE 。<br> ST-Link：ST 公司专用于 STM8/STM32，支持全速运行和单步调试 。</li>
</ul>
<h1 id="第四章-嵌入式C语言"><a href="#第四章-嵌入式C语言" class="headerlink" title="第四章 嵌入式C语言"></a>第四章 嵌入式C语言</h1><p><strong>C语言的优势</strong>：汇编语言操作底层、功能性强<br><strong>外设功能模块化设计</strong>：包含一个源文件（.c文件）和一个头文件（.h文件）</p>
<p><img src="2025-12-23-16-49-29.png" alt=""></p>
<h2 id="1-数据类型-amp-运算符"><a href="#1-数据类型-amp-运算符" class="headerlink" title="1. 数据类型 &amp; 运算符"></a>1. 数据类型 &amp; 运算符</h2><h3 id="（1）数据类型"><a href="#（1）数据类型" class="headerlink" title="（1）数据类型"></a>（1）数据类型</h3><ul>
<li>STM32 是32位处理器，与一般64位电脑不同。</li>
</ul>
<p><img src="2025-12-23-17-02-22.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据类型</th>
<th style="text-align:center">字节数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">char</td>
<td style="text-align:center"><strong>1</strong></td>
</tr>
<tr>
<td style="text-align:center">short</td>
<td style="text-align:center"><strong>2</strong></td>
</tr>
<tr>
<td style="text-align:center">int/long/float/<strong>指针</strong></td>
<td style="text-align:center"><strong>4</strong></td>
</tr>
<tr>
<td style="text-align:center">double</td>
<td style="text-align:center"><strong>8</strong></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>1字节 = 8 bit</li>
</ul>
<p><img src="2025-12-23-17-08-26.png" alt=""></p>
<h3 id="（2）运算符"><a href="#（2）运算符" class="headerlink" title="（2）运算符"></a>（2）运算符</h3><p><img src="2025-12-23-17-42-19.png" alt=""></p>
<ul>
<li>三大战术 (背下来)：<ol>
<li>置1 (Set)：用 |。 例如 Reg |= (1 &lt;&lt; 3); (第3位置1)</li>
<li>清0 (Clear)：用 &amp; ~。 例如 Reg &amp;= ~(1 &lt;&lt; 3); (第3位清0)</li>
<li>翻转 (Toggle)：用 ^。 例如 Reg ^= (1 &lt;&lt; 3); (第3位翻转)</li>
</ol>
</li>
</ul>
<p><strong>注意</strong>：“异或”是<strong>不同取1，相同取0</strong>。</p>
<p><img src="2025-12-23-17-45-27.png" alt=""></p>
<h2 id="2-四大核心修饰符"><a href="#2-四大核心修饰符" class="headerlink" title="2. 四大核心修饰符"></a>2. 四大核心修饰符</h2><h3 id="（1）const（只读）"><a href="#（1）const（只读）" class="headerlink" title="（1）const（只读）"></a>（1）const（只读）</h3><ul>
<li><strong>作用</strong>：定义<strong>只读</strong>变量，保证其值在编译时不能改变。</li>
<li><strong>目的</strong>：<strong>防止变量的值被误改</strong>。</li>
<li><strong>要求</strong>：const关键词修饰的变量在声明时<strong>必须初始化</strong>。</li>
<li><strong>属性</strong>：变量是<strong>全局</strong>定义的。</li>
<li><strong>格式</strong>：` const uint32_t(此处为初始化) a = 0xffff1111;</li>
</ul>
<h3 id="（2）static（静态）"><a href="#（2）static（静态）" class="headerlink" title="（2）static（静态）"></a>（2）static（静态）</h3><p><strong>修饰局部变量</strong>：修饰后的变量称为<strong>静态变量</strong>，存储在静态区，函数结束后不销毁，<strong>下次调用保留上次的值</strong>。<br><strong>修饰全局变量/函数</strong>：限制作用域，只在当前.c文件中可见（私有化），其它源文件不能引用，避免了<strong>相同变量名而引发的错误</strong>。</p>
<h3 id="（3）volatile（易变）"><a href="#（3）volatile（易变）" class="headerlink" title="（3）volatile（易变）"></a>（3）volatile（易变）</h3><ul>
<li><strong>作用</strong>：每次<strong>读取</strong>或者<strong>修改</strong>变量值的时候，必须<strong>从内存中重新读取</strong>，而不是使用保存在寄存器里的备份。</li>
</ul>
<p><img src="2025-12-23-17-22-25.png" alt=""></p>
<p><img src="2025-12-23-17-25-16.png" alt=""></p>
<h3 id="（4）extern（外部）"><a href="#（4）extern（外部）" class="headerlink" title="（4）extern（外部）"></a>（4）extern（外部）</h3><p><strong>注意</strong>：extern是一个重新声明，而不是定义。</p>
<ul>
<li><strong>含义</strong>：声明变量/函数是在别的文件定义的。例如：main.c要调用led.c里的变量，必须在main.c里用extern声明。</li>
</ul>
<h2 id="3-struct结构体"><a href="#3-struct结构体" class="headerlink" title="3. struct结构体"></a>3. struct结构体</h2><p><img src="2025-12-24-19-41-44.png" alt=""></p>
<h1 id="第五章-GPIO"><a href="#第五章-GPIO" class="headerlink" title="第五章 GPIO"></a>第五章 GPIO</h1><ul>
<li>GPIO ：General-Purpose Input/Output，即<strong>通用输入/输出</strong>模块。</li>
<li>实现与外部设备的<strong>数字信号交互</strong>，可以通过软件配置输入/输出模式。</li>
</ul>
<h2 id="1-GPIO-内部结构"><a href="#1-GPIO-内部结构" class="headerlink" title="1. GPIO 内部结构"></a>1. GPIO 内部结构</h2><h2 id="2-四种输入模式"><a href="#2-四种输入模式" class="headerlink" title="2. 四种输入模式"></a>2. 四种输入模式</h2><h3 id="（1）上拉输入（Pull-up）"><a href="#（1）上拉输入（Pull-up）" class="headerlink" title="（1）上拉输入（Pull-up）"></a>（1）上拉输入（Pull-up）</h3><ul>
<li>内部接一个上拉电阻到VDD。外部悬空（I/O引脚无信号）时，默认输入<strong>高电平</strong>。</li>
</ul>
<h3 id="（2）下拉输入（Pull-down）"><a href="#（2）下拉输入（Pull-down）" class="headerlink" title="（2）下拉输入（Pull-down）"></a>（2）下拉输入（Pull-down）</h3><ul>
<li>接一个下拉电阻。外部悬空时，默认输入<strong>低电平</strong>。</li>
<li>应用：接按键（按键另一端接地）</li>
</ul>
<h3 id="（3）浮空输入（Floating）"><a href="#（3）浮空输入（Floating）" class="headerlink" title="（3）浮空输入（Floating）"></a>（3）浮空输入（Floating）</h3><ul>
<li>浮空输入模式下引脚内部既不接上拉电阻也不连接下拉电阻（但是不关闭施密特触发器），<strong>直接经施密特触发器输入I/O引脚的信号</strong>。</li>
<li>即：<strong>电平完全由外部电路决定</strong>，如果外部悬空，读到电平会乱跳。</li>
<li>应用：通信协议的接收端（如UART_RX），或者按键检测（外部有电阻时）</li>
</ul>
<h3 id="（4）模拟输入（Analog）"><a href="#（4）模拟输入（Analog）" class="headerlink" title="（4）模拟输入（Analog）"></a>（4）模拟输入（Analog）</h3><ul>
<li>模拟输入模式下，施密特触发器关闭，既不接上拉电阻也不连接下拉电阻，引脚信号连接到芯片内部的片上外设，其典型应用是<strong>A/D模拟输入</strong>，对外部信号进行采集。</li>
<li>应用：ADC采集电压（生物传感器那道大题就用这个！）</li>
</ul>
<h2 id="3-四种输出模式"><a href="#3-四种输出模式" class="headerlink" title="3. 四种输出模式"></a>3. 四种输出模式</h2><h3 id="（1）推挽输出（PP）"><a href="#（1）推挽输出（PP）" class="headerlink" title="（1）推挽输出（PP）"></a>（1）推挽输出（PP）</h3><ul>
<li>两个MOS管按<strong>互补对称</strong>的方式连接。</li>
<li><strong>目的</strong>：增大输出电流，输出能力强；提高电路负载能力和开关速度</li>
<li>高电平3.3V，低电平0V。</li>
<li>应用：点亮LED，驱动蜂鸣器</li>
</ul>
<h3 id="（2）开漏输出（OD）"><a href="#（2）开漏输出（OD）" class="headerlink" title="（2）开漏输出（OD）"></a>（2）开漏输出（OD）</h3><ul>
<li>只有下拉MOS管，没有上拉MOS管。</li>
<li>不与电源连接，处于悬空状态，只能输出低电平。</li>
<li><strong>目的</strong>：减少芯片内部驱动。</li>
<li><strong>特点</strong>：想输出高电平，只能外接上拉电阻。</li>
<li><strong>重点</strong>：方便实现“逻辑与”功能。<code>12C总线</code>（必考关联点）</li>
<li>应用：I2C总线 (必考关联点)</li>
</ul>
<h3 id="（3）复用推挽输出（AF-PP）"><a href="#（3）复用推挽输出（AF-PP）" class="headerlink" title="（3）复用推挽输出（AF_PP）"></a>（3）复用推挽输出（AF_PP）</h3><ul>
<li>GPIO引脚除了作为通用IO引脚外，还可作为片上外设的I/O引脚，即<strong>一个引脚可以作为多个外设引脚使用</strong>，称为复用I/O端口 AFIO。</li>
<li>一个引脚某一时刻<strong>只能使用复用功能中的一个</strong>。</li>
</ul>
<h3 id="（4）复用开漏输出（AF-OD）"><a href="#（4）复用开漏输出（AF-OD）" class="headerlink" title="（4）复用开漏输出（AF_OD）"></a>（4）复用开漏输出（AF_OD）</h3><p>同上。</p>
<h1 id="第六章-中断和异常"><a href="#第六章-中断和异常" class="headerlink" title="第六章 中断和异常"></a>第六章 中断和异常</h1><blockquote>
<p>本质：改变处理器执行指令的顺序。</p>
</blockquote>
<ul>
<li>区别：<ul>
<li>中断：响应外部事件</li>
<li>异常：内部程序错误</li>
</ul>
</li>
</ul>
<p><code>目的</code>：</p>
<ol>
<li>提高cpu效率</li>
<li>实时处理</li>
<li>异常处理</li>
<li>数据传输</li>
<li>不占用cpu资源</li>
</ol>
<h2 id="1-中断处理流程"><a href="#1-中断处理流程" class="headerlink" title="1. 中断处理流程"></a>1. 中断处理流程</h2><p><img src="2025-12-26-14-01-22.png" alt=""><br><img src="2025-12-26-14-01-48.png" alt=""></p>
<h2 id="Cortex-M3-内嵌中断控制器——NVIC"><a href="#Cortex-M3-内嵌中断控制器——NVIC" class="headerlink" title="Cortex-M3 内嵌中断控制器——NVIC"></a>Cortex-M3 内嵌中断控制器——NVIC</h2><p><img src="2025-12-26-14-04-06.png" alt=""></p>
<h3 id="（1）中断优先级"><a href="#（1）中断优先级" class="headerlink" title="（1）中断优先级"></a>（1）中断优先级</h3><p><img src="2025-12-26-14-11-52.png" alt=""></p>
<ul>
<li>抢占优先级 &gt; 响应优先级</li>
<li>数值越小，优先级别越高</li>
<li>Reset，NMI（不可屏蔽中断 ） ， Hard Fault 的优先级为负，且不可修改，高于普通的中断优先级</li>
</ul>
<p><strong>中断优先级判断</strong>：</p>
<ol>
<li>先判断抢占优先级的大小</li>
<li>如果抢占优先级相同，则比较响应优先级的大小</li>
<li>若抢占优先级和响应优先级均相同，则根据<strong>中断向量表</strong>中的顺序来决定<br><img src="2025-12-26-14-16-21.png" alt=""></li>
</ol>
<h3 id="（2）外部中断EXTI"><a href="#（2）外部中断EXTI" class="headerlink" title="（2）外部中断EXTI"></a>（2）外部中断EXTI</h3><p><img src="2025-12-26-14-22-12.png" alt=""><br><img src="2025-12-26-14-22-23.png" alt=""></p>
<ul>
<li>映射关系：STM32的引脚很多，但中断线只有 16+N 条。<br>规则：所有 Pin x 共用一条中断线 EXTI x<ul>
<li>PA0, PB0, PC0 … -&gt; EXTI0</li>
<li>PA5, PB5, PC5 … -&gt; EXTI5</li>
</ul>
</li>
</ul>
<p><em>考点：不能同时开启 PA0 和 PB0 的中断！因为它们都要抢 EXTI0 这条线。</em></p>
<ul>
<li>触发方式<ul>
<li>上升沿触发：电平 0 -&gt; 1 瞬间触发（如按键松开）。</li>
<li>下降沿触发：电平 1 -&gt; 0 瞬间触发（如按键按下）。</li>
<li>双边沿触发：变高变低都触发。</li>
</ul>
</li>
</ul>
<h1 id="第七章-定时器"><a href="#第七章-定时器" class="headerlink" title="第七章 定时器"></a>第七章 定时器</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">定时器主要功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>定时</strong>：时钟脉冲计数</td>
</tr>
<tr>
<td style="text-align:left"><strong>计数</strong>：固定周期的脉冲信号</td>
</tr>
<tr>
<td style="text-align:left"><strong>输入捕获</strong>：对脉冲信号宽度测量</td>
</tr>
<tr>
<td style="text-align:left"><strong>输出比较</strong>：控制输出波形</td>
</tr>
</tbody>
</table>
</div>
<h2 id="第一部分：STM32定时器模块"><a href="#第一部分：STM32定时器模块" class="headerlink" title="第一部分：STM32定时器模块"></a>第一部分：STM32定时器模块</h2><p><img src="2025-12-28-11-32-59.png" alt=""></p>
<p><em>注意分辨不同定时器的<strong>功能</strong>、引脚</em></p>
<ul>
<li><p><strong>基本定时器 (TIM6, TIM7)</strong></p>
<ul>
<li>能力：只能用来定时（数数），没有 GPIO 引脚，不能 产生 PWM。</li>
<li><em>就像个纯粹的闹钟。</em></li>
</ul>
</li>
<li><p><strong>通用定时器 (TIM2 ~ TIM5)</strong></p>
<ul>
<li>能力：定时 + 输入捕获 (测脉宽) + 输出比较 (PWM)。</li>
<li><em>这是最常用的主力。</em></li>
</ul>
</li>
<li><p><strong>高级定时器 (TIM1, TIM8)</strong></p>
<ul>
<li>能力：通用功能 + 死区控制 (Dead-time) + 互补输出。</li>
<li><em>这是给电机控制专用的。</em></li>
</ul>
</li>
</ul>
<h3 id="1-定时时间计算"><a href="#1-定时时间计算" class="headerlink" title="1. 定时时间计算"></a>1. 定时时间计算</h3><script type="math/tex; mode=display">
时钟源+预分频器PSC+计数器CNT+自动装载寄存器ARR</script><p><code>核心公式</code>：</p>
<script type="math/tex; mode=display">
定时时间 =\frac{(ARR+1)\times(PSC+1)}{时钟频率}</script><ul>
<li><em>注意：PSC 和 ARR 的输入范围为（1，65536），因为最高16位。</em></li>
</ul>
<p><strong>例题</strong>：<br><img src="2025-12-28-11-46-51.png" alt=""></p>
<h3 id="2-PWM-相关分析题"><a href="#2-PWM-相关分析题" class="headerlink" title="2. PWM 相关分析题"></a>2. PWM 相关分析题</h3><p><img src="2025-12-28-11-55-12.png" alt=""><br><img src="2025-12-28-11-57-21.png" alt=""></p>
<script type="math/tex; mode=display">
占空比 =\frac{CCR}{ARR+1}\times 100\%</script><p><img src="2025-12-28-11-59-12.png" alt=""><br><img src="2025-12-28-11-59-26.png" alt=""></p>
<ul>
<li>只要 <strong>CNT&gt;CCR</strong> ，就输出高电平。</li>
<li>ARR 是阈值，达到后<strong>重新计数</strong>。</li>
<li><strong>ARR 决定PWM的周期，CCR 决定占空比</strong>。</li>
</ul>
<h2 id="第二部分：SysTick定时器"><a href="#第二部分：SysTick定时器" class="headerlink" title="第二部分：SysTick定时器"></a>第二部分：SysTick定时器</h2><p><strong>1. 基础属性 (填空/判断题核心)</strong></p>
<ul>
<li><strong>归属</strong>：属于 <strong>Cortex-M3 内核 (Core)</strong>，<strong>不属于</strong> 片上外设 (Peripheral)。</li>
<li><strong>位数</strong>：<strong>24位</strong> (注意：通用定时器通常是 16 位)。</li>
<li><strong>计数方式</strong>：<strong>向下计数</strong> (Down-counter)，从设定值 (LOAD) 减到 0。</li>
</ul>
<p><strong>2. 核心用途 (考干什么)</strong></p>
<ul>
<li><strong>操作系统心跳</strong>：为 RTOS (如 FreeRTOS, uCOS) 提供任务调度的基准时钟。</li>
<li><strong>精准延时</strong>：<strong>HAL_Delay()</strong> 函数就是基于 SysTick 实现的。<ul>
<li><em>注：如果不开启 SysTick 中断，HAL_Delay 函数将无法工作。</em></li>
</ul>
</li>
</ul>
<p><strong>3. 关键寄存器 (眼熟即可)</strong></p>
<ul>
<li><strong>CTRL (Control)</strong>：控制寄存器，用于使能定时器、开启中断。</li>
<li><strong>LOAD (Reload)</strong>：重装载数值寄存器，决定了定时器的溢出周期。</li>
<li><strong>VAL (Value)</strong>：当前数值寄存器，实时显示当前的计数值。</li>
</ul>
<p><strong>4. 考点辨析：SysTick vs 通用定时器</strong></p>
<ul>
<li><strong>位置区别</strong>：SysTick 在内核；通用定时器在芯片外设区。</li>
<li><strong>位数区别</strong>：SysTick 是 24 位；通用定时器通常是 16 位。</li>
<li><strong>方向区别</strong>：SysTick 只能向下计数；通用定时器可以向上/向下/中心对齐。</li>
<li><strong>功能区别</strong>：SysTick 专注延时/心跳；通用定时器专注 PWM、输入捕获、脉冲计数。</li>
</ul>
<p><strong>看门狗</strong>：确保系统可靠稳定运行，可使得应用程序脱离正常执行流程时复位。<br><img src="2025-12-28-12-21-39.png" alt=""></p>
<h1 id="第八章-USART通信协议技术"><a href="#第八章-USART通信协议技术" class="headerlink" title="第八章 USART通信协议技术"></a>第八章 USART通信协议技术</h1><h2 id="第一部分-基础概念"><a href="#第一部分-基础概念" class="headerlink" title="第一部分 基础概念"></a>第一部分 基础概念</h2><p><strong>按传输格式划分</strong>：</p>
<ul>
<li><strong>并行</strong>：多条独立数据线同时传输。</li>
<li><strong>串行</strong>：单条/两条数据线逐位传输。（<em>最为流行，eg.U盘、USB接口设备、<code>USART</code></em>）</li>
</ul>
<p><strong>按同步方式划分</strong>：</p>
<ul>
<li><strong>异步</strong>：发送方&amp;接收方 无统一时钟线。</li>
<li><strong>同步</strong>：发送方&amp;接收方 统一时钟线。</li>
</ul>
<p><strong>按传输方式划分</strong>：</p>
<ul>
<li><strong>单工通信</strong>：数据传输时单向的。（<em>广播、电视、打印机、BB机</em>）</li>
<li><strong>半双工通信</strong>：可以双向，但不能同时。（<em>对讲机</em>）</li>
<li><strong>全双工通信</strong>：可以同时双向传输。（<em>电话、以太网、<code>USART</code></em>）</li>
</ul>
<h2 id="第二部分：波特率"><a href="#第二部分：波特率" class="headerlink" title="第二部分：波特率"></a>第二部分：波特率</h2><p><strong>波特率</strong>：每秒传输的二进制位数（bit/s），衡量<strong>传输速度快慢</strong>。</p>
<script type="math/tex; mode=display">
波特率 =字符速率（每秒传输字符数）\times 每个字符包含位数 =\frac{f_{PCLK}}{16\times 分频系数}</script><ul>
<li><em>其中 $f_{PCLK}$为外设<strong>时钟频率</strong></em>。</li>
</ul>
<p><strong>例题</strong>：<br><img src="2025-12-28-14-13-16.png" alt=""></p>
<p><strong>常用波特率</strong>：</p>
<ul>
<li>9600</li>
<li>115200（<em>调试用、速度快</em>）<br><em>注意：异步通信双方波特率要一致</em></li>
</ul>
<h2 id="第三部分：异步串行通信协议（波形分析）"><a href="#第三部分：异步串行通信协议（波形分析）" class="headerlink" title="第三部分：异步串行通信协议（波形分析）"></a>第三部分：异步串行通信协议（波形分析）</h2><p><img src="2025-12-28-14-31-00.png" alt=""></p>
<ul>
<li><code>注意：数据位要从右向左读</code>  </li>
</ul>
<p><img src="2025-12-28-14-33-40.png" alt=""></p>
<blockquote>
<p>UART：全双工、异步、串行。<br><img src="2025-12-28-14-35-57.png" alt=""></p>
</blockquote>
<h2 id="第四部分：USART内部结构"><a href="#第四部分：USART内部结构" class="headerlink" title="第四部分：USART内部结构"></a>第四部分：USART内部结构</h2><ol>
<li><strong>波特率发生器</strong>：提供同步时钟信号。<br>$波特率时钟频率 =波特率\times 采样倍数$</li>
<li><strong>发送器</strong>：将并行数据转化为串行数据，并按照（起始位、数据位、校验位、停止位）发送到<strong>TX引脚</strong>。</li>
<li><strong>接收器</strong>：从<strong>RX引脚</strong>接收串行数据，转化为并行数据。</li>
</ol>
<p><img src="2025-12-28-14-46-41.png" alt=""></p>
<ul>
<li><strong>USART1～3</strong>:全支持</li>
<li><strong>USART4～5</strong>:除去<em>同步模式、硬件流控制、智能卡</em></li>
</ul>
<p><img src="2025-12-28-14-50-23.png" alt=""></p>
<h1 id="第九章-DMA控制器"><a href="#第九章-DMA控制器" class="headerlink" title="第九章 DMA控制器"></a>第九章 DMA控制器</h1><h2 id="第一部分：基础概念"><a href="#第一部分：基础概念" class="headerlink" title="第一部分：基础概念"></a>第一部分：基础概念</h2><p><strong>DMA（Direct Memory Access）</strong>：直接内存访问。</p>
<ul>
<li><strong>作用</strong>：允许外设设备直接与储存器进行数据交换，<strong>无需cpu介入</strong>。</li>
<li><strong>优势</strong>：<strong>解放cpu、提高传输效率、降低功耗</strong>。</li>
</ul>
<p><img src="2025-12-29-12-42-37.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">DMA适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>储存设备I/O</strong>：硬盘与内存间的大量数据交换</td>
</tr>
<tr>
<td style="text-align:left"><strong>高速通信通道</strong>：网络接口、光纤通信</td>
</tr>
<tr>
<td style="text-align:left"><strong>数据采集系统</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>图像处理</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>多处理机系统</strong></td>
</tr>
</tbody>
</table>
</div>
<h2 id="第二部分：STM32多DMA架构"><a href="#第二部分：STM32多DMA架构" class="headerlink" title="第二部分：STM32多DMA架构"></a>第二部分：STM32多DMA架构</h2><p><img src="2025-12-29-13-22-16.png" alt=""></p>
<p><img src="2025-12-29-13-22-38.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">关键配置参数</th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>传输方向</strong></td>
<td style="text-align:left"><strong>外设/内存</strong>之间相互传输（4种情况的排列组合）</td>
</tr>
<tr>
<td style="text-align:left"><code>数据宽度</code></td>
<td style="text-align:left">8/16/32 （<strong>如果源和目标位数不匹配，DMA会自动帮你补位</strong>）</td>
</tr>
<tr>
<td style="text-align:left"><strong>地址递增</strong></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><strong>传输模式</strong></td>
<td style="text-align:left">单次模式、循环模式</td>
</tr>
<tr>
<td style="text-align:left"><strong>中断请求</strong></td>
<td style="text-align:left">传输完成、半传输、传输错误</td>
</tr>
</tbody>
</table>
</div>
<p><strong>传输模式</strong>：</p>
<ul>
<li>单次模式：DMA传输结束后不再产生新的DMA操作。</li>
<li>循环模式：每轮传输结束时，要传输的数据数量将自动用设置的初始值进行加载，并继续响应DMA请求。<em>（ADC扫描模式）</em></li>
</ul>
<h2 id="第三部分：通道映射"><a href="#第三部分：通道映射" class="headerlink" title="第三部分：通道映射"></a>第三部分：通道映射</h2><h3 id="DMA优先级"><a href="#DMA优先级" class="headerlink" title="DMA优先级"></a>DMA优先级</h3><p><img src="2025-12-29-13-41-55.png" alt=""></p>
<h3 id="DMA请求通道"><a href="#DMA请求通道" class="headerlink" title="DMA请求通道"></a>DMA请求通道</h3><p><img src="2025-12-29-13-43-34.png" alt=""><br><img src="2025-12-29-13-43-45.png" alt=""></p>
<h3 id="DMA中断请求"><a href="#DMA中断请求" class="headerlink" title="DMA中断请求"></a>DMA中断请求</h3><p><img src="2025-12-29-13-44-50.png" alt=""></p>
<h1 id="第十章-同步串行通信协议"><a href="#第十章-同步串行通信协议" class="headerlink" title="第十章 同步串行通信协议"></a>第十章 同步串行通信协议</h1><h2 id="第一部分：SPI通信协议"><a href="#第一部分：SPI通信协议" class="headerlink" title="第一部分：SPI通信协议"></a>第一部分：SPI通信协议</h2><h3 id="1-基础概念"><a href="#1-基础概念" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h3><ul>
<li><strong>SPI（Serial Peripheral interface）</strong>：串行外围设备接口。</li>
<li><p><strong>特点</strong>：</p>
<ul>
<li>高速、全双工、同步</li>
<li>在芯片管脚上只占用四根线</li>
<li><em>主要应用：EEPROM、FLASH、实时时钟、AD转换器、数字信号处理器</em></li>
</ul>
</li>
<li><p><strong>主从模式</strong>：</p>
<ul>
<li>SPI通讯系统包含一个（只能一个）主设备、多个或一个从设备。</li>
<li>主设备<strong>提供时钟</strong>，从设备<strong>接收时钟</strong>。</li>
<li>多个从设备<strong>通过各自片选信号管理</strong>。</li>
<li>读写操作都是由主设备发起。</li>
<li>通信速度一般能达到10Mbps。</li>
</ul>
</li>
</ul>
<h3 id="2-四根信号线"><a href="#2-四根信号线" class="headerlink" title="2. 四根信号线"></a>2. 四根信号线</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">引脚名称</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">MOSI（master output slave input）</td>
<td style="text-align:left">主设备输出/从设备输入。即：在主模式下发送数据，从模式下接收数据</td>
</tr>
<tr>
<td style="text-align:left">MISO（master input slave output）</td>
<td style="text-align:left">主设备输入/从设备输出。即：在主模式下接收数据，从模式下发送数据</td>
</tr>
<tr>
<td style="text-align:left">SCLK</td>
<td style="text-align:left">主设备产生的串行时钟信号。</td>
</tr>
<tr>
<td style="text-align:left">CS/SS</td>
<td style="text-align:left">从设备的<strong>片选信号</strong>，低电平有效。让主设备可以单独与指定从设备通讯$\rightarrow$避免冲突。</td>
</tr>
</tbody>
</table>
</div>
<p><img src="2025-12-29-15-28-07.png" alt=""></p>
<p><img src="2025-12-29-15-30-32.png" alt=""></p>
<h3 id="3-四种配置模式"><a href="#3-四种配置模式" class="headerlink" title="3. 四种配置模式"></a>3. 四种配置模式</h3><ul>
<li>通信双方必须在同一模式下才可以通信。</li>
<li>通过<code>CPOL（时钟极性）</code>和<code>CPHA（时钟相位）</code>来配置主设备的通信模式。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">SPI模式</th>
<th style="text-align:left">CPOL</th>
<th style="text-align:left">CPHA</th>
<th style="text-align:left">空闲时SCK时钟</th>
<th style="text-align:left">采样时刻</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">0（最常用）</td>
<td style="text-align:left">0</td>
<td style="text-align:left">0</td>
<td style="text-align:left">低电平</td>
<td style="text-align:left">奇数边沿</td>
</tr>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">0</td>
<td style="text-align:left">1</td>
<td style="text-align:left">低电平</td>
<td style="text-align:left">偶数边沿</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">1</td>
<td style="text-align:left">0</td>
<td style="text-align:left">高电平</td>
<td style="text-align:left">奇数边沿</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">1</td>
<td style="text-align:left">1</td>
<td style="text-align:left">高电平</td>
<td style="text-align:left">偶数边沿</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>CPOL（clock polarity）</strong>：=0:SLK低电平/=1:SLK高电平</li>
<li><strong>CPHA（clock phase）</strong>：=0:奇数沿采样/=1:偶数沿采样</li>
</ul>
<p><img src="2025-12-29-15-46-11.png" alt=""></p>
<ul>
<li>SCK一开始是低电平$\rightarrow$CPOL=0</li>
<li>偶数沿采样（注意不要看图片里的数标）$\rightarrow$CPAL=1</li>
<li>得到$\longrightarrow$模式=1</li>
</ul>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
  <entry>
    <title>人工智能原理与应用</title>
    <url>/2025/12/11/AI/</url>
    <content><![CDATA[<h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><p>智能：智能指人类在认识客观世界中，由思维过程和脑<br>力活动所表现出的综合能力。</p>
<h2 id="智能包含的能力"><a href="#智能包含的能力" class="headerlink" title="智能包含的能力"></a>智能包含的能力</h2><p>  <img src="2025-12-11-19-04-02.png" alt=""></p>
<h2 id="AI的研究目标"><a href="#AI的研究目标" class="headerlink" title="AI的研究目标"></a>AI的研究目标</h2><p>  <img src="2025-12-11-19-05-39.png" alt=""></p>
<h2 id="三大学派"><a href="#三大学派" class="headerlink" title="三大学派"></a>三大学派</h2><p>  <img src="2025-12-11-19-07-05.png" alt=""><br>  <img src="2025-12-11-19-07-54.png" alt=""></p>
<h2 id="新的发展"><a href="#新的发展" class="headerlink" title="新的发展"></a>新的发展</h2><p>  <img src="2025-12-11-19-08-50.png" alt=""></p>
<h2 id="智能模拟方法"><a href="#智能模拟方法" class="headerlink" title="智能模拟方法"></a>智能模拟方法</h2><p>  <img src="2025-12-11-19-12-03.png" alt=""></p>
<h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><p>  <strong>推理</strong>：是指按照某种策略从已知事实出发利用知识推出所需结论的过程。<br>  <strong>推理方法</strong>：归纳、演绎、（不）确定性推理</p>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>  <strong>搜索</strong>：依靠经验，利用已有知识，根据问题的实际情况，不断寻找可利用知识，从而构造一条代价最小的推理路线，使问题得以解决的过程。<br>  <strong>智能搜索</strong>：利用搜索过程中得到的信息来引导搜索向最优方向发展的算法。</p>
<h2 id="神经元（画图）"><a href="#神经元（画图）" class="headerlink" title="神经元（画图）"></a>神经元（画图）</h2><p>  <img src="2025-12-11-19-17-46.png" alt=""></p>
<h2 id="大数据的特性："><a href="#大数据的特性：" class="headerlink" title="大数据的特性："></a>大数据的特性：</h2><ul>
<li><strong>规模性、多样性、实时性、价值性</strong></li>
</ul>
<h2 id="机器视觉的流程：图像获取-rightarrow-图像解释"><a href="#机器视觉的流程：图像获取-rightarrow-图像解释" class="headerlink" title="机器视觉的流程：图像获取$\rightarrow$图像解释"></a>机器视觉的流程：图像获取$\rightarrow$图像解释</h2><p>  <img src="2025-12-11-19-20-13.png" alt=""></p>
<h2 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h2><p>  <img src="2025-12-11-19-21-05.png" alt=""></p>
<h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><p>  <img src="2025-12-11-19-21-26.png" alt=""></p>
<h2 id="智能控制"><a href="#智能控制" class="headerlink" title="智能控制"></a>智能控制</h2><p>  <img src="2025-12-11-19-21-48.png" alt=""></p>
<h2 id="人工智能工程"><a href="#人工智能工程" class="headerlink" title="人工智能工程"></a>人工智能工程</h2><p>  <img src="2025-12-11-19-23-37.png" alt=""><br>  <img src="2025-12-11-19-24-19.png" alt=""></p>
<h2 id="人工智能面临的问题"><a href="#人工智能面临的问题" class="headerlink" title="人工智能面临的问题"></a>人工智能面临的问题</h2><ol>
<li>道德伦理问题</li>
<li>法律法规的制定问题</li>
<li>安全问题（信息安全、交通安全、人身安全）</li>
<li>稀缺数据资源条件下的学习</li>
<li>目前人工智能属于弱人工智能</li>
</ol>
<h2 id="未来发展趋势"><a href="#未来发展趋势" class="headerlink" title="未来发展趋势"></a>未来发展趋势</h2><ol>
<li>从专用智能向通用是能发展</li>
<li>从人工智能向人机融合发展</li>
<li>自动化AI技术</li>
<li>AI药物的研发</li>
<li>减少对数据的需求</li>
<li>可解释性和鲁棒性</li>
</ol>
<h1 id="第二章-数学基础"><a href="#第二章-数学基础" class="headerlink" title="第二章 数学基础"></a>第二章 数学基础</h1><h2 id="向量的范数"><a href="#向量的范数" class="headerlink" title="向量的范数"></a>向量的范数</h2><p>1) 1-范数</p>
<script type="math/tex; mode=display">\|\alpha\|_1 = |\alpha_1| + |\alpha_2| + \cdots + |\alpha_n|</script><p>2) 2-范数</p>
<script type="math/tex; mode=display">\|\alpha\|_2 = \sqrt{\alpha_1^2 + \alpha_2^2 + \cdots + \alpha_n^2}</script><p>3) $\infty$-范数</p>
<script type="math/tex; mode=display">\|\alpha\|_\infty = \max\{|\alpha_1|, |\alpha_2|, \cdots, |\alpha_n|\}</script><p>4) p-范数</p>
<script type="math/tex; mode=display">\|\alpha\|_p = \left( \sum_{i=1}^{n} |\alpha_i|^p \right)^{1/p}, \quad p \ge 1</script><h2 id="矩阵的范数"><a href="#矩阵的范数" class="headerlink" title="矩阵的范数"></a>矩阵的范数</h2><p>1) Frobenius 范数 (F-范数)</p>
<script type="math/tex; mode=display">\|X\|_F = \left( \sum_{i,j=1}^{n} |x_{ij}|^2 \right)^{1/2} = \left( tr(X^H X) \right)^{1/2}</script><p>2) 1-范数</p>
<script type="math/tex; mode=display">\|X\|_1 = \max_{j} \sum_{i=1}^{n} |x_{ij}|</script><p>3) 2-范数</p>
<script type="math/tex; mode=display">\|X\|_2 = \sqrt{\lambda} \text{，其中 } \lambda \text{ 为 } X^H X \text{ 的最大特征值}</script><p>4) $\infty$-范数</p>
<script type="math/tex; mode=display">\|X\|_\infty = \max_{i} \sum_{j=1}^{n} |x_{ij}|</script><p>其中，$X^H$ 为 $X$ 的共轭转置矩阵。</p>
<h1 id="第三章-确定性知识系统"><a href="#第三章-确定性知识系统" class="headerlink" title="第三章 确定性知识系统"></a>第三章 确定性知识系统</h1><ul>
<li><strong>知识</strong>是改造客观世界中积累起来的认识和经验。</li>
<li>知识 = 信息 + 关联</li>
</ul>
<h2 id="知识的类型"><a href="#知识的类型" class="headerlink" title="知识的类型"></a>知识的类型</h2><p><img src="2025-12-11-22-06-18.png" alt=""></p>
<h2 id="知识的作用效果"><a href="#知识的作用效果" class="headerlink" title="知识的作用效果"></a>知识的作用效果</h2><p><img src="2025-12-11-22-08-05.png" alt=""></p>
<h2 id="知识的表示要求"><a href="#知识的表示要求" class="headerlink" title="知识的表示要求"></a>知识的表示要求</h2><ul>
<li>可组织性与可维护性</li>
<li>可理解性与可实现性</li>
</ul>
<h2 id="知识的类型-1"><a href="#知识的类型-1" class="headerlink" title="知识的类型"></a>知识的类型</h2><p><img src="2025-12-11-22-15-11.png" alt=""></p>
<h2 id="推理方法及其分类"><a href="#推理方法及其分类" class="headerlink" title="推理方法及其分类"></a>推理方法及其分类</h2><ol>
<li>演绎推理：<br><img src="2025-12-11-22-17-36.png" alt=""></li>
<li>归纳推理<br><img src="2025-12-11-22-22-55.png" alt=""><br><img src="2025-12-11-22-23-09.png" alt=""><br><strong>演绎推理 &amp; 归纳推理区别</strong>：<br><img src="2025-12-11-22-22-30.png" alt=""></li>
</ol>
<h2 id="推理控制策略"><a href="#推理控制策略" class="headerlink" title="推理控制策略"></a>推理控制策略</h2><ul>
<li>推理的控制策略是使推理过程尽快达到目标。</li>
<li>分为两类：推理策略和搜索策略</li>
</ul>
<p><strong>冲突消解策略</strong>：当推理过程有多条知识可用时，如何从这多条可用知识中选出一条<strong>最佳知识</strong>用于推理的策略，常用的冲突消解策略有<em>领域知识优先和新鲜知识优先</em>等。</p>
<h1 id="第四章-不确定性知识系统"><a href="#第四章-不确定性知识系统" class="headerlink" title="第四章 不确定性知识系统"></a>第四章 不确定性知识系统</h1><h2 id="知识的不确定性表示"><a href="#知识的不确定性表示" class="headerlink" title="知识的不确定性表示"></a>知识的不确定性表示</h2><p><img src="2025-12-11-22-30-33.png" alt=""><br><img src="2025-12-11-22-30-45.png" alt=""></p>
<h2 id="主观贝叶斯推理"><a href="#主观贝叶斯推理" class="headerlink" title="主观贝叶斯推理"></a>主观贝叶斯推理</h2><h2 id="贝叶斯网络原理"><a href="#贝叶斯网络原理" class="headerlink" title="贝叶斯网络原理"></a>贝叶斯网络原理</h2><p><img src="2025-12-11-22-31-50.png" alt=""></p>
<h1 id="第五章-智能搜索技术"><a href="#第五章-智能搜索技术" class="headerlink" title="第五章 智能搜索技术"></a>第五章 智能搜索技术</h1><h2 id="状态空间问题的求解"><a href="#状态空间问题的求解" class="headerlink" title="状态空间问题的求解"></a>状态空间问题的求解</h2><p><img src="2025-12-12-18-20-32.png" alt=""><br><img src="2025-12-12-18-20-56.png" alt=""></p>
<h2 id="与或树"><a href="#与或树" class="headerlink" title="与或树"></a>与或树</h2><ul>
<li><strong>分解</strong>：$P 归纳为 P_i, P_i 都有解的时候，P 有解。$</li>
<li><strong>等价变换</strong>：$P 可以等价变为 P_i，有一个 P_i 有解，P 有解。$</li>
</ul>
<p><img src="2025-12-12-18-28-38.png" alt=""></p>
<ul>
<li>本原问题：不能再进行分解或者变换，可以直接解答的问题</li>
<li>端节点：没有子节点的节点</li>
<li>终止节点：本原问题对应的节点 <em>（终止节点一定是端节点，端节点不一定是终止节点）</em></li>
<li>可解问题：<ul>
<li>任何终止节点</li>
<li>或树中有一个可解</li>
<li>与树中都有解</li>
</ul>
</li>
</ul>
<h2 id="进化搜索"><a href="#进化搜索" class="headerlink" title="进化搜索"></a>进化搜索</h2><p><img src="2025-12-12-18-36-12.png" alt=""><br><strong>进化计算的生物学基础</strong>：</p>
<ol>
<li>遗传理论</li>
<li>变异理论</li>
<li>进化论</li>
</ol>
<h2 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h2><p><img src="2025-12-12-18-38-36.png" alt=""></p>
<ul>
<li>八数码难题：</li>
</ul>
<p><img src="2025-12-12-18-39-09.png" alt=""></p>
<h2 id="A-算法"><a href="#A-算法" class="headerlink" title="A 算法"></a>A 算法</h2><p><img src="2025-12-12-18-40-15.png" alt=""><br><img src="2025-12-12-18-41-00.png" alt=""><br><img src="2025-12-12-18-41-22.png" alt=""></p>
<h2 id="与或树的启发式搜索"><a href="#与或树的启发式搜索" class="headerlink" title="与或树的启发式搜索"></a>与或树的启发式搜索</h2><p><img src="2025-12-12-18-41-55.png" alt=""></p>
<h2 id="博弈树"><a href="#博弈树" class="headerlink" title="博弈树"></a>博弈树</h2><p><img src="2025-12-12-18-42-41.png" alt=""><br><img src="2025-12-12-18-43-02.png" alt=""></p>
<h2 id="进化搜索（这一部分看PPT理解）"><a href="#进化搜索（这一部分看PPT理解）" class="headerlink" title="进化搜索（这一部分看PPT理解）"></a>进化搜索（这一部分看PPT理解）</h2><ul>
<li>蚁群算法</li>
<li>遗传算法</li>
</ul>
<ol>
<li>基本步骤：<br><img src="2025-12-13-16-33-40.png" alt=""></li>
<li>三角函数的遗传算法优化（重点看一下）（17-20课时）</li>
<li>轮盘赌：<br><img src="2025-12-13-16-38-57.png" alt=""></li>
</ol>
<ul>
<li>粒子群优化算法<br>看ppt（21-24课时），重点求解tsp问题的步骤。</li>
</ul>
<p><strong>群智能的优点</strong>：</p>
<ol>
<li>灵活性：群体可以随时适应变化的环境。</li>
<li>稳健性：即使个体失败，群体仍可以完成任务；自我组织；活动既不受中央控制，也不受局部监督。</li>
</ol>
<h1 id="第六章-机器学习"><a href="#第六章-机器学习" class="headerlink" title="第六章 机器学习"></a>第六章 机器学习</h1><h2 id="信息熵与信息增益"><a href="#信息熵与信息增益" class="headerlink" title="信息熵与信息增益"></a>信息熵与信息增益</h2><p><img src="2025-12-13-16-44-14.png" alt=""><br><img src="2025-12-13-16-44-31.png" alt=""></p>
<h2 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h2><ul>
<li><strong>测试属性</strong>：<code>信息增益</code>  </li>
</ul>
<p><strong>具体计算流程</strong>：（设$C_1+C_2=D$）</p>
<ol>
<li><p><strong>计算不考虑特征的信息熵</strong>：</p>
<script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k= -\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}</script><p>其中 $|D|$ 为样本总数，$|C_k|$ 为属于第 $k$ 类的样本数(eg: <em>买/不买</em>)。</p>
</li>
<li><p><strong>计算条件熵</strong>：</p>
<ul>
<li>先选择一个 <strong>特征 $A_i$</strong>，将数据集 $D$ 根据特征 $A_i$ 的<strong>不同取值</strong>划分为 $n$ 个子集 $D_1, D_2, \dots, D_n$。</li>
<li>计算特征 $A_i$ 下不同取值 $D_i$ 的信息熵 $H(D_i)$：<script type="math/tex; mode=display">H(D_i)= -\sum_{k=1}^{K} \frac{|C_k|}{|D_i|} \log_2 \frac{|C_k|}{|D_i|}</script>其中，$C_k$ 是在集合 $D_i$ 里的第 $k$ 种类别的数量 (eg: <em>买/不买</em>)。</li>
<li>计算条件熵 $H(D|A_i)$：<script type="math/tex; mode=display">H(D|A_i) = \sum_{i=1}^{n} p_i H(D_i)= \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)</script></li>
</ul>
</li>
<li><p><strong>计算信息增益</strong>：</p>
<script type="math/tex; mode=display">g(D, A_i) = H(D) - H(D|A_i)</script></li>
<li><p><strong>特征选择</strong>：<br>选取 <strong>信息增益最大</strong> 的特征$A_i$作为当前的分裂节点（根节点或内部节点）。</p>
</li>
<li><p><strong>递归构建</strong>：<br>对子节点重复以上步骤，以此类推，直至分类完全（或满足停止条件）。</p>
</li>
</ol>
<h2 id="集成学习（Boosting-Bagging）"><a href="#集成学习（Boosting-Bagging）" class="headerlink" title="集成学习（Boosting/Bagging）"></a>集成学习（Boosting/Bagging）</h2><p><img src="2025-12-13-16-47-29.png" alt=""><br><img src="2025-12-13-16-47-43.png" alt=""></p>
<h2 id="回归的含义、思想"><a href="#回归的含义、思想" class="headerlink" title="回归的含义、思想"></a>回归的含义、思想</h2><p><img src="2025-12-13-16-50-09.png" alt=""><br><img src="2025-12-13-16-50-28.png" alt=""></p>
<h2 id="总体回归函数"><a href="#总体回归函数" class="headerlink" title="总体回归函数"></a>总体回归函数</h2><p><img src="2025-12-13-16-51-14.png" alt=""><br><img src="2025-12-13-16-51-28.png" alt=""><br><img src="2025-12-13-16-51-39.png" alt=""></p>
<h2 id="计算题：最小二乘估计-OLS"><a href="#计算题：最小二乘估计-OLS" class="headerlink" title="计算题：最小二乘估计 $OLS$"></a>计算题：最小二乘估计 $OLS$</h2><ul>
<li>看ppt（25-28）</li>
</ul>
<h1 id="第七章-支持向量机"><a href="#第七章-支持向量机" class="headerlink" title="第七章 支持向量机"></a>第七章 支持向量机</h1><ul>
<li>看模式识别的笔记。</li>
</ul>
<h1 id="第八章-人工神经网络与连接学习"><a href="#第八章-人工神经网络与连接学习" class="headerlink" title="第八章 人工神经网络与连接学习"></a>第八章 人工神经网络与连接学习</h1><p>后面看三个ppt吧。</p>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
  <entry>
    <title>模式识别与统计学习</title>
    <url>/2025/11/27/statistical-learning/</url>
    <content><![CDATA[<p>考试心得：</p>
<ul>
<li><strong>考的很简单，完全没必要花很多精力复习</strong></li>
</ul>
<h1 id="第一章-统计学习方法概述"><a href="#第一章-统计学习方法概述" class="headerlink" title="第一章 统计学习方法概述"></a>第一章 统计学习方法概述</h1><blockquote>
<p>❝ 机器学习是一门人工智能的科学, 该领域的主要研究对象是人工智能, 特别是如何在经验学习中改善具体算法的性能。 ❞</p>
<p>—— 机器学习是对能通过经验自动改进的计算机算法的研究。</p>
</blockquote>
<h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><ul>
<li><strong>概念</strong>：计算机基于数据构建概率统计<strong>模型</strong>，运用<strong>模型</strong>对数据进行预测和分析。</li>
<li><strong>研究对象</strong>：数据、基本假设。</li>
<li><strong>目的</strong>：对数据（特别是未知数据）进行预测和分析。</li>
</ul>
<p><strong>🤔 为什么需要机器学习？</strong></p>
<ol>
<li><strong>数据量太大</strong>：人脑无法处理海量维度。</li>
<li><strong>知识盲区</strong>：人类无法解释专业知识，或专业知识本身不存在。</li>
</ol>
<hr>
<h3 id="发展历程与三要素"><a href="#发展历程与三要素" class="headerlink" title="发展历程与三要素"></a>发展历程与三要素</h3><p>模式识别与机器学习发展历程：<br><img src="2025-11-27-15-37-49.png" alt=""></p>
<h3 id="核心：统计学三要素"><a href="#核心：统计学三要素" class="headerlink" title="核心：统计学三要素"></a>核心：统计学三要素</h3><script type="math/tex; mode=display">\text{方法} = \text{模型} + \text{策略} + \text{算法}</script><p><strong>1. 模型 (Model)</strong></p>
<ul>
<li><strong>非概率模型</strong>：<ul>
<li>假设空间：决策函数 $F=\{f \mid Y=f(X)\}$</li>
<li>参数空间：$F=\{f \mid Y=f_\theta(X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
<li><strong>概率模型</strong>：<ul>
<li>假设空间：条件概率 $F=\{P \mid P(Y|X)\}$</li>
<li>参数空间：$F=\{P \mid P_\theta(Y|X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
</ul>
<p><strong>2. 策略 (Strategy)</strong></p>
<ul>
<li><strong>损失函数</strong>：度量<strong>一次</strong>预测的好坏。</li>
<li><strong>风险函数</strong>：度量平均意义下的好坏（损失函数的期望）。<script type="math/tex; mode=display">R(f)=E_P[L(Y,f(X))]=\int L(y,f(x))P(x,y)dxdy</script></li>
</ul>
<p><strong>3. 算法 (Algorithm)</strong></p>
<ul>
<li>求解得到最优模型（通常指最优化问题的求解算法）。</li>
</ul>
<hr>
<h3 id="分类体系"><a href="#分类体系" class="headerlink" title="分类体系"></a>分类体系</h3><ul>
<li><strong>学习分类</strong>：监督学习、无监督学习、强化学习 （具体概念）</li>
<li><strong>模型分类</strong>：(非)概率、(非)线性、(非)参数、生成/判别<br><img src="2025-12-16-14-01-34.png" alt=""><br><img src="2025-12-16-14-01-48.png" alt=""></li>
</ul>
<hr>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><strong>⚡️ 快速记忆口诀：</strong></p>
<blockquote>
<p><span style="background-color:#fff5f5; color:#ff5050; padding:2px 6px; border-radius:4px;">先看预测，再看结果</span></p>
<ul>
<li>预测为<strong>真</strong> $\rightarrow$ <strong>P</strong> (Positive)；预测为<strong>假</strong> $\rightarrow$ <strong>N</strong> (Negative)</li>
<li>预测<strong>正确</strong> $\rightarrow$ <strong>T</strong> (True)；预测<strong>错误</strong> $\rightarrow$ <strong>F</strong> (False)</li>
</ul>
</blockquote>
<p><strong>核心指标：</strong></p>
<ul>
<li><strong>精确率 (Precision)</strong>：$P=\frac{TP}{TP+FP}$ <em>（你认为是对的里面，有多少真是对的？）</em></li>
<li><strong>召回率 (Recall)</strong>：$R=\frac{TP}{TP+FN}$ <em>（把精确率的 FP 换成 FN）</em></li>
<li><strong>F1值</strong>：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}$</li>
</ul>
<hr>
<h2 id="过拟合-Overfitting"><a href="#过拟合-Overfitting" class="headerlink" title="过拟合 (Overfitting)"></a>过拟合 (Overfitting)</h2><p>在<strong>训练数据</strong>上表现很好（把噪声也学进去了），对于<strong>未知数据</strong>表现很差。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>正则化</li>
<li>增大数据集</li>
<li>早停机制</li>
<li>减少模型参数</li>
</ul>
<h2 id="欠拟合-Underfitting"><a href="#欠拟合-Underfitting" class="headerlink" title="欠拟合 (Underfitting)"></a>欠拟合 (Underfitting)</h2><p><strong>模型过于简单</strong>，未能学习到数据中的基本规律，导致在<strong>训练集和未知数据</strong>上表现均不理想。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>增加模型复杂度</li>
<li>增加特征数量</li>
<li>减少或移除正则化参数</li>
<li>延长训练时间/充分训练  </li>
</ul>
<h1 id="第二章-线性感知机"><a href="#第二章-线性感知机" class="headerlink" title="第二章 线性感知机"></a>第二章 线性感知机</h1><h2 id="1-超平面-Hyperplane"><a href="#1-超平面-Hyperplane" class="headerlink" title="1. 超平面 (Hyperplane)"></a>1. 超平面 (Hyperplane)</h2><h3 id="怎么理解？"><a href="#怎么理解？" class="headerlink" title="怎么理解？"></a>怎么理解？</h3><blockquote>
<p><strong>定义</strong>：在 $n$ 维空间中，将空间一分为二的 $n-1$ 维子空间。</p>
</blockquote>
<ul>
<li><strong>超平面方程</strong>：<script type="math/tex; mode=display">w \cdot x + b = 0</script></li>
</ul>
<h3 id="点到超平面的几何距离"><a href="#点到超平面的几何距离" class="headerlink" title="点到超平面的几何距离"></a>点到超平面的几何距离</h3><script type="math/tex; mode=display">d = \frac{|w \cdot x + b|}{\|w\|}</script><hr>
<h2 id="2-感知机模型-Perceptron"><a href="#2-感知机模型-Perceptron" class="headerlink" title="2. 感知机模型 (Perceptron)"></a>2. 感知机模型 (Perceptron)</h2><blockquote>
<p><strong>关键词</strong>：<code>线性二分模型、判别模型</code></p>
</blockquote>
<h3 id="1-模型"><a href="#1-模型" class="headerlink" title="(1) 模型"></a>(1) 模型</h3><ul>
<li><strong>输入空间</strong>：$x \in R^n$</li>
<li><strong>输出空间</strong>：$y \in \{+1, -1\}$</li>
<li><p><strong>假设空间</strong>：</p>
<script type="math/tex; mode=display">f(x) = sign(w \cdot x + b)</script><p><strong>符号函数 (Sign Function)</strong>：</p>
<script type="math/tex; mode=display">
sign(x) =
\begin{cases}
+1, & x \geq 0 \\
-1, & x < 0
\end{cases}</script></li>
<li><p><strong>模型参数</strong>：</p>
<ul>
<li>$w$：权值向量</li>
<li>$b$：偏置</li>
<li>$w \cdot x$：内积</li>
</ul>
</li>
</ul>
<h3 id="2-策略"><a href="#2-策略" class="headerlink" title="(2) 策略"></a>(2) 策略</h3><p>感知机的目标是<strong>最小化误分类点到超平面的总距离。</strong></p>
<ul>
<li><strong>损失函数</strong>（误分类点集合 $M$）：<script type="math/tex; mode=display">
  L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script>  <em>(注：由于 $\frac{1}{|w|}$ 不影响梯度方向，为了计算简便，在损失函数中省略)</em></li>
</ul>
<h3 id="3-算法"><a href="#3-算法" class="headerlink" title="(3) 算法"></a>(3) 算法</h3><p>转化为求解最优化问题：</p>
<script type="math/tex; mode=display">
\min_{w, b} L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script><hr>
<h2 id="3-算法实现形式"><a href="#3-算法实现形式" class="headerlink" title="3. 算法实现形式"></a>3. 算法实现形式</h2><h3 id="1-梯度下降法"><a href="#1-梯度下降法" class="headerlink" title="(1) 梯度下降法"></a>(1) 梯度下降法</h3><ul>
<li><strong>输入</strong>：训练数据集 $T = {(x_1, y_1), \dots, (x_N, y_N)}$，学习率 $\eta$ ($0 &lt; \eta \le 1$)</li>
<li><strong>输出</strong>：$w, b$</li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$w_0 \leftarrow 0, b_0 \leftarrow 0$，$\eta$看题目</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件 $y_i (w \cdot x_i + b) \le 0$，则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
w &\leftarrow w + \eta y_i x_i \\
b &\leftarrow b + \eta y_i
\end{aligned}</script></li>
<li><strong>循环</strong>：转至 (2)，直至训练集中没有误分类点。</li>
</ol>
<blockquote>
<p><strong>特点</strong>：直观，但如果数据量大或维度高，计算量大。</p>
</blockquote>
<h3 id="2-对偶形式"><a href="#2-对偶形式" class="headerlink" title="(2) 对偶形式"></a>(2) 对偶形式</h3><ul>
<li><strong>核心思想</strong>：将 $w$ 表示为实例 $x_i$ 和标记 $y_i$ 的线性组合。</li>
<li><strong>输出</strong>：$\alpha, b$</li>
<li><strong>假设空间变为</strong>：<script type="math/tex; mode=display">f(x) = sign\left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right)</script>  <em>(其中 $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_N)^T$)</em></li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$\alpha \leftarrow 0, b \leftarrow 0$</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件：<script type="math/tex; mode=display">
y_i \left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right) \le 0</script>则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
\alpha_i &\leftarrow \alpha_i + \eta \\
b &\leftarrow b + \eta y_i
\end{aligned}</script>（$b$和梯度下降一样）</li>
<li><strong>循环</strong>：转至 (2) 直到无误分类数据。</li>
</ol>
<h4 id="💡-重点：如何计算判别式："><a href="#💡-重点：如何计算判别式：" class="headerlink" title="💡 重点：如何计算判别式："></a>💡 重点：如何计算判别式：</h4><p>在对偶形式中，样本之间的内积计算非常频繁，通常预先计算 <strong>Gram 矩阵</strong> 并存储：</p>
<script type="math/tex; mode=display">
G = [x_i \cdot x_j]_{N \times N} =
\begin{bmatrix}
x_1 \cdot x_1 & x_1 \cdot x_2 & \cdots & x_1 \cdot x_N \\
x_2 \cdot x_1 & x_2 \cdot x_2 & \cdots & x_2 \cdot x_N \\
\vdots & \vdots & \ddots & \vdots \\
x_N \cdot x_1 & x_N \cdot x_2 & \cdots & x_N \cdot x_N \\
\end{bmatrix}</script><p><strong>注意：$x_i$ 有几个，$\alpha_i$ 就有几个 $=0$（初始化）</strong><br><strong>做题技巧（表格法迭代）</strong>：<br><img src="2025-11-30-18-07-41.png" alt=""></p>
<hr>
<h2 id="4-感知机算法收敛性定理"><a href="#4-感知机算法收敛性定理" class="headerlink" title="4. 感知机算法收敛性定理"></a>4. 感知机算法收敛性定理</h2><ul>
<li><p><strong>收敛定理 (Novikoff)</strong>：<br>  误分类次数 $k$ 有上界。这意味着，<strong>只要训练数据集是线性可分的，感知机算法一定能收敛</strong>（在有限步内找到解）。</p>
</li>
<li><p><strong>局限性</strong>：</p>
<ol>
<li><strong>解不唯一</strong>：解依赖于初值的选择，也依赖于误分类点的选择顺序。</li>
<li><strong>震荡</strong>：如果数据集<strong>线性不可分</strong>，算法会一直震荡，无法做到完全正确分类。</li>
</ol>
</li>
<li><p><strong>下一步引申</strong>：<br>  为了得到<strong>唯一</strong>且<strong>最优</strong>的分离超平面，我们需要引入更强的约束 —— 这就是 <strong>SVM (支持向量机)</strong> 的由来。  </p>
</li>
</ul>
<h1 id="第三章-K-近邻"><a href="#第三章-K-近邻" class="headerlink" title="第三章 $K$近邻"></a>第三章 $K$近邻</h1><h2 id="1-算法简介"><a href="#1-算法简介" class="headerlink" title="1. 算法简介"></a>1. 算法简介</h2><blockquote>
<p><strong>关键词</strong>：<code>分类模型、回归模型、适用于(数值型、标称型)数据、1-NN训练错误率=0</code></p>
</blockquote>
<h3 id="算法特点及优缺点"><a href="#算法特点及优缺点" class="headerlink" title="算法特点及优缺点"></a>算法特点及优缺点</h3><h4 id="懒惰学习-Lazy-Learning"><a href="#懒惰学习-Lazy-Learning" class="headerlink" title="懒惰学习 (Lazy Learning)"></a>懒惰学习 (Lazy Learning)</h4><blockquote>
<p><strong>定义</strong>：<strong>无实质性模型训练</strong>，利用训练集数据对特征空间进行划分。测试时，要计算输入实例距每个样本的距离。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优点 (Pros)</th>
<th style="text-align:left">缺点 (Cons)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>精确度高</strong></td>
<td style="text-align:left"><strong>时间复杂度高</strong>（花时间）</td>
</tr>
<tr>
<td style="text-align:left"><strong>对异常值不敏感</strong></td>
<td style="text-align:left"><strong>空间复杂度高</strong>（耗内存）</td>
</tr>
<tr>
<td style="text-align:left"><strong>无数据输入假定</strong>（无训练阶段）</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
</div>
<h3 id="K-值的选择"><a href="#K-值的选择" class="headerlink" title="$K$ 值的选择"></a>$K$ 值的选择</h3><ul>
<li><p><strong>$K$ 值较小</strong> (模型复杂)</p>
<ul>
<li>近似误差（<em>偏差</em>）减小 $\rightarrow$ 估计误差（<em>方差</em>）增大（对噪声敏感）</li>
<li><strong>噪声敏感</strong></li>
<li>整体模型变得复杂，容易发生<strong>过拟合</strong></li>
</ul>
</li>
<li><p><strong>$K$ 值较大</strong> (模型简单)</p>
<ul>
<li>近似误差增大 $\rightarrow$ 估计误差减小</li>
<li>整体的模型变得简单，但容易发生<strong>欠拟合</strong></li>
</ul>
</li>
<li><p><strong>通过交叉验证选择 $K$ 值</strong></p>
<ul>
<li>模型准确率随 $K$ 值的变化<strong>非单调</strong></li>
<li>$K$ 一般低于训练集样本容量的平方根：$k &lt; \sqrt{N}$</li>
<li>经验上，$k$ 值一般不超过 20：$k &lt; 20$</li>
</ul>
</li>
</ul>
<h3 id="kNN-算法的应用挑战"><a href="#kNN-算法的应用挑战" class="headerlink" title="$kNN$ 算法的应用挑战"></a>$kNN$ 算法的应用挑战</h3><ul>
<li><strong>距离函数确定</strong><ul>
<li>距离度量方式众多</li>
</ul>
</li>
<li><strong>$k$ 值确定</strong><ul>
<li>$k$ 值对模型精度的影响<strong>非单调</strong></li>
<li>边界易出错 (软分类)</li>
</ul>
</li>
<li><strong>特征的选择</strong><ul>
<li>不同特征的影响程度不同</li>
</ul>
</li>
<li><strong>复杂度</strong><ul>
<li>需要计算测试例 $x’$ 与所有训练数据的距离</li>
<li>计算复杂度正比于训练样本容量</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-距离度量"><a href="#2-距离度量" class="headerlink" title="2. 距离度量"></a>2. 距离度量</h2><h3 id="L-p-距离"><a href="#L-p-距离" class="headerlink" title="$L_p$ 距离"></a>$L_p$ 距离</h3><ul>
<li><p><strong>欧氏距离 ($L_2$)</strong>：</p>
<script type="math/tex; mode=display">L_2(x_i, x_j) = \left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^2\right)^{\frac{1}{2}}</script></li>
<li><p><strong>曼哈顿距离 ($L_1$)</strong>：</p>
<script type="math/tex; mode=display">L_1(x_i, x_j) = \sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|</script></li>
<li><p><strong>切比雪夫距离 ($L_{\infty}$)</strong>：（取同维度坐标差最大者）</p>
<script type="math/tex; mode=display">L_{\infty}(x_i, x_j) = \max_{l} |x_i^{(l)} - x_j^{(l)}|</script></li>
</ul>
<h3 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h3><ul>
<li>了解即可</li>
</ul>
<h3 id="距离度量的性质"><a href="#距离度量的性质" class="headerlink" title="距离度量的性质"></a>距离度量的性质</h3><ul>
<li><strong>非负性 (non-negativity)</strong>:<script type="math/tex; mode=display">D(a, b) \ge 0</script></li>
<li><strong>自反性 (reflexivity)</strong>:<script type="math/tex; mode=display">D(a, b) = 0 \iff a = b</script></li>
<li><strong>对称性 (symmetry)</strong>:<script type="math/tex; mode=display">D(a, b) = D(b, a)</script></li>
<li><strong>三角不等式 (triangle inequality)</strong>:<script type="math/tex; mode=display">D(a, b) + D(b, c) \ge D(a, c)</script></li>
</ul>
<hr>
<h2 id="3-分类决策规则"><a href="#3-分类决策规则" class="headerlink" title="3. 分类决策规则"></a>3. 分类决策规则</h2><ul>
<li><strong>核心规则</strong>：多数表决规则。</li>
<li><strong>理论意义</strong>：等价于 $0-1$ 损失函数下的<strong>经验风险最小化</strong>。</li>
<li><strong>误分类概率</strong>：在一个领域中，与领域样本特征不同的总数 $\div$ 领域样本总数。</li>
</ul>
<hr>
<h2 id="4-kd-树"><a href="#4-kd-树" class="headerlink" title="4. kd 树"></a><del>4. kd 树</del></h2><h3 id="搜索方法"><a href="#搜索方法" class="headerlink" title="搜索方法"></a>搜索方法</h3><p><img src="2025-12-01-15-48-55.png" alt=""></p>
<p><em>(注：$kd$ 树是二叉树)</em>  </p>
<ol>
<li>先在 $x$ 轴找到中位数，垂直划分。</li>
<li>再在划分的两块区域里找 $y$ 轴中位数，继续划分。</li>
<li>直至划分完毕。</li>
</ol>
<h3 id="kd-树计算复杂度"><a href="#kd-树计算复杂度" class="headerlink" title="$kd$ 树计算复杂度"></a>$kd$ 树计算复杂度</h3><ul>
<li><strong>一般情况</strong>：时间复杂度为 $O(\log N)$</li>
<li><strong>最坏情况</strong>：时间复杂度 $O(N)$</li>
<li><strong>适用场景</strong>：更适用于<strong>训练实例维度远大于空间维度</strong>时的搜索。~~</li>
</ul>
<h1 id="第四章-贝叶斯方法"><a href="#第四章-贝叶斯方法" class="headerlink" title="第四章 贝叶斯方法"></a>第四章 贝叶斯方法</h1><h2 id="1-概率统计基础知识"><a href="#1-概率统计基础知识" class="headerlink" title="1. 概率统计基础知识"></a>1. 概率统计基础知识</h2><h3 id="pmf-amp-pdf"><a href="#pmf-amp-pdf" class="headerlink" title="$pmf$ &amp; $pdf$"></a>$pmf$ &amp; $pdf$</h3><p><strong>概率质量函数（Probability mass function）</strong><br>定义：$P(X=x_i)$，即<strong>离散随机变量</strong>在各特定取值上的概率。</p>
<ul>
<li>$\sum_{i} P(X=x_i) = 1$</li>
<li>$P(X=x_i \cap X=x_j) = 0 \text{ if } i \neq j$</li>
<li>$P(X=x_i \cup X=x_j) = P(X=x_i) + P(X=x_j) \text{ if } i \neq j$</li>
<li>$P(X=x_1 \cup X=x_2 \cup \dots \cup X=x_k) = 1$</li>
</ul>
<p><strong>概率密度函数（Probability density function）</strong><br>定义：描述了<strong>连续随机变量</strong> $x$ 的概率密度。</p>
<ul>
<li>$f(x) \ge 0, \forall x$</li>
<li>$\int_{-\infty}^{+\infty} f(x) = 1$</li>
<li>真实概率通过 pdf 的积分得到</li>
<li>e.g. $X$ 取 0-1 的概率为：<script type="math/tex; mode=display">P(0 \le X \le 1) = \int_{0}^{1} f(x) dx</script></li>
</ul>
<h3 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h3><p>描述两个或多个随机变量<strong>同时</strong>发生的概率。</p>
<ul>
<li><strong>离散型</strong>：$P(X=x, Y=y)$<ul>
<li>性质：$\sum_x \sum_y P(X=x, Y=y) = 1$</li>
</ul>
</li>
<li><strong>连续型</strong>：$f(x, y)$<ul>
<li>性质：$\iint f(x, y) dx dy = 1$</li>
</ul>
</li>
<li><strong>边缘分布 (Marginalization)</strong>：从联合分布中求出单个变量的分布。<script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x, Y=y)</script></li>
<li><strong>独立性</strong>：若 $X$ 与 $Y$ 相互独立，则：<script type="math/tex; mode=display">P(X, Y) = P(X)P(Y)</script></li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>定义：在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。</p>
<script type="math/tex; mode=display">P(A|B) = \frac{P(A, B)}{P(B)}</script><p>（其中 $P(A, B)$ 是联合概率，$P(B)$ 是边缘概率）</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>将联合概率分解为条件概率的乘积。<br>对于两个变量：</p>
<script type="math/tex; mode=display">P(A, B) = P(A|B)P(B) = P(B|A)P(A)</script><p>推广到 $n$ 个变量：</p>
<script type="math/tex; mode=display">P(X_1, X_2, \dots, X_n) = P(X_1) \prod_{i=2}^{n} P(X_i | X_1, \dots, X_{i-1})</script><h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>如果事件组 $B_1, B_2, \dots, B_n$ 构成样本空间的一个划分（互斥且完备），则对于任意事件 $A$：</p>
<script type="math/tex; mode=display">P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i)</script><ul>
<li><strong>直观理解</strong>：求 $A$ 发生的概率，等于把 $A$ 在各种不同情况（$B_i$）下发生的概率加权求和。</li>
</ul>
<h3 id="均值与方差"><a href="#均值与方差" class="headerlink" title="均值与方差"></a>均值与方差</h3><ul>
<li><strong>期望 (Expectation / Mean)</strong> $\mu$：<ul>
<li>离散：$E[X] = \sum x_i P(x_i)$</li>
<li>连续：$E[X] = \int_{-\infty}^{+\infty} x f(x) dx$</li>
<li>性质：$E[aX + b] = aE[X] + b$</li>
</ul>
</li>
<li><strong>方差 (Variance)</strong> $\sigma^2$：衡量数据的离散程度。<script type="math/tex; mode=display">Var(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2</script></li>
</ul>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>定义：衡量两个变量 $X$ 和 $Y$ 总体误差的期望，反映它们之间变化的<strong>方向</strong>。</p>
<script type="math/tex; mode=display">Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)]</script><ul>
<li><strong>$Cov &gt; 0$</strong>：正相关</li>
<li><strong>$Cov &lt; 0$</strong>：负相关</li>
<li><strong>$Cov = 0$</strong>：不相关（线性无关）</li>
</ul>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p><strong>皮尔逊相关系数 (Correlation Coefficient)</strong> $\rho$：<br>将协方差归一化，消除了量纲的影响，衡量线性相关的强度。</p>
<script type="math/tex; mode=display">\rho_{X,Y} = \frac{Cov(X, Y)}{\sigma_x \sigma_y}</script><ul>
<li>取值范围：$[-1, 1]$。<br>（$1$ 为完全正相关，$-1$ 为完全负相关，$0$ 为无线性相关）</li>
</ul>
<h2 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2. 朴素贝叶斯"></a>2. 朴素贝叶斯</h2><h3 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h3><ul>
<li><strong><code>贝叶斯公式 (Bayes&#39; Theorem)</code></strong>：<script type="math/tex; mode=display">P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}</script><ul>
<li>$P(Y|X)$: 后验概率 (Posterior)</li>
<li>$P(X|Y)$: 似然 (Likelihood)</li>
<li>$P(Y)$: 先验概率 (Prior)</li>
<li>$P(X)$: 证据 (Evidence)</li>
</ul>
</li>
<li><strong>贝叶斯决策</strong>：<ul>
<li>已知<strong>类条件概率密度</strong>参数表达式和<strong>先验概率</strong></li>
<li>利用<strong>贝叶斯公式</strong>转换成<strong>后验概率</strong></li>
<li>根据<strong>后验概率</strong>大小进行决策分类</li>
</ul>
</li>
</ul>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>基于贝叶斯定理与特征条件独立假设的分类算法。</p>
<ul>
<li><strong>“朴素” (Naive) 的含义</strong>：假设样本的各个特征之间<strong>相互独立</strong>。<script type="math/tex; mode=display">P(x_1, x_2, \dots, x_n | y) = \prod_{i=1}^{n} P(x_i | y)</script></li>
</ul>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>给定输入特征向量 $\mathbf{x} = (x_1, x_2, \dots, x_n)$，预测类别 $y$。<br>我们只需最大化分子部分<code>(这个公式考试一定要写)</code>：</p>
<script type="math/tex; mode=display">\hat{y} = \arg\max_{y} P(y) \prod_{i=1}^{n} P(x_i | y)</script><ul>
<li>$P(y)$：先验概率，即类别的频率。</li>
<li>$P(x_i|y)$：条件概率（似然），即在某类中观察到该特征的概率。</li>
</ul>
<h3 id="朴素贝叶斯参数估计"><a href="#朴素贝叶斯参数估计" class="headerlink" title="朴素贝叶斯参数估计"></a>朴素贝叶斯参数估计</h3><ol>
<li><strong>极大似然估计 (MLE)</strong><br>直接使用频率来估计概率。<ul>
<li>$P(y = c_k) = \frac{N_{c_k}}{N}$</li>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik}}{N_{c_k}}$</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i}{N}$</li>
</ul>
</li>
<li><strong>贝叶斯估计 (Bayes)</strong><br>引入平滑项（如拉普拉斯平滑）解决零概率问题。<ul>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}$</li>
<li>其中 $\lambda \ge 0$ 为平滑系数（常取 1），$S_i$ 为特征 $x_i$ 可能取值的个数。</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i+\alpha}{N+\alpha+\beta}$ ($\hat{\theta}$ ~ $Beta(\alpha,\beta)$)</li>
</ul>
</li>
<li><strong>拉普拉斯平滑 (Laplace Smoothing)</strong><br>解决零概率问题（即某个特征在训练集中未出现导致概率为0）。<script type="math/tex; mode=display">P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}</script><ul>
<li>$\lambda$: 平滑系数（通常取 1）。</li>
<li>$S_i$: 特征 $x_i$ 可能取值的个数。</li>
</ul>
</li>
</ol>
<h1 id="第五章-决策树"><a href="#第五章-决策树" class="headerlink" title="第五章 决策树"></a>第五章 决策树</h1><h2 id="1-决策树模型"><a href="#1-决策树模型" class="headerlink" title="1. 决策树模型"></a>1. 决策树模型</h2><h3 id="决策树的表示"><a href="#决策树的表示" class="headerlink" title="决策树的表示"></a>决策树的表示</h3><ul>
<li>一种描述对实例分类的 <strong>树形结构</strong>，包含：<ul>
<li><strong>根结点 (Root Node)</strong>：最上面的结点，是整个决策树的开始。</li>
<li><strong>内部结点 (Internal Node)</strong>：代表一个<strong>问题</strong>或者<strong>决策</strong>，通常对应待<strong>分类对象的属性</strong>。</li>
<li><strong>叶结点 (Leaf Node)</strong>：代表一种可能的<strong>分类结果</strong>。</li>
<li><strong>有向边</strong>：连接各个结点。</li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>可解释性强</strong>：推理过程容易理解，可以表示成 <code>If-Then</code> 形式。</li>
<li><strong>依赖属性取值</strong>：推理过程完全依赖于属性变量的取值特点。</li>
<li><strong>特征筛选</strong>：可自动忽略对目标变量<strong>没有贡献的属性变量</strong>，为判断属性重要性、减少变量数目提供参考。</li>
</ul>
<h3 id="学习目的"><a href="#学习目的" class="headerlink" title="学习目的"></a>学习目的</h3><ul>
<li><strong>理想的决策树</strong>（通常有以下3种目标）：<ol>
<li>叶结点数最少</li>
<li>叶结点深度最小</li>
<li>叶结点数最少且叶结点深度最小</li>
</ol>
</li>
<li><strong>现实困境</strong>：找到这种绝对最优的决策树是 <strong>NP难题</strong>。</li>
<li><strong>实际目标</strong>：找到 <strong>尽可能</strong> 趋向于最优的决策树。</li>
</ul>
<hr>
<h2 id="2-特征选择与信息熵"><a href="#2-特征选择与信息熵" class="headerlink" title="2. 特征选择与信息熵"></a>2. 特征选择与信息熵</h2><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h3><ul>
<li><strong>定义</strong>：度量样本集合 <strong>纯度 (Purity)</strong> 最常用的一种指标，代表随机变量<strong>不确定性的度量</strong>。<blockquote>
<p><strong>规律</strong>：熵越大，随机变量的不确定性就越大。</p>
</blockquote>
</li>
<li><strong>计算公式</strong>：<br>  设 $X$ 是一个取有限个值的离散随机变量，其概率分布为 $P(X=x_i) = p_i, \quad i=1,2,\dots,n$，则随机变量 $X$ 的熵 $H(X)$ 定义为：<script type="math/tex; mode=display">H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i</script>  <strong>注</strong>：若 $p_i=0$，则定义 $0 \log 0 = 0$。</li>
</ul>
<h3 id="条件熵-Conditional-Entropy"><a href="#条件熵-Conditional-Entropy" class="headerlink" title="条件熵 (Conditional Entropy)"></a>条件熵 (Conditional Entropy)</h3><ul>
<li><strong>定义</strong>：表示在已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。</li>
<li><strong>计算公式</strong>：<script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)</script>  其中 $p_i = P(X=x_i)$。</li>
</ul>
<h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益 (Information Gain)"></a>信息增益 (Information Gain)</h3><ul>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差。<script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A)</script></li>
<li><strong>物理意义</strong>：由于特征 $A$ 而使得对数据集 $D$ 的分类不确定性减少的程度。<strong>信息增益大的特征具有更强的分类能力。</strong></li>
</ul>
<h3 id="信息增益比-Gain-Ratio"><a href="#信息增益比-Gain-Ratio" class="headerlink" title="信息增益比 (Gain Ratio)"></a>信息增益比 (Gain Ratio)</h3><ul>
<li><strong>背景</strong>：信息增益倾向于选择 <strong>取值较多</strong> 的特征（例如唯一的 ID 号），这往往不是我们想要的。</li>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>  其中，<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>   $n$ 是特征 $A$ 取值的个数。</li>
</ul>
<hr>
<h2 id="3-经典算法"><a href="#3-经典算法" class="headerlink" title="3. 经典算法"></a>3. 经典算法</h2><h3 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="$ID3$ 算法"></a>$ID3$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益</code>  </li>
</ul>
<p><strong>具体计算流程</strong>：（设$C_1+C_2=D$）</p>
<ol>
<li><p><strong>计算不考虑特征的信息熵</strong>：</p>
<script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k= -\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}</script><p>其中 $|D|$ 为样本总数，$|C_k|$ 为属于第 $k$ 类的样本数(eg: <em>买/不买</em>)。</p>
</li>
<li><p><strong>计算条件熵</strong>：</p>
<ul>
<li>先选择一个 <strong>特征 $A_i$</strong>，将数据集 $D$ 根据特征 $A_i$ 的<strong>不同取值</strong>划分为 $n$ 个子集 $D_1, D_2, \dots, D_n$。</li>
<li>计算特征 $A_i$ 下不同取值 $D_i$ 的信息熵 $H(D_i)$：<script type="math/tex; mode=display">H(D_i)= -\sum_{k=1}^{K} \frac{|C_k|}{|D_i|} \log_2 \frac{|C_k|}{|D_i|}</script>其中，$C_k$ 是在集合 $D_i$ 里的第 $k$ 种类别的数量 (eg: <em>买/不买</em>)。</li>
<li>计算条件熵 $H(D|A_i)$：<script type="math/tex; mode=display">H(D|A_i) = \sum_{i=1}^{n} p_i H(D_i)= \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)</script></li>
</ul>
</li>
<li><p><strong>计算信息增益</strong>：</p>
<script type="math/tex; mode=display">g(D, A_i) = H(D) - H(D|A_i)</script></li>
<li><p><strong>特征选择</strong>：<br>选取 <strong>信息增益最大</strong> 的特征$A_i$作为当前的分裂节点（根节点或内部节点）。</p>
</li>
<li><p><strong>递归构建</strong>：<br>对子节点重复以上步骤，以此类推，直至分类完全（或满足停止条件）。</p>
</li>
</ol>
<h3 id="C4-5-算法"><a href="#C4-5-算法" class="headerlink" title="$C4.5$ 算法"></a>$C4.5$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益比</code> （除<strong>选择特征的标准不同</strong>外，其余步骤与 $ID3$ <strong>一致</strong>）。</li>
<li>计算完<strong>信息增益</strong>后，计算<strong>信息增益比</strong>：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>其中，<strong>特征固有值</strong>：<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>这里 $|D|$ 是样本总数，注意和 $H(D_i)$ 的计算区分。</li>
</ul>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><ul>
<li><strong>现象</strong>：决策树算法增长树的每一个分支的深度，直到恰好能对训练样例比较完美地分类。</li>
<li><strong>原因</strong>：当数据中 <strong>有噪声</strong> 或 <strong>训练样例数量太少</strong>，以至于不能产生目标函数的有代表性的采样时，简单算法产生的树会过拟合训练样例。</li>
</ul>
<hr>
<h2 id="4-CART-算法"><a href="#4-CART-算法" class="headerlink" title="4. $CART$算法"></a>4. $CART$算法</h2><h3 id="基尼指数-Gini-Index"><a href="#基尼指数-Gini-Index" class="headerlink" title="基尼指数 (Gini Index)"></a>基尼指数 (Gini Index)</h3><ul>
<li><strong>定义</strong>：度量数据纯度的指标。<strong>基尼指数越小，模型的不确定性越小，纯度越高</strong>。</li>
<li><p><strong>计算公式</strong>：<br>  假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$，则概率分布的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K} p_k^2</script><p>  对于给定的样本集合 $D$，其基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D) = 1 - \sum_{k=1}^{K} \left(\frac{|C_k|}{|D|}\right)^2</script><p>  其中 $C_k$ 是 $D$ 中属于第 $k$ 类的样本子集。</p>
</li>
<li><p><strong>特征下的基尼指数</strong>：（一分为二的划分问题）<br>  如果特征 $A$ 的取值将集合 $D$ 划分成 $D_1$ 和 $D_2$ 两部分（<strong>注意：CART 生成的是二叉树</strong>），则在特征 $A$ 的条件下，集合 $D$ 的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D, A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)</script><p>  <img src="2025-12-04-20-06-28.png" alt=""><br>  <img src="2025-12-04-20-06-41.png" alt=""><br>  <img src="2025-12-04-20-06-57.png" alt=""></p>
</li>
<li><strong>选择标准</strong>：选择使得 <strong>基尼指数最小</strong> 的<strong>特征</strong>及其<strong>划分点</strong>作为最优特征和最优切分点。<img src="2025-12-04-20-08-35.png" alt=""><img src="2025-12-04-20-10-08.png" alt=""></li>
</ul>
<h3 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h3><ul>
<li><del><strong>回归树</strong>：使用 <strong>平方误差最小化</strong> 准则。</del></li>
<li><strong>分类树</strong>：使用 <strong>基尼指数最小化</strong> 准则。</li>
<li><strong>具体生成流程 (分类树)</strong>：<em>（上一部分的图片例子可以帮助理解）</em><ol>
<li>对训练数据集 $D$ 的每个特征 $A$，以及该特征的每个可能取值 $a$，根据 $A=a$ 与 $A \neq a$ 将 $D$ 分割为 $D_1$ 和 $D_2$。</li>
<li>计算该切分下的 $Gini(D, A)$。</li>
<li>在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择 <strong>基尼指数最小</strong> 的特征及其切分点作为最优特征与最优切分点。</li>
<li>生成两个子结点，将训练数据集依特征分配到两个子结点中。</li>
<li>对子结点递归地调用上述步骤，直到满足停止条件（如结点样本数小于阈值、基尼指数小于阈值或没有更多特征）。</li>
</ol>
</li>
</ul>
<h3 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h3><ul>
<li><strong>目的</strong>：防止过拟合。</li>
<li><strong>手段</strong>：剪枝，限定叶节点最小样本数，强制决策树最大深度，交叉验证（$CART$剪枝）。</li>
</ul>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><ul>
<li>在决策树生成过程中，对每个结点在 <strong>划分前先进行估计</strong>。</li>
<li>若当前结点的划分不能带来决策树 <strong>泛化性能提升</strong>，则停止划分，并标记当前结点为叶结点。</li>
</ul>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><ul>
<li>从训练集先生成一棵 <strong>完整的决策树</strong>。</li>
<li>自底而上地考察非叶结点，若将该结点对应的子树替换为叶结点能带来决策树 <strong>泛化性能提升</strong>，则 <strong>将该子树替换为叶结点</strong>。</li>
<li>CART 常用 <strong>CCP (Cost-Complexity Pruning, 代价复杂度剪枝)</strong> 方法。</li>
</ul>
<blockquote>
<p><strong>对比</strong>：一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能优于预剪枝决策树。但训练时间开销比未剪枝决策树和预剪枝决策树要大得多。</p>
</blockquote>
<hr>
<h2 id="5-随机森林-Random-Forest"><a href="#5-随机森林-Random-Forest" class="headerlink" title="5. 随机森林 (Random Forest)"></a><del>5. 随机森林 (Random Forest)</del></h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>一种基于 <strong>Bagging</strong>（装袋法）的集成学习方法。</li>
<li>通过构建多棵决策树，并将它们的预测结果进行汇总（分类问题用 <strong>多数投票</strong>，回归问题用 <strong>平均值</strong>）来输出最终结果。</li>
<li><strong>“随机”的含义</strong>：体现在 <strong>样本选择的随机性</strong> 和 <strong>特征选择的随机性</strong>。</li>
</ul>
<h3 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h3><ol>
<li><p><strong>Bootstrap 采样 (样本随机)</strong>：</p>
<ul>
<li>对于 $N$ 个样本的训练集，有放回地随机抽取 $N$ 次，得到一个新的训练集。</li>
<li>未被抽到的样本称为 <strong>袋外数据 (Out-of-Bag, OOB)</strong>，可用于验证模型泛化能力。</li>
<li>重复上述步骤 $k$ 次，生成 $k$ 个独立的训练集。</li>
</ul>
</li>
<li><p><strong>特征随机选择 (特征随机)</strong>：</p>
<ul>
<li>在构建每棵树的每个分裂结点时，不是从所有 $M$ 个特征中选择最优特征，而是随机从 $M$ 个特征中选取 $m$ 个特征 ($m \ll M$) 作为一个子集。</li>
<li>从这 $m$ 个特征中选择最优的分裂属性。</li>
</ul>
</li>
<li><p><strong>生成决策树</strong>：</p>
<ul>
<li>利用上述采样和特征子集，完全生长决策树（通常 <strong>不剪枝</strong>）。</li>
</ul>
</li>
<li><p><strong>集成输出</strong>：</p>
<ul>
<li><strong>分类</strong>：所有树投票，票数最多的类别为最终结果。</li>
<li><strong>回归</strong>：所有树预测值的简单算术平均。</li>
</ul>
</li>
</ol>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>准确率高</strong>：在许多数据集上表现良好，抗过拟合能力强（因为引入了双重随机性）。</li>
<li><strong>并行处理</strong>：每棵树互不依赖，可以并行训练，速度快。</li>
<li><strong>高维数据友好</strong>：能够处理具有成千上万个特征的输入矩阵，而无需进行特征降维。</li>
<li><strong>自带评估</strong>：可以使用 OOB 数据进行内部评估，无需额外的验证集。</li>
</ul>
<h1 id="第六章-Logistic-回归"><a href="#第六章-Logistic-回归" class="headerlink" title="第六章 $Logistic$回归"></a>第六章 $Logistic$回归</h1><p>关键词：<code>映射到[0,1]、分类（虽然叫回归）、判别模型</code></p>
<ul>
<li>思想：利用线性回归的思路去拟合数据，通过一个函数将预测结果“压缩”到 $[0,1]$，从而表示某种类别发生的概率。</li>
</ul>
<p><strong>几何意义与决策</strong>：</p>
<ul>
<li><strong>二项逻辑回归</strong>在几何上是在特征空间中寻找一个<strong>超平面</strong>（Decision Boundary）来分割两类数据。（通常以 <strong>0.5</strong> 为概率阈值）：</li>
<li>若 $P(Y=1|x) \ge 0.5$，预测为 <strong>1 (正类)</strong>。</li>
<li>若 $P(Y=1|x) &lt; 0.5$，预测为 <strong>0 (负类)</strong>。</li>
</ul>
<h2 id="1-二项-Logistic-回归"><a href="#1-二项-Logistic-回归" class="headerlink" title="1. 二项$Logistic$回归"></a>1. 二项$Logistic$回归</h2><h3 id="事件的几率"><a href="#事件的几率" class="headerlink" title="事件的几率"></a>事件的几率</h3><ul>
<li><p><strong>事件的几率 (odds)</strong>：事件发生与事件不发生的概率之比为</p>
<script type="math/tex; mode=display">\frac{p}{1 - p}</script><p>  称为事件的发生比 (the odds of experiencing an event)。</p>
</li>
<li><p><strong>对数几率 (log odds)</strong>：</p>
<script type="math/tex; mode=display">\text{logit}(p) = \log \frac{p}{1 - p}</script></li>
<li><p><strong>Logistic 回归 (对数几率回归)</strong>：</p>
<script type="math/tex; mode=display">\log \frac{P(Y = 1|x)}{1 - P(Y = 1|x)} = w \cdot x</script></li>
</ul>
<h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><ol>
<li><p><strong>线性预测</strong>：<br>  首先，模型根据输入特征 $x$ 计算一个线性得分（Logits）：</p>
<script type="math/tex; mode=display">z = w \cdot x + b</script></li>
<li><p><strong>$Sigmoid$ 映射</strong>：<br>  为了将 $z \in (-\infty, +\infty)$ 映射为概率 $[0,1]$，引入 <strong>Sigmoid 函数</strong>：</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1 + e^{-z}}</script></li>
</ol>
<p>在数学表达上，它有两种等价形式（这也是教科书与PPT中常见的两种写法）：</p>
<ul>
<li><strong>标准倒数形式</strong>：<script type="math/tex; mode=display">P(Y=1|x) = \frac{1}{1 + e^{-(w \cdot x + b)}}</script></li>
<li><strong>指数形式</strong>（分子分母同时乘以 $e^z$）：<script type="math/tex; mode=display">P(Y=1|x) = \frac{\exp(w \cdot x + b)}{1 + \exp(w \cdot x + b)}</script></li>
</ul>
<p>对应地，预测为负类（$Y=0$）的概率为：</p>
<script type="math/tex; mode=display">P(Y=0|x) = 1 - P(Y=1|x) = \frac{1}{1 + \exp(w \cdot x + b)}</script><ul>
<li><strong>参数的向量化表示 (Bias Trick)</strong></li>
</ul>
<p>在李航《统计学习方法》等教材中，为了简化公式推导，通常会将偏置项 $b$ 并入权重向量 $w$ 中。</p>
<ul>
<li><strong>扩充权重向量</strong>：$w = (w^{(1)}, w^{(2)}, \dots, w^{(n)}, b)^T$</li>
<li><strong>扩充输入向量</strong>：$x = (x^{(1)}, x^{(2)}, \dots, x^{(n)}, 1)^T$</li>
</ul>
<p>通过这种方式，线性项 $w \cdot x + b$ 就变成了单纯的向量点积 $w \cdot x$。最终模型公式简化为：</p>
<script type="math/tex; mode=display">P(Y=1|x) = \frac{\exp(w \cdot x)}{1 + \exp(w \cdot x)}</script><h3 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h3><ul>
<li>名为“回归”，实际是<strong>分类算法</strong>。</li>
<li><code>输入变量与输出变量之间不存在线性关系</code>。</li>
<li>直接对分类可能进行建模。</li>
<li>给出类别的近似概率。</li>
<li>任意阶可导凸函数。</li>
</ul>
<h3 id="多项-Logistic-回归"><a href="#多项-Logistic-回归" class="headerlink" title="多项 $Logistic$ 回归"></a>多项 $Logistic$ 回归</h3><ul>
<li>设 $Y$ 的取值集合为 ${1, 2, \cdots, K}$</li>
<li>多项 logistic 回归模型：</li>
</ul>
<script type="math/tex; mode=display">
P(Y=k|x) = \frac{\exp(\boldsymbol{w}_k \cdot \boldsymbol{x})}{1 + \sum_{k=1}^{K-1} \exp(\boldsymbol{w}_k \cdot \boldsymbol{x})}, \quad k = 1, 2, \cdots, K-1</script><script type="math/tex; mode=display">
P(Y=K|x) = \frac{1}{1 + \sum_{k=1}^{K-1} \exp(\boldsymbol{w}_k \cdot \boldsymbol{x})}</script><script type="math/tex; mode=display">
\boldsymbol{x} \in R^{n+1} \quad \boldsymbol{w}_k \in R^{n+1}</script><h2 id="2-极大似然函数"><a href="#2-极大似然函数" class="headerlink" title="2. 极大似然函数"></a>2. 极大似然函数</h2><p>通过<strong>极大似然估计</strong>获得 $Logistic$ 分类器由一组权值系数 $\omega$。  </p>
<ul>
<li>对于 $N$ 个观测事件：<script type="math/tex; mode=display">
\{(x_i, y_i)\}_{i=1}^N, \quad \mathbf{x}_i \in \mathbb{R}^n, y_i \in \{0, 1\}</script>设：<script type="math/tex; mode=display">
P(Y = 1|\mathbf{x}) = \pi(\mathbf{x}), \quad P(Y = 0|\mathbf{x}) = 1 - \pi(\mathbf{x})</script></li>
<li>其联合概率密度函数，即似然函数为：<script type="math/tex; mode=display">
\begin{aligned}
L &= \prod_{i=1}^N P(X=\boldsymbol{x}_i)P(Y=y_i|X=\boldsymbol{x}_i) \\
&\propto \prod_{i=1}^N ([\pi(\boldsymbol{x}_i)]^{y_i} [1 - \pi(\boldsymbol{x}_i)]^{1-y_i})
\end{aligned}</script></li>
<li>取对数，得到<code>损失函数</code> $L(\omega)$：<script type="math/tex; mode=display">
\begin{aligned}
L(\mathbf{w}) &= \sum_{i=1}^{N} [y_i \log \pi (\mathbf{x}_i) + (1 - y_i) \log ( 1 - \pi(\mathbf{x}_i))] \\
&= \sum_{i=1}^{N} \left[y_i \log \frac{\pi(\mathbf{x}_i)}{1 - \pi(\mathbf{x}_i)} + \log ( 1 - \pi(\mathbf{x}_i)) \right] \\
&= \sum_{i=1}^{N} [y_i (\mathbf{w} \cdot \mathbf{x}_i) - \log ( 1 + \exp ( \mathbf{w} \cdot \mathbf{x}_i))]
\end{aligned}</script>注意：第三个等号是代入 $\pi(\boldsymbol{x})= P(Y=1|\boldsymbol{x})= \frac{\exp(w \cdot x)}{1 + \exp(w \cdot x)}…$</li>
<li>最后，对 $L(\omega)$ 求极大值（梯度下降-<strong>求导</strong>）, 得到 $\omega$ 的估计值。  </li>
</ul>
<p><strong>三种函数图示</strong>：<br><img src="2025-12-05-17-55-15.png" alt=""></p>
<h2 id="3-最大熵模型"><a href="#3-最大熵模型" class="headerlink" title="3. 最大熵模型"></a>3. 最大熵模型</h2><ul>
<li><strong>熵最大的模型是最好的模型</strong></li>
</ul>
<h2 id="4-模型学习最优化算法"><a href="#4-模型学习最优化算法" class="headerlink" title="4. 模型学习最优化算法"></a>4. 模型学习最优化算法</h2><ul>
<li>梯度下降法<br><img src="2025-12-05-17-51-09.png" alt=""><br><img src="2025-12-05-17-51-25.png" alt=""></li>
<li>牛顿法</li>
<li>拟牛顿法</li>
<li>改进的迭代尺度法</li>
</ul>
<h2 id="5-Logistic-Regression-（推导）"><a href="#5-Logistic-Regression-（推导）" class="headerlink" title="5. $Logistic Regression$（推导）"></a>5. $Logistic Regression$（推导）</h2><p><img src="2025-12-05-20-35-01.png" alt=""><br><strong>推导过程</strong>：<br><img src="2025-12-05-21-04-01.png" alt=""><br><strong>三个关键点</strong>：</p>
<ul>
<li>$Sigmoid$函数的导数</li>
<li>构造概率似然函数</li>
<li>把求导拆分成两个求导</li>
</ul>
<h1 id="第七章-支持向量机"><a href="#第七章-支持向量机" class="headerlink" title="第七章 支持向量机"></a>第七章 支持向量机</h1><ul>
<li>定义：在特征空间上的<strong>间隔最大线性分类器</strong>（与感知机的不同）</li>
<li><strong>核技巧</strong>，所以存在<strong>非线性分类器</strong></li>
<li>学习策略：间隔最大化$\longrightarrow$正则化的<strong>合页损失函数</strong>最小化问题</li>
</ul>
<h2 id="1-线性可分-SVM-（硬间隔）"><a href="#1-线性可分-SVM-（硬间隔）" class="headerlink" title="1. 线性可分 SVM （硬间隔）"></a>1. 线性可分 SVM （硬间隔）</h2><blockquote>
<ul>
<li>所有样本必须线性可分，对噪音敏感，容易过拟合。</li>
</ul>
</blockquote>
<h3 id="（1）原始算法"><a href="#（1）原始算法" class="headerlink" title="（1）原始算法"></a>（1）原始算法</h3><ol>
<li>构造并求解最优化问题：<script type="math/tex; mode=display">
\left\{
\begin{aligned}
& \min_{w,b} \;\; \frac{1}{2}\|w\|^2 \\
& \text{s.t.} \;\; y_i(w \cdot x_i + b) - 1 \ge 0, \quad i = 1, 2, \cdots, N
\end{aligned}
\right.</script></li>
<li>得到分离超平面：<script type="math/tex; mode=display">w^*\cdot x+b^*=0</script></li>
<li>分类决策函数<script type="math/tex; mode=display">f(x)=sign(w^*\cdot x+b^*=0)</script></li>
</ol>
<h3 id="（2）对偶问题（KKT条件）"><a href="#（2）对偶问题（KKT条件）" class="headerlink" title="（2）对偶问题（KKT条件）"></a>（2）对偶问题（KKT条件）</h3><h4 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h4><p>SVM 的基本型是一个凸二次规划问题。我们的目标是最小化参数的范数（最大化间隔），同时满足分类约束。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{w,b} \quad & \frac{1}{2}\|w\|^2 \\
\text{s.t.} \quad & 1 - y_i(w \cdot x_i + b) \le 0, \quad i=1,\dots,N
\end{aligned}</script><h4 id="A-构造拉格朗日函数"><a href="#A-构造拉格朗日函数" class="headerlink" title="A.构造拉格朗日函数"></a>A.构造拉格朗日函数</h4><p>引入拉格朗日乘子 $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_N)^T$，且 $\alpha_i \ge 0$。将<strong>约束条件融合到目标函数中</strong>：</p>
<script type="math/tex; mode=display">
L(w, b, \alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^{N} \alpha_i \left( 1 - y_i(w \cdot x_i + b) \right)</script><p>根据拉格朗日对偶性，原始问题等价于极小极大问题：</p>
<script type="math/tex; mode=display">\min_{w,b} \max_{\alpha \ge 0} L(w, b, \alpha)</script><p>其对偶问题则是交换顺序（满足kkt条件），变为极大极小问题：</p>
<script type="math/tex; mode=display">\max_{\alpha \ge 0} \min_{w,b} L(w, b, \alpha)</script><h4 id="B-求解对偶问题"><a href="#B-求解对偶问题" class="headerlink" title="B.求解对偶问题"></a>B.求解对偶问题</h4><ul>
<li>第一步：求内层最小值 $\min_{w,b} L(w, b, \alpha)$</li>
</ul>
<p>对 $w$ 和 $b$ 分别求偏导并令其为 0：</p>
<ol>
<li><strong>对 $w$ 求导：</strong><script type="math/tex; mode=display">\nabla_w L = w - \sum_{i=1}^{N} \alpha_i y_i x_i = 0 \implies \mathbf{w = \sum_{i=1}^{N} \alpha_i y_i x_i}</script></li>
<li><strong>对 $b$ 求导：</strong><script type="math/tex; mode=display">\nabla_b L = - \sum_{i=1}^{N} \alpha_i y_i = 0 \implies \mathbf{\sum_{i=1}^{N} \alpha_i y_i = 0}</script></li>
</ol>
<ul>
<li>第二步：代回拉格朗日函数</li>
</ul>
<p>将上述两个关系式代回 $L(w, b, \alpha)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(w, b, \alpha) &= \frac{1}{2} \underbrace{w \cdot w}_{\|w\|^2} - w \cdot \underbrace{\sum_{i=1}^{N} \alpha_i y_i x_i}_{w} - b \underbrace{\sum_{i=1}^{N} \alpha_i y_i}_{0} + \sum_{i=1}^{N} \alpha_i \\
&= \frac{1}{2} \|w\|^2 - \|w\|^2 + \sum_{i=1}^{N} \alpha_i \\
&= \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \|w\|^2
\end{aligned}</script><p>展开 $|w|^2$ 项：</p>
<script type="math/tex; mode=display">\|w\|^2 = \left( \sum_{i=1}^{N} \alpha_i y_i x_i \right) \cdot \left( \sum_{j=1}^{N} \alpha_j y_j x_j \right) = \sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)</script><ul>
<li>第三步：最终的对偶问题</li>
</ul>
<p>此时目标是 $\max_{\alpha}$，通常我们习惯转换为 $\min_{\alpha}$ (取负号)。</p>
<p><strong>最终对偶形式 (Dual Form)：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^{N} \alpha_i \\
\text{s.t.} \quad & \sum_{i=1}^{N} \alpha_i y_i = 0 \\
& \alpha_i \ge 0 \quad (\text{硬间隔}) \\
& 0 \le \alpha_i \le C \quad (\text{软间隔})
\end{aligned}</script><hr>
<h4 id="C-KKT-条件"><a href="#C-KKT-条件" class="headerlink" title="C.KKT 条件"></a>C.KKT 条件</h4><p>为了保证对偶问题的解（Dual Optimal）同时也确实是原始问题（Primal Optimal）的解，必须满足 KKT 条件。这是 SVM 具有“稀疏性”（支持向量特性）的数学根源。</p>
<p>对于最优解 $w^{\ast}, b^{\ast}, \alpha^{\ast}$ 必须满足以下四个条件：</p>
<p><strong>1. 稳定性条件 (Stationarity):</strong><br>即导数为 0：</p>
<script type="math/tex; mode=display">w^* = \sum_{i=1}^{N} \alpha_i^* y_i x_i</script><script type="math/tex; mode=display">\sum_{i=1}^{N} \alpha_i^* y_i = 0</script><p><strong>2. <code>互补松弛性 (Complementary Slackness)</code> [核心]:</strong></p>
<script type="math/tex; mode=display">\alpha_i^* (1 - y_i(w^* \cdot x_i + b^*)) = 0</script><p><strong>互补松弛性的物理意义：</strong><br>这个条件告诉我们，对于任意样本 $i$ ，或者 $\alpha_i^{\ast} = 0$ ，或者 $1 - y_i(w^{\ast} \cdot x_i + b^{\ast}) = 0$ 。</p>
<ul>
<li>如果 $\alpha_i^{\ast} = 0$ ，该样本对模型没有贡献（非支持向量）。</li>
<li>如果 $\alpha_i^{\ast} &gt; 0$ ，则必须满足 $y_i(w^{\ast} \cdot x_i + b^{\ast}) = 1$ ，即样本点必须位于 <strong>间隔边界</strong> 上。这些点就是 <strong>支持向量</strong>。</li>
</ul>
<p><strong>3. 原始可行性 (Primal Feasibility):</strong><br>样本必须满足分类间隔约束：</p>
<script type="math/tex; mode=display">1 - y_i(w^* \cdot x_i + b^*) \le 0</script><p><strong>4. 对偶可行性 (Dual Feasibility):</strong><br>拉格朗日乘子必须非负：</p>
<script type="math/tex; mode=display">\alpha_i^* \ge 0</script><h2 id=""><a href="#" class="headerlink" title=""></a><img src="2025-12-06-15-09-54.png" alt=""></h2><p><strong>关键点总结</strong></p>
<p><strong>1. 为什么要做对偶？</strong></p>
<ul>
<li><strong>计算效率：</strong> 摆脱了对特征维度 $d$ 的依赖，计算复杂度仅与样本数量 $N$ 有关。</li>
<li><strong>核技巧 (Kernel Trick)：</strong> 最终公式中只包含内积 $(x_i \cdot x_j)$。我们可以直接用核函数 $K(x_i, x_j)$ 替换它，从而在不显式增加维度的情况下解决非线性分类问题。</li>
</ul>
<p><strong>2. 软间隔 (Soft Margin) 的变化</strong></p>
<ul>
<li>如果是软间隔 SVM，KKT 条件中的对偶可行性变为 $0 \le \alpha_i \le C$。</li>
<li>当 $\alpha_i = C$ 时，样本点可能是一个异常点（位于间隔内部或分错）。</li>
</ul>
<hr>
<h2 id="2-线性-SVM-（软间隔）"><a href="#2-线性-SVM-（软间隔）" class="headerlink" title="2. 线性 SVM （软间隔）"></a>2. 线性 SVM （软间隔）</h2><blockquote>
<ul>
<li>允许样本存在噪声，有较好的鲁棒性。</li>
</ul>
</blockquote>
<h3 id="（1）核心思想与动机"><a href="#（1）核心思想与动机" class="headerlink" title="（1）核心思想与动机"></a><strong>（1）核心思想与动机</strong></h3><ul>
<li><strong>动机：</strong> 现实中的数据往往不是完全线性可分的，或者包含噪声（异常点）。硬间隔 SVM 强行划分会导致过拟合或无解。</li>
<li><strong>解决：</strong> 引入<strong>松弛变量 (Slack Variable)</strong>，允许部分样本点不满足严格的间隔约束（即允许犯错），以换取更大的间隔和更好的模型泛化能力。</li>
</ul>
<h3 id="（2）原始问题的数学表达"><a href="#（2）原始问题的数学表达" class="headerlink" title="（2）原始问题的数学表达"></a><strong>（2）原始问题的数学表达</strong></h3><ul>
<li><p><strong>引入松弛变量 $\xi_i$ (Xi)：</strong><br>  为了容忍错误，约束条件由原来的硬性限制改为软性限制：</p>
<script type="math/tex; mode=display">y_i(w \cdot x_i + b) \ge 1 - \xi_i</script><p>  其中 $\xi_i \ge 0$。</p>
</li>
<li><p><strong>几何意义：</strong>（这个最好别看）</p>
<ul>
<li>$\xi_i = 0$：样本分类正确且在间隔边界外（理想情况）。</li>
<li>$0 &lt; \xi_i &lt; 1$：样本分类正确，但落入间隔带内部（违背了最大间隔，但在正确的一侧）。</li>
<li>$\xi_i = 1$：样本落在决策边界（超平面）上。</li>
<li>$\xi_i &gt; 1$：样本被错误分类（跑到对面去了）。</li>
</ul>
</li>
<li><p><strong>目标函数：</strong><br>  需要在“最大化间隔”（结构风险）和“最小化误分程度”（经验风险）之间做权衡：</p>
<script type="math/tex; mode=display">\min_{w,b,\xi} \quad \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{N} \xi_i</script></li>
</ul>
<h3 id="（3）核心参数-C-的物理意义"><a href="#（3）核心参数-C-的物理意义" class="headerlink" title="（3）核心参数 C 的物理意义"></a><strong>（3）核心参数 C 的物理意义</strong></h3><ul>
<li><strong>定义：</strong> $C &gt; 0$ 是惩罚系数（正则化参数的倒数），由用户设定的超参数。</li>
<li><strong>C 值较大：</strong> 对误分容忍度低。模型会迫使 $\xi_i$ 趋近于 0，追求训练集的高准确率。<ul>
<li><em>后果：</em> 接近硬间隔，间隔变窄，容易<strong>过拟合</strong>。</li>
</ul>
</li>
<li><strong>C 值较小：</strong> 对误分容忍度高。允许更多样本违背约束，更看重 $\frac{1}{2}|w|^2$（即间隔宽度）。<ul>
<li><em>后果：</em> 间隔变宽，容易<strong>欠拟合</strong>（若太小），但泛化能力通常更强。</li>
</ul>
</li>
</ul>
<h3 id="（4）对偶问题与解的特性"><a href="#（4）对偶问题与解的特性" class="headerlink" title="（4）对偶问题与解的特性"></a><strong>（4）对偶问题与解的特性</strong></h3><ul>
<li><p><strong>拉格朗日乘子范围的变化：</strong><br>  这是与硬间隔最直接的数学区别。在求解对偶问题时，乘子 $\alpha_i$ 的约束变为：</p>
<script type="math/tex; mode=display">0 \le \alpha_i \le C</script><p>  （硬间隔是 $0 \le \alpha_i &lt; +\infty$）</p>
</li>
<li><p><strong>支持向量 (Support Vectors) 的判定：</strong><br>  根据 KKT 条件中的互补松弛性，只有 $\alpha_i &gt; 0$ 的样本是支持向量：</p>
<ul>
<li>若 $0 &lt; \alpha_i &lt; C$：对应的 $\xi_i = 0$，样本恰好落在<strong>间隔边界</strong>上（它是决定间隔宽度的核心力量）。</li>
<li>若 $\alpha_i = C$， $0 &lt; \xi_i &lt; 1$，样本<strong>分类正确</strong>，在间隔边界与超平面之间。</li>
<li>若 $\alpha_i = C$， $\xi_i = 1$，样本在超平面上。</li>
<li>若 $\alpha_i = C$， $\xi_i &gt; 1$，样本<strong>误分类</strong>。</li>
</ul>
</li>
</ul>
<h3 id="（5）等价形式：合页损失函数-Hinge-Loss"><a href="#（5）等价形式：合页损失函数-Hinge-Loss" class="headerlink" title="（5）等价形式：合页损失函数 (Hinge Loss)"></a><strong>（5）等价形式：合页损失函数 (Hinge Loss)</strong></h3><ul>
<li>软间隔 SVM 的优化目标等价于<code>合页损失函数</code>：<script type="math/tex; mode=display">\min_{w,b} \sum_{i=1}^{N} [1 - y_i(w \cdot x_i + b)]_+ + \lambda \|w\|^2</script></li>
<li><strong>Hinge Loss 定义：</strong> $L(y, f(x)) = \max(0, 1 - y f(x))$</li>
<li><strong>解释：</strong> 只有当样本不仅分对了，而且距离超平面距离足够远（函数间隔大于 1）时，损失才为 0；否则损失随着错误的程度线性增加。</li>
</ul>
<h2 id="3-支持向量是什么？"><a href="#3-支持向量是什么？" class="headerlink" title="3. 支持向量是什么？"></a>3. 支持向量是什么？</h2><ul>
<li>支持向量是使得<strong>约束条件等号成立</strong>的点，即 $\text{s.t.} \;\; y_i(w \cdot x_i + b) - 1 = 0$</li>
<li>直观地，大多数点对于边界在哪没有影响，支持向量是<strong>决定了决策边界在哪</strong>的样本点。<code>分离超平面完全由支持向量决定</code>。</li>
<li>支持向量一定在间隔边界上。</li>
</ul>
<p><img src="2025-12-06-16-40-33.png" alt=""></p>
<ol>
<li>线性可分：与分离超平面<strong>距离最近</strong>的样本点</li>
</ol>
<h2 id="4-核函数与非线性-SVM"><a href="#4-核函数与非线性-SVM" class="headerlink" title="4. 核函数与非线性 SVM"></a>4. 核函数与非线性 SVM</h2><h3 id="（1）非线性-SVM-的核心直观"><a href="#（1）非线性-SVM-的核心直观" class="headerlink" title="（1）非线性 SVM 的核心直观"></a><strong>（1）非线性 SVM 的核心直观</strong></h3><ul>
<li><strong>问题：</strong> 原始样本空间中，数据线性不可分（如异或问题、环形数据）。</li>
<li><strong>策略：</strong> 将样本从低维空间映射到高维特征空间（Hilbert Space）。根据 Cover 定理，在足够高的维度中，数据更有可能是线性可分的。</li>
<li><strong>映射函数：</strong> $\phi: \mathcal{X} \to \mathcal{H}$</li>
</ul>
<h3 id="（2）核技巧-Kernel-Trick"><a href="#（2）核技巧-Kernel-Trick" class="headerlink" title="（2）核技巧 (Kernel Trick)"></a><strong>（2）核技巧 (Kernel Trick)</strong></h3><ul>
<li><strong>困境：</strong> 如果直接计算高维向量 $\phi(x)$，计算复杂度极高，甚至因维度无穷大而无法计算。</li>
<li><strong>观察：</strong> 在 SVM 对偶问题中，样本仅以<strong>内积</strong>形式出现：$\langle x_i, x_j \rangle$。在高维空间中对应 $\langle \phi(x_i), \phi(x_j) \rangle$。</li>
<li><strong>核函数定义：</strong> 设 $\mathcal{X}$ 是输入空间，$\mathcal{H}$ 是特征空间。如果存在函数 $K(x, z)$ 满足：<script type="math/tex; mode=display">K(x, z) = \phi(x) \cdot \phi(z)</script>  则称 $K$ 为核函数。</li>
<li><strong>作用：</strong> 我们不需要显式知道 $\phi(x)$ 是什么，只需在低维空间计算 $K(x, z)$，就能等效达到在高维空间做超平面划分的效果。</li>
</ul>
<h3 id="（3）对偶问题的修改"><a href="#（3）对偶问题的修改" class="headerlink" title="（3）对偶问题的修改"></a><strong>（3）对偶问题的修改</strong></h3><p>引入核函数后，非线性 SVM 的优化目标变为：</p>
<script type="math/tex; mode=display">
\min_{\alpha} \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) - \sum_{i=1}^{N} \alpha_i</script><p>决策函数变为：</p>
<script type="math/tex; mode=display">f(x) = \text{sign}\left( \sum_{i=1}^{N} \alpha_i y_i K(x_i, x) + b \right)</script><h3 id="（4）常用核函数"><a href="#（4）常用核函数" class="headerlink" title="（4）常用核函数"></a><strong>（4）常用核函数</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">核函数</th>
<th style="text-align:left">公式 $K(x, z)$</th>
<th style="text-align:left">特点与应用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>线性核 (Linear)</strong></td>
<td style="text-align:left">$x \cdot z$</td>
<td style="text-align:left">无映射。适用于特征维数高的情况。</td>
</tr>
<tr>
<td style="text-align:left"><strong>多项式核 (Poly)</strong></td>
<td style="text-align:left">$(x \cdot z + 1)^d$</td>
<td style="text-align:left">映射到 $d$ 阶多项式空间。参数多，计算稍慢。</td>
</tr>
<tr>
<td style="text-align:left"><strong>高斯核 (RBF)</strong></td>
<td style="text-align:left">$\exp(-\frac{\lVert x - z \rVert^2}{2\sigma^2})$</td>
<td style="text-align:left"><strong>最常用</strong>。映射到无穷维空间。衡量样本相似度。$\gamma$ 越大，模型越复杂（易过拟合）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>Sigmoid 核</strong></td>
<td style="text-align:left">$\tanh(\gamma x \cdot z + r)$</td>
<td style="text-align:left">类似于神经网络中的激活函数。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="（5）什么样的函数能做核函数？"><a href="#（5）什么样的函数能做核函数？" class="headerlink" title="（5）什么样的函数能做核函数？"></a><strong>（5）什么样的函数能做核函数？</strong></h3><ul>
<li><strong>Mercer 定理 (Mercer’s Theorem)：</strong><br>  只要函数 $K$ 满足<strong>对称性</strong>，且对应的核矩阵（Gram Matrix）是<strong>半正定</strong>（所有特征值非负）的，它就是合法的核函数。</li>
</ul>
<p><img src="2025-12-06-16-32-38.png" alt=""></p>
<h1 id="第八章-提升算法"><a href="#第八章-提升算法" class="headerlink" title="第八章 提升算法"></a>第八章 提升算法</h1><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p><img src="2025-12-07-16-09-35.png" alt=""></p>
<ul>
<li>（b）是欠拟合（高偏差）</li>
<li>（c）是过拟合（高方差）</li>
</ul>
<h3 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h3><ol>
<li>并行化：不存在强依赖关系，关注<strong>降低方差</strong>。代表：<strong>随机森林，Bagging</strong></li>
<li>序列化：又叫串行化，是强依赖关系，关注<strong>降低偏差</strong>。代表：<strong>Boosting</strong><ul>
<li>思想：只要找到一个<strong>比随机猜测略好（分类误差 &lt;0.5）</strong>的弱学习算法，就可以直接将其提升为强学习算法。</li>
</ul>
</li>
</ol>
<h2 id="AdaBoost-算法"><a href="#AdaBoost-算法" class="headerlink" title="$AdaBoost$ 算法"></a>$AdaBoost$ 算法</h2><p>关键词：<code>加法模型、指数损失、前向分布算法</code><br><strong>注意点</strong>：</p>
<ol>
<li>计算 $G(m)$ 的训练误差 $e_m$</li>
<li>计算系数 $\alpha_m =\frac{1}{2}ln\frac{1-e_m}{e_m}$ <code>&lt;0.5</code> （不考计算，但这一点要注意）</li>
<li>正确分类权值更新：$\times e^{-\alpha}$ ；错误分类权值更新：$\times e^{\alpha}$ </li>
<li>损失函数：<strong>指数损失</strong> $L(y,f(x)) =exp(-yf(x))$</li>
<li><code>分错样本权值变大，分对样本权值变小</code>，权值和 =1（下图选 $B$）<br><img src="2025-12-07-16-31-16.png" alt=""><ul>
<li><strong>注意</strong>：样本权重 $\omega$ 之和 $=1$，但是基分类器权重 $\alpha_m$ 之和 $\not ={1}$</li>
</ul>
</li>
<li>为什么 $AdaBoost$ 对噪声敏感？<br>因为 AdaBoost 的机制是指数级地增加被分错样本的权重，是串行化模型，关住提高模型准确度（降低偏差），所以对噪声非常敏感。如果数据中存在标签错误的噪声点，模型会投入极大的资源去强行拟合这个噪声，导致正常样本被牺牲，从而过拟合。</li>
</ol>
<h1 id="第九章-EM算法"><a href="#第九章-EM算法" class="headerlink" title="第九章 EM算法"></a>第九章 EM算法</h1><h2 id="1-EM算法的导出"><a href="#1-EM算法的导出" class="headerlink" title="1. EM算法的导出"></a>1. EM算法的导出</h2><p><strong>输入</strong>：观测变量数据 $X$，隐变量 $Z$，联合分布 $P(X,Z|\theta)$<br><strong>输出</strong>：模型参数 $\theta$</p>
<ol>
<li><p><strong>初始化 (Initialization)</strong></p>
<ul>
<li>随机选取模型参数的初值 $\theta^{(0)}$。</li>
<li><em>注：EM 算法对初值敏感，不同的初值可能导致收敛到不同的局部极值。</em></li>
</ul>
</li>
<li><p><strong>E步 (Expectation Step)</strong></p>
<ul>
<li><strong>目的</strong>：利用当前参数推测隐变量的分布。</li>
<li><strong>操作</strong>：基于当前参数 $\theta^{(t)}$ 和观测数据 $X$，计算隐变量 $Z$ 的条件概率（后验概率）。</li>
<li><strong>构造 Q 函数</strong>（即完全数据对数似然函数的期望）：<script type="math/tex; mode=display">Q(\theta, \theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\log P(X, Z | \theta)]</script></li>
</ul>
</li>
<li><p><strong>M步 (Maximization Step)</strong></p>
<ul>
<li><strong>目的</strong>：根据推测的隐变量分布，更新模型参数。</li>
<li><strong>操作</strong>：寻找一个新的参数 $\theta$，使得 Q 函数的值最大化：<script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max_{\theta} Q(\theta, \theta^{(t)})</script></li>
</ul>
</li>
<li><p><strong>收敛判断 (Convergence Check)</strong></p>
<ul>
<li><strong>操作</strong>：计算参数的更新幅度 $|\theta^{(t+1)} - \theta^{(t)}|$ 或对数似然函数的增量。</li>
<li><strong>判断</strong>：<ul>
<li>若变化量小于预设阈值 $\epsilon$（如 $1e-5$），则停止迭代，输出最终参数 $\hat{\theta} = \theta^{(t+1)}$。</li>
<li>否则，令 $t \leftarrow t+1$，返回第 2 步继续迭代。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="2-EM算法的收敛性"><a href="#2-EM算法的收敛性" class="headerlink" title="2. EM算法的收敛性"></a>2. EM算法的收敛性</h2><h3 id="1-核心性质：单调递增"><a href="#1-核心性质：单调递增" class="headerlink" title="(1). 核心性质：单调递增"></a>(1). 核心性质：单调递增</h3><p>EM 算法保证每次迭代后，观测数据的对数似然函数值是非递减的。即：</p>
<script type="math/tex; mode=display">L(\theta^{(t+1)}) \ge L(\theta^{(t)})</script><p>由于似然函数通常有上界（Bounded from above），单调递增且有上界的序列必然收敛。</p>
<h3 id="2-数学原理：下界逼近"><a href="#2-数学原理：下界逼近" class="headerlink" title="(2). 数学原理：下界逼近"></a>(2). 数学原理：下界逼近</h3><ul>
<li><strong>原理</strong>：EM 算法并不直接优化复杂的 $L(\theta)$，而是优化其<strong>下界</strong>。</li>
<li><strong>工具</strong>：利用 <strong>Jensen 不等式 (Jensen’s Inequality)</strong> 构造下界函数 $B(\theta, \theta^{(t)})$。</li>
<li><strong>过程</strong>：<script type="math/tex; mode=display">L(\theta) - L(\theta^{(t)}) \ge 0</script>通过最大化下界 Q 函数，间接推高了原目标函数 $L(\theta)$。</li>
</ul>
<h3 id="3-收敛目标：局部最优"><a href="#3-收敛目标：局部最优" class="headerlink" title="(3). 收敛目标：局部最优"></a>(3). 收敛目标：局部最优</h3><ul>
<li>EM 算法<strong>不能保证</strong>收敛到全局最优解 (Global Maximum)。</li>
<li>它通常收敛到似然函数的<strong>稳定点 (Stationary Point)</strong>，大多数情况下是<strong>局部极大值 (Local Maximum)</strong>。</li>
<li>只有在目标函数是凸函数（Concave）等特殊情况下，才能保证全局最优。</li>
</ul>
<h3 id="4-工程启示"><a href="#4-工程启示" class="headerlink" title="(4). 工程启示"></a>(4). 工程启示</h3><ul>
<li><strong>初值敏感性</strong>：由于存在多个局部极值，最终结果高度依赖于初始参数 $\theta^{(0)}$ 的选择。<ul>
<li><em>解决方案</em>：采用多次随机初始化 (Random Restarts)，取结果最好的那一组；或使用 K-Means 等算法预热初始化。</li>
</ul>
</li>
<li><strong>收敛速度</strong>：算法在迭代初期收敛较快，但在接近极值点时收敛速度变慢（通常为一阶线性收敛）。</li>
</ul>
<h2 id="3-高斯混合模型-GMM"><a href="#3-高斯混合模型-GMM" class="headerlink" title="3. 高斯混合模型 (GMM)"></a>3. 高斯混合模型 (GMM)</h2><p>在统计学习和模式识别中，<strong>高斯混合模型 (GMM)</strong> 和 <strong>EM 算法</strong> 通常是绑定出现的。简单来说：GMM 是我们要建立的<strong>模型</strong>（Model），而 EM 算法是用于训练该模型的<strong>优化方法</strong>（Optimization Method）。</p>
<h3 id="1-高斯混合模型-GMM"><a href="#1-高斯混合模型-GMM" class="headerlink" title="(1). 高斯混合模型 (GMM)"></a>(1). 高斯混合模型 (GMM)</h3><h4 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h4><p>传统的聚类算法（如 K-Means）是<strong>硬聚类</strong>，即一个样本点要么属于 A 类，要么属于 B 类。但在实际应用中，很多数据具有模糊性（Ambiguity）。</p>
<p><strong>GMM (Gaussian Mixture Model)</strong> 是一种<strong>软聚类</strong>模型。它假设数据是由 $K$ 个不同的高斯分布（正态分布）混合生成的。对于每一个数据点，模型给出的是它属于每个簇的<strong>概率</strong>。</p>
<h4 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h4><p>GMM 的概率密度函数定义为 $K$ 个高斯密度的加权和：</p>
<script type="math/tex; mode=display">
P(x|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)</script><p>其中：</p>
<ul>
<li>$x$: 观测数据。</li>
<li>$\pi_k$: <strong>混合系数</strong> (Mixing Coefficient)，即第 $k$ 个高斯分布被选中的概率，满足 $\sum \pi_k = 1$ 且 $\pi_k \ge 0$。</li>
<li>$\mathcal{N}(x | \mu_k, \Sigma_k)$: 第 $k$ 个<strong>高斯分布密度函数</strong>，参数为均值 $\mu_k$ 和协方差矩阵 $\Sigma_k$。</li>
</ul>
<p>我们需要估计的参数集合为 $\theta = { \pi_1, \dots, \pi_K, \mu_1, \dots, \mu_K, \Sigma_1, \dots, \Sigma_K }$。</p>
<hr>
<h3 id="2-为什么需要-EM-算法？"><a href="#2-为什么需要-EM-算法？" class="headerlink" title="(2). 为什么需要 EM 算法？"></a>(2). 为什么需要 EM 算法？</h3><p>如果我们知道每个样本具体属于哪个高斯分布（即已知<strong>隐变量 Latent Variable</strong>），我们直接用<strong>极大似然估计 (MLE)</strong> 就可以算出 $\mu$ 和 $\Sigma$。</p>
<p>但在非监督学习中，我们<strong>不知道</strong>样本的类别标签。这就陷入了一个“鸡生蛋，蛋生鸡”的循环：</p>
<ol>
<li>要算出参数 $\theta$，需要先知道每个样本属于哪个分布（隐变量）。</li>
<li>要知道样本属于哪个分布，又需要先有准确的参数 $\theta$ 来计算概率。</li>
</ol>
<p><strong>EM 算法 (Expectation-Maximization Algorithm)</strong> 正是为了解决这种含有隐变量的概率模型参数估计问题。</p>
<hr>
<h3 id="3-EM-算法在-GMM-中的流程"><a href="#3-EM-算法在-GMM-中的流程" class="headerlink" title="(3). EM 算法在 GMM 中的流程"></a>(3). EM 算法在 GMM 中的流程</h3><p>EM 算法通过迭代的方式逼近最优解，主要分为两步：<strong>E步（期望步）</strong>和 <strong>M步（最大化步）</strong>。</p>
<h4 id="Step-0-初始化"><a href="#Step-0-初始化" class="headerlink" title="Step 0: 初始化"></a>Step 0: 初始化</h4><p>随机初始化参数 $\mu_k, \Sigma_k, \pi_k$。</p>
<h4 id="Step-1-E-step-计算责任-后验概率"><a href="#Step-1-E-step-计算责任-后验概率" class="headerlink" title="Step 1: E-step (计算责任/后验概率)"></a>Step 1: E-step (计算责任/后验概率)</h4><p>固定参数，计算第 $n$ 个样本来自第 $k$ 个分模型的概率（也称为“责任” $\gamma_{nk}$）：</p>
<script type="math/tex; mode=display">
\gamma_{nk} = P(z_n = k | x_n) = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}</script><blockquote>
<p><strong>直观解释</strong>：看当前这个点，在第 $k$ 个分布里的相对密度有多大。</p>
</blockquote>
<h4 id="Step-2-M-step-更新参数"><a href="#Step-2-M-step-更新参数" class="headerlink" title="Step 2: M-step (更新参数)"></a>Step 2: M-step (更新参数)</h4><p>利用 E 步算出的 $\gamma_{nk}$ 作为权重，重新估计参数（最大化 Q 函数）：</p>
<ol>
<li><strong>更新均值 $\mu_k$</strong>（加权平均）：<script type="math/tex; mode=display">
\mu_k^{new} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} x_n</script></li>
<li><strong>更新协方差 $\Sigma_k$</strong>（加权协方差）：<script type="math/tex; mode=display">
\Sigma_k^{new} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k^{new})(x_n - \mu_k^{new})^T</script></li>
<li><strong>更新混合系数 $\pi_k$</strong>：<script type="math/tex; mode=display">
\pi_k^{new} = \frac{N_k}{N}</script>其中 $N<em>k = \sum</em>{n=1}^{N} \gamma_{nk}$ 是第 $k$ 类权重的总和。</li>
</ol>
<h4 id="Step-3-收敛判断"><a href="#Step-3-收敛判断" class="headerlink" title="Step 3: 收敛判断"></a>Step 3: 收敛判断</h4><p>重复 E 步和 M 步，直到对数似然函数 (Log-Likelihood) 的增长小于阈值，或参数不再变化。</p>
<hr>
<h3 id="4-GMM-vs-K-Means"><a href="#4-GMM-vs-K-Means" class="headerlink" title="(4). GMM vs K-Means"></a>(4). GMM vs K-Means</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">维度</th>
<th style="text-align:left">K-Means</th>
<th style="text-align:left">GMM (高斯混合模型)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>聚类性质</strong></td>
<td style="text-align:left"><strong>硬聚类</strong> (0 或 1)</td>
<td style="text-align:left"><strong>软聚类</strong> (概率分布)</td>
</tr>
<tr>
<td style="text-align:left"><strong>模型假设</strong></td>
<td style="text-align:left">假设簇是球状的 (方差一致)</td>
<td style="text-align:left">假设簇是高斯分布 (可通过协方差描述椭圆)</td>
</tr>
<tr>
<td style="text-align:left"><strong>参数复杂度</strong></td>
<td style="text-align:left">仅中心点 (Centroids)</td>
<td style="text-align:left">中心点 + 协方差矩阵 + 混合系数</td>
</tr>
<tr>
<td style="text-align:left"><strong>本质关系</strong></td>
<td style="text-align:left">GMM 的一种特例</td>
<td style="text-align:left">EM 算法的通用应用</td>
</tr>
<tr>
<td style="text-align:left"><strong>对异常值</strong></td>
<td style="text-align:left">敏感</td>
<td style="text-align:left">相对鲁棒 (概率模型具有一定容错性)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><h3 id="引理9-2"><a href="#引理9-2" class="headerlink" title="引理9.2"></a>引理9.2</h3><h3 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h3><h1 id="第十章-聚类方法"><a href="#第十章-聚类方法" class="headerlink" title="第十章 聚类方法"></a>第十章 聚类方法</h1><h2 id="1-层次聚类"><a href="#1-层次聚类" class="headerlink" title="1. 层次聚类"></a>1. 层次聚类</h2><ul>
<li>聚类是<strong>无监督学习</strong></li>
</ul>
<h3 id="聚类的核心参数"><a href="#聚类的核心参数" class="headerlink" title="聚类的核心参数"></a>聚类的核心参数</h3><ol>
<li>距离（马氏距离、L距离）<ul>
<li>距离越小，样本越相似</li>
</ul>
</li>
<li>相似度（夹角余弦、相关系数）<ul>
<li>相似度越大，样本越相似</li>
</ul>
</li>
</ol>
<ul>
<li>直接影响聚类结果，其选择是聚类的根本问题。</li>
</ul>
<h3 id="硬聚类-amp-软聚类"><a href="#硬聚类-amp-软聚类" class="headerlink" title="硬聚类 &amp; 软聚类"></a>硬聚类 &amp; 软聚类</h3><ul>
<li>一个样本只能属于一个类，就叫<strong>硬聚类</strong>。</li>
<li>一个样本可以属于多个类，就叫<strong>软聚类</strong>。</li>
</ul>
<h3 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h3><p><strong>假设有两个类：$G_p , G_q$</strong></p>
<ul>
<li>类的中心：一个类中所有样本的均值</li>
<li>类的直径：两个距离最远的样本距离</li>
<li><strong>单连接</strong>：两个类中最近样本的距离</li>
<li><strong>完全连接</strong>：两个类中最远样本的距离</li>
<li>中心距离：两个样本中心间的距离</li>
<li>平均距离：两个样本中所有样本的平均距离</li>
</ul>
<p>注意：做层次聚类题目的时候注意要求是按照<strong>单连接/完全连接</strong>来聚类，画图的时候<strong>长度要按比例</strong>。</p>
<h3 id="聚合聚类-amp-分裂聚类"><a href="#聚合聚类-amp-分裂聚类" class="headerlink" title="聚合聚类 &amp; 分裂聚类"></a>聚合聚类 &amp; 分裂聚类</h3><ul>
<li><p><strong>聚合聚类（自下而上聚类）</strong></p>
<ul>
<li>开始将每个样本各自分到一个类</li>
<li>之后将相距最近的两类合并，建立一个新的类</li>
<li>重复此操作直到满足停止条件</li>
</ul>
</li>
<li><p><strong>分裂聚类（自上而下聚类）</strong></p>
<ul>
<li>开始将所有样本分到一个类</li>
<li>之后将已有类中相距最远的样本分到两个新的类</li>
<li>重复此操作直到满足停止条件</li>
</ul>
</li>
</ul>
<h2 id="2-K-mean"><a href="#2-K-mean" class="headerlink" title="2. K-mean"></a>2. K-mean</h2><ul>
<li>硬聚类、分成 k 个簇、非层次化、无监督学习</li>
<li>时间复杂度：$O(nkm)$，n个样本点到k个中心的距离，维度为m。</li>
</ul>
<h3 id="1-核心原理"><a href="#1-核心原理" class="headerlink" title="(1). 核心原理"></a>(1). 核心原理</h3><p>算法的核心目标是将 $n$ 个样本点划分到 $K$ 个簇 (Cluster) 中，使得每个样本点到其所属簇的中心的距离平方和最小。</p>
<ul>
<li><strong>输入</strong>：样本集 $X$，聚类簇数 $K$。</li>
<li><strong>输出</strong>：$K$ 个簇的质心向量 $\mu$ 及每个样本的类别标签。</li>
</ul>
<h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="(2). 损失函数"></a>(2). 损失函数</h3><p>最小化<strong>每个簇内样本点到中心的距离之和</strong>：</p>
<script type="math/tex; mode=display">
J = \sum_{k=1}^{K} \sum_{x \in C_k} || x - \mu_k ||^2</script><p>其中：</p>
<ul>
<li>$C_k$：第 $k$ 个簇。</li>
<li>$\mu_k$：第 $k$ 个簇的中心。</li>
<li>$|| x - \mu_k ||^2$：欧几里得距离的平方。</li>
</ul>
<h3 id="3-算法流程"><a href="#3-算法流程" class="headerlink" title="(3). 算法流程"></a>(3). 算法流程</h3><p>K-Means 的迭代过程体现了 EM (Expectation-Maximization) 的思想：</p>
<ol>
<li><p><strong>初始化</strong>：<br>随机选择 $K$ 个样本点作为初始中心 ${ \mu_1, \dots, \mu_K }$。</p>
</li>
<li><p><strong>分配阶段</strong> —— <em>对应 E-step</em><br>计算每个样本 $x^{(i)}$ 到各个中心的距离，将其分配给最近的中心：</p>
<script type="math/tex; mode=display">
c^{(i)} = \arg\min_k || x^{(i)} - \mu_k ||^2</script></li>
<li><p><strong>更新阶段</strong> —— <em>对应 M-step</em><br>重新计算每个簇的<strong>中心</strong>：</p>
<script type="math/tex; mode=display">
\mu_k = \frac{1}{|C_k|} \sum_{x \in C_k} x</script></li>
<li><p><strong>收敛判断</strong>：<br>重复步骤 2 和 3，直到中心位置不再变化或变化非常小（小于阈值）。</p>
</li>
</ol>
<h3 id="4-初值选择的解决（K-means-）"><a href="#4-初值选择的解决（K-means-）" class="headerlink" title="(4). 初值选择的解决（K-means++）"></a>(4). 初值选择的解决（K-means++）</h3><ul>
<li><strong>问题</strong>：传统的随机初始化可能导致陷入局部最优解。</li>
<li><strong>解法：K-Means++</strong>：<ul>
<li>第一个质心随机选。</li>
<li>后续质心选择时，距离当前已有质心<strong>越远的点，被选中的概率越大</strong>。</li>
<li>以此优化初始质心的分布。</li>
</ul>
</li>
</ul>
<p>（优点：减少局部最优风险，加快收敛速度，更好聚类效果）</p>
<h3 id="5-优缺点分析"><a href="#5-优缺点分析" class="headerlink" title="(5). 优缺点分析"></a>(5). 优缺点分析</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>原理简单</strong>，易于理解和实现。</td>
<td style="text-align:left"><strong>需要预先指定 K 值（k-means++/elbow方法）</strong>，且 K 值对结果影响大。</td>
</tr>
<tr>
<td style="text-align:left"><strong>收敛速度快</strong>，时间复杂度近乎线性 $O(N \cdot K \cdot M)$。</td>
<td style="text-align:left"><strong>对初始中心选择敏感</strong>，可能陷入局部最优。</td>
</tr>
<tr>
<td style="text-align:left">适合处理球状分布、密集的簇。</td>
<td style="text-align:left"><strong>噪声敏感</strong>，噪声会拉偏均值。</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>无法处理非凸形状，且高维数据处理能力差</strong> 。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="第十一章-奇异值分解"><a href="#第十一章-奇异值分解" class="headerlink" title="第十一章 奇异值分解"></a>第十一章 奇异值分解</h1><h2 id="1-基础概念"><a href="#1-基础概念" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h2><p><strong>矩阵分解的目的：</strong>方便计算、方便储存、数据压缩。</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n} =diag（\sigma_1,\sigma_2...\sigma_n）</script><p>其中，$U、V$ 都是正交矩阵，$\Sigma$ 是对角阵（从大到小排）；diag（ ）为奇异值的对角阵。</p>
<ul>
<li>奇异值分解不要求A是方阵。</li>
<li><strong>奇异值分解基本定理</strong>：奇异值分解对任意实矩阵都存在。</li>
</ul>
<h3 id="1-奇异值分解基本定理证明"><a href="#1-奇异值分解基本定理证明" class="headerlink" title="(1). 奇异值分解基本定理证明"></a>(1). 奇异值分解基本定理证明</h3><ul>
<li>证明不要求。</li>
</ul>
<h3 id="2-SVD-的几何意义"><a href="#2-SVD-的几何意义" class="headerlink" title="(2). SVD 的几何意义"></a>(2). SVD 的几何意义</h3><ul>
<li>$m \times n$ 矩阵A: <strong>表示从 n 维空间 $R_n$ 到 m 维空间 $R_m$ 的一个线性变换</strong>。</li>
</ul>
<p>对于任意矩阵 $A = U \Sigma V^T$，其线性变换的几何过程可以分解为：</p>
<ol>
<li><strong>旋转 ($V^T$)</strong>：<ul>
<li>在输入空间（$R_n$）中进行旋转。</li>
<li>目的是找到一组正交基（主方向），将其对齐到坐标轴。</li>
</ul>
</li>
<li><strong>拉伸 ($\Sigma$)</strong>：<ul>
<li>沿坐标轴方向进行缩放。</li>
<li>缩放倍率即为<strong>奇异值</strong> $\sigma_i$。</li>
<li>这一步将“单位圆/球”变换为“椭圆/椭球”。</li>
</ul>
</li>
<li><strong>旋转 ($U$)</strong>：<ul>
<li>将拉伸后的椭圆旋转到输出空间（$R_m$）的目标方向。</li>
<li>$U$ 的列向量即为椭圆的主轴方向。</li>
</ul>
</li>
</ol>
<p><strong>直观理解</strong>：任何线性变换本质上都是把一个圆（或球）拉伸成一个椭圆（或椭球），SVD 让这个过程直观化。</p>
<h3 id="3-奇异值分解的不唯一性"><a href="#3-奇异值分解的不唯一性" class="headerlink" title="(3). 奇异值分解的不唯一性"></a>(3). 奇异值分解的不唯一性</h3><ul>
<li>$\Sigma$ 是唯一的，但是 $U、V$ 是不唯一的。</li>
</ul>
<h2 id="2-紧奇异值分解-amp-截断奇异值分解"><a href="#2-紧奇异值分解-amp-截断奇异值分解" class="headerlink" title="2. 紧奇异值分解 &amp; 截断奇异值分解"></a>2. 紧奇异值分解 &amp; 截断奇异值分解</h2><h3 id="1-紧奇异值分解"><a href="#1-紧奇异值分解" class="headerlink" title="(1). 紧奇异值分解"></a>(1). 紧奇异值分解</h3><p><strong>核心概念</strong>：去伪存真。只保留<strong>非零</strong>奇异值及其对应的奇异向量（即秩 $r$）。</p>
<ul>
<li><strong>定义</strong>：去掉 $\Sigma$ 中为 0 的奇异值，以及 $U$ 和 $V$ 中对应的列（这些列对应于零空间，对重构 $A$ 没有贡献）。</li>
<li><strong>公式</strong>：<script type="math/tex; mode=display">
  A = U_r \Sigma_r V_r^T</script></li>
<li><strong>维度</strong>：<ul>
<li>$U_r$: $m \times r$</li>
<li>$\Sigma_r$: $r \times r$ (无零元素的方阵)</li>
<li>$V_r^T$: $r \times n$</li>
</ul>
</li>
<li><strong>特点</strong>：<strong>无损分解</strong>。计算结果严格等于原矩阵 $A$，相比完整 SVD 更节省存储空间。</li>
</ul>
<h3 id="2-截断奇异值分解"><a href="#2-截断奇异值分解" class="headerlink" title="(2). 截断奇异值分解"></a>(2). 截断奇异值分解</h3><p><strong>核心概念</strong>：抓大放小。只保留<strong>前 $k$ 个</strong>最大的奇异值（$k &lt; r$）。</p>
<ul>
<li><strong>定义</strong>：人为设定超参数 $k$，认为较小的奇异值是噪音并将其舍弃，仅保留主要特征。</li>
<li><strong>公式</strong>：<script type="math/tex; mode=display">
  A \approx A_k = U_k \Sigma_k V_k^T</script></li>
<li><strong>维度</strong>：<ul>
<li>$U_k$: $m \times k$</li>
<li>$\Sigma_k$: $k \times k$</li>
<li>$V_k^T$: $k \times n$</li>
</ul>
</li>
<li><strong>特点</strong>：<strong>有损近似</strong>。它是原矩阵 $A$ 的最佳低秩近似（Best Low-Rank Approximation），常用于<strong>LSA（潜在语义分析）</strong>、<strong>图像压缩</strong>和<strong>降噪</strong>。 </li>
</ul>
<h2 id="3-奇异值分解的计算"><a href="#3-奇异值分解的计算" class="headerlink" title="3. 奇异值分解的计算"></a>3. 奇异值分解的计算</h2><h3 id="1-计算流程"><a href="#1-计算流程" class="headerlink" title="(1). 计算流程"></a>(1). 计算流程</h3><ol>
<li><p>求右奇异向量 $V$</p>
<ul>
<li><strong>构建对称阵</strong>：计算 $W = A^T A$。</li>
<li><strong>特征分解</strong>：求 $W$ 的特征值 $\lambda_i$ 和特征向量 $v_i$。</li>
<li><strong>排序</strong>：将特征值按降序排列 $\lambda_1 \ge \lambda_2 \ge \dots \ge 0$。</li>
<li><strong>单位化</strong>：将特征向量归一化（长度为1）。</li>
<li><strong>构造 $V$</strong>：$V = [v_1, v_2, \dots, v_n]$。</li>
</ul>
</li>
<li><p>求奇异值矩阵 $\Sigma$</p>
<ul>
<li><strong>计算奇异值</strong>：$\sigma_i = \sqrt{\lambda_i}$。</li>
<li><strong>构造 $\Sigma$</strong>：将 $\sigma_i$ 填入对角矩阵的主对角线，其余位置补 0。</li>
</ul>
</li>
<li><p>求左奇异向量 $U$</p>
<ul>
<li><strong>非零奇异值部分</strong>：利用公式直接计算对应的 $u_i$：<script type="math/tex; mode=display">
u_i = \frac{1}{\sigma_i} A v_i</script></li>
<li><strong>零奇异值/维度缺失部分</strong>：若 $\sigma_i = 0$ （直接找与 $u_i$ 垂直的向量即可）或矩阵 $A$ 行数大于列数（$m &gt; n$），需利用 <strong>Gram-Schmidt 正交化</strong> 在 $U$ 的正交补空间中寻找剩余的单位向量，以补全标准正交基。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>注意</strong>：此方法（利用 $A^T A$）仅适用于手算练习。计算机数值计算通常采用 <strong>Golub-Kahan 双对角化算法</strong>，以避免 $A^T A$ 带来的条件数平方和精度损失。</p>
</blockquote>
<h3 id="2-矩阵的外积展开式（重点看一下）"><a href="#2-矩阵的外积展开式（重点看一下）" class="headerlink" title="(2). 矩阵的外积展开式（重点看一下）"></a>(2). 矩阵的外积展开式（重点看一下）</h3><script type="math/tex; mode=display">
A = U\Sigma V^T, \Sigma = \text{diag}(\sigma_1, \sigma_2, \cdots, \sigma_p)</script><ul>
<li>矩阵 $A$ 的奇异值分解 $U\Sigma V^T$ 也可以由 <strong>外积</strong> 形式表示</li>
<li>将 $A$ 的奇异值分解看成矩阵 $U\Sigma$ 和 $V^T$ 的乘积</li>
<li>将 $U\Sigma$ 按照列向量分块，将 $V^T$ 按行向量分块，即得<script type="math/tex; mode=display">
  U\Sigma = [\sigma_1 u_1 \quad \sigma_2 u_2 \quad \cdots \quad \sigma_n u_n]</script><script type="math/tex; mode=display">
  V^T = \begin{bmatrix} v_1^T \\ v_2^T \\ \vdots \\ v_n^T \end{bmatrix}</script></li>
<li>则<script type="math/tex; mode=display">
  A = (U\Sigma)V^T = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \cdots + \sigma_n u_n v_n^T</script></li>
</ul>
<p><strong>最优近似矩阵：</strong></p>
<ul>
<li>若 $A$ 的秩为 $r$，则<script type="math/tex; mode=display">
  A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \cdots + \sigma_r u_r v_r^T</script></li>
<li>设矩阵<script type="math/tex; mode=display">
  A_k = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \cdots + \sigma_k u_k v_k^T</script></li>
<li>矩阵 $A_k$ 就是 $A$ 的截断奇异值分解</li>
<li>$A_k$ 的秩为 $k$，并且 $A_k$ 是秩为 $k$ 的矩阵中在 <strong>弗罗贝尼乌斯范数意义</strong> 下 $A$ 的 <strong>最优近似矩阵</strong></li>
<li>由于通常奇异值递减很快，所以 $k$ 取很小值时，$A_k$ 也可以对 $A$ 有很好的近似</li>
</ul>
<p><strong>矩阵的 F 范数：</strong></p>
<ul>
<li>衡量两个矩阵的相似度。  </li>
</ul>
<p>设矩阵 $A \in R^{m \times n}$， $A$ 的奇异值分解为 $U \Sigma V^T$，其中 $\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_n)$，则：</p>
<script type="math/tex; mode=display">
\|A\|_F = (\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_n^2)^{\frac{1}{2}}</script><p><strong>定理15.3：</strong><br><img src="2025-12-09-21-02-32.png" alt=""></p>
<ul>
<li>$X$ 是 $A$ 的最优近似，因为它们的 F 范数最小。</li>
<li>k 截断的矩阵等价于最优近似。</li>
</ul>
<h2 id="SVD的性质（5条）"><a href="#SVD的性质（5条）" class="headerlink" title="SVD的性质（5条）"></a>SVD的性质（5条）</h2><p><img src="2025-12-18-14-42-08.png" alt=""><br><img src="2025-12-18-14-41-12.png" alt=""><br><img src="2025-12-18-14-41-33.png" alt=""><br><img src="2025-12-18-14-42-29.png" alt=""></p>
<h1 id="第十二章-主成分分析"><a href="#第十二章-主成分分析" class="headerlink" title="第十二章 主成分分析"></a>第十二章 主成分分析</h1><p>关键词：<code>无监督、降维、效率高、更好泛化能力</code></p>
<ul>
<li>尽量减少信息损失的情况下，把复杂高维数据压缩成简单低维数据。</li>
<li>利用正交变换把由线性相关变量表示的观测数据转换为少数几个由<strong>线性无关</strong>变量 (主成分) 表示的数据。</li>
</ul>
<p><strong>在数据总体/有限样本上进行的主成分分析称为总体/样本主成分分析：</strong><br><em>总体主成分分析是样本主成分分析的基础</em></p>
<h2 id="1-总体主成分分析"><a href="#1-总体主成分分析" class="headerlink" title="1. 总体主成分分析"></a>1. 总体主成分分析</h2><ul>
<li>选择<strong>方差最大</strong>的方向 (第一主成分) 作为新坐标系的第一坐标轴, 即y1轴。</li>
<li>选择与第一坐标轴正交, 且方差次之的方向 (第二主成分) 作为新坐标系的第二坐标轴, 即y2轴。</li>
</ul>
<p><img src="2025-12-11-15-37-07.png" alt=""></p>
<h2 id="2-样本主成分分析"><a href="#2-样本主成分分析" class="headerlink" title="2. 样本主成分分析"></a>2. 样本主成分分析</h2><p><img src="2025-12-11-15-34-08.png" alt=""></p>
<h2 id="3-定理-16-1"><a href="#3-定理-16-1" class="headerlink" title="3. 定理 16.1"></a>3. 定理 16.1</h2><p><strong>1. 定理 16.1</strong> 设 $x$ 是 $m$ 维随机变量，$\Sigma$ 是 $x$ 的协方差矩阵，$\Sigma$ 的特征值分别是 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_m \ge 0$，特征值对应的单位特征向量分别是 $\alpha_1, \alpha_2, \cdots, \alpha_m$，则 $x$ 的第 $k$ 主成分是</p>
<script type="math/tex; mode=display">y_k = \alpha_k^T x = \alpha_{1k}x_1 + \alpha_{2k}x_2 + \cdots + \alpha_{mk}x_m, k=1, 2, \cdots, m \quad (16.5)</script><p>$x$ 的第 $k$ 主成分的方差是</p>
<script type="math/tex; mode=display">\text{var}(y_k) = \alpha_k^T \Sigma \alpha_k = \lambda_k, k=1, 2, \cdots, m \quad (16.6)</script><p>即协方差矩阵 $\Sigma$ 的第 $k$ 个特征值。</p>
<p><strong>2. 例题 16.1：</strong></p>
<p><strong>例 16.1</strong> 假设有 $n$ 个学生参加四门课程的考试，将学生们的考试成绩看作随机变量的取值，对考试成绩数据进行标准化处理，得到样本相关矩阵 $R$，列于表 16.1。</p>
<p><strong>表 16.1 样本相关矩阵 $R$</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">语文</th>
<th style="text-align:center">外语</th>
<th style="text-align:center">数学</th>
<th style="text-align:center">物理</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">语文</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.44</td>
<td style="text-align:center">0.29</td>
<td style="text-align:center">0.33</td>
</tr>
<tr>
<td style="text-align:left">外语</td>
<td style="text-align:center">0.44</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.35</td>
<td style="text-align:center">0.32</td>
</tr>
<tr>
<td style="text-align:left">数学</td>
<td style="text-align:center">0.29</td>
<td style="text-align:center">0.35</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:left">物理</td>
<td style="text-align:center">0.33</td>
<td style="text-align:center">0.32</td>
<td style="text-align:center">0.60</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>试对数据进行主成分分析。</p>
<p><strong>解</strong> 设变量 $x_1, x_2, x_3, x_4$ 分别表示语文、外语、数学、物理的成绩。对样本相关矩阵进行特征值分解，得到相关矩阵的特征值，并按大小排序：</p>
<script type="math/tex; mode=display">\lambda_1 = 2.17, \lambda_2 = 0.87, \lambda_3 = 0.57, \lambda_4 = 0.39</script><p>这些特征值就是各主成分的方差贡献率。假设要求主成分的累计方差贡献率大于 75%，那么只需取前两个主成分即可，即 $k=2$，因为</p>
<script type="math/tex; mode=display">\frac{\lambda_1 + \lambda_2}{\sum_{j=1}^{4} \lambda_j} = 0.76</script><p>求出对应于特征值 $\lambda_1, \lambda_2$ 的单位特征向量，列于表 16.2，表中最后一列为主成分的方差贡献率。</p>
<p><strong>表 16.2 单位特征向量和主成分的方差贡献率</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">项目</th>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$x_3$</th>
<th style="text-align:center">$x_4$</th>
<th style="text-align:center">方差贡献率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$y_1$</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">0.476</td>
<td style="text-align:center">0.523</td>
<td style="text-align:center">0.537</td>
<td style="text-align:center">0.543</td>
</tr>
<tr>
<td style="text-align:left">$y_2$</td>
<td style="text-align:center">0.574</td>
<td style="text-align:center">0.486</td>
<td style="text-align:center">-0.476</td>
<td style="text-align:center">-0.456</td>
<td style="text-align:center">0.218</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
\text{方差贡献率} =\frac{该成分特征值}{特征值总和}</script><p>由此按照式(16.50)可得第一、第二主成分：</p>
<script type="math/tex; mode=display">y_1 = 0.460x_1 + 0.476x_2 + 0.523x_3 + 0.537x_4</script><script type="math/tex; mode=display">y_2 = 0.574x_1 + 0.486x_2 - 0.476x_3 - 0.456x_4</script><p>这就是主成分分析的结果。变量 $y_1$ 和 $y_2$ 表示第一、第二主成分。</p>
<p>接下来由特征值和单位特征向量求出第一、第二主成分的因子负荷量，以及第一、第二主成分对变量 $x_i$ 的贡献率，列于表 16.3。</p>
<p><strong>表 16.3 主成分的因子负荷量和贡献率</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">项目</th>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$x_3$</th>
<th style="text-align:center">$x_4$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$y_1$</td>
<td style="text-align:center">0.678</td>
<td style="text-align:center">0.701</td>
<td style="text-align:center">0.770</td>
<td style="text-align:center">0.791</td>
</tr>
<tr>
<td style="text-align:left">$y_2$</td>
<td style="text-align:center">0.536</td>
<td style="text-align:center">0.453</td>
<td style="text-align:center">-0.444</td>
<td style="text-align:center">-0.425</td>
</tr>
<tr>
<td style="text-align:left">$y_1, y_2$ 对 $x_i$ 的贡献率</td>
<td style="text-align:center">0.747</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.790</td>
<td style="text-align:center">0.806</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
\text{因子负荷量} = \sqrt{\text{特征值}} \times \text{对应特征向量分量}</script><script type="math/tex; mode=display">
\text{贡献率}= \Sigma(\text{因子负荷量})^2</script><p>从表 16.3 中可以看出，第一主成分 $y_1$ 对应的因子负荷量 $\rho(y_1, x_i), i=1,2,3,4$，均为正数，表明各门课程成绩提高都可使 $y_1$ 提高，也就是说，第一主成分 $y_1$ 反映了学生的整体成绩，还可以看出，因子负荷量的数值相近，且 $\rho(y_1, x_4)$ 的数值最大，这表明物理成绩在整体成绩中占最重要位置。</p>
<p>第二主成分 $y_2$ 对应的因子负荷量 $\rho(y_2, x_i), i=1,2,3,4$，有正有负，正的是语文和外语，负的是数学和物理，表明文科成绩提高都可使 $y_2$ 提高，而理科成绩提高都可使 $y_2$ 降低，也就是说，第二主成分 $y_2$ 反映了学生的文科成绩与理科成绩的关系。</p>
<p>图 16.3 将原变量 $x_1, x_2, x_3, x_4$（分别表示语文、外语、数学、物理）和主成分 $y_1, y_2$（分别表示整体成绩、文科对理科成绩）的因子负荷量在平面坐标系中表示，可以看出变量之间的关系，4 个原变量聚成了两类：因子负荷量相近的语文、外语为一类，数学、物理为一类，前者反映文科课程成绩，后者反映理科课程成绩。</p>
<p><img src="2025-12-11-14-44-52.png" alt=""><br>(横轴为 y1 整体成绩，纵轴为 y2 文科成绩)</p>
<p><strong>图 16.3 因子负荷量的分布图</strong>  </p>
<ul>
<li>未知相关系数矩阵 R 的情况：</li>
</ul>
<p><img src="2025-12-11-15-44-54.png" alt=""></p>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
  <entry>
    <title>智能无人系统</title>
    <url>/2025/12/24/AutoDriving/</url>
    <content><![CDATA[<h1 id="第一章-认识无人驾驶系统"><a href="#第一章-认识无人驾驶系统" class="headerlink" title="第一章 认识无人驾驶系统"></a>第一章 认识无人驾驶系统</h1><h2 id="1-无人驾驶分级标准"><a href="#1-无人驾驶分级标准" class="headerlink" title="1. 无人驾驶分级标准"></a>1. 无人驾驶分级标准</h2><p><img src="2025-12-24-05-17-53.png" alt=""><br><img src="2025-12-24-05-18-20.png" alt=""></p>
<h2 id="2-无人驾驶系统框架"><a href="#2-无人驾驶系统框架" class="headerlink" title="2. 无人驾驶系统框架"></a>2. 无人驾驶系统框架</h2><p><img src="2025-12-24-05-23-00.png" alt=""></p>
<h1 id="第二章-定位方法以及传感器应用"><a href="#第二章-定位方法以及传感器应用" class="headerlink" title="第二章 定位方法以及传感器应用"></a>第二章 定位方法以及传感器应用</h1><blockquote>
<p><strong>章节核心逻辑</strong>：无人驾驶系统如何解决两个终极问题？</p>
<ol>
<li><strong>“感官”</strong>：用传感器感知周围环境（雷达、相机）。</li>
<li><strong>“大脑”</strong>：用算法（定位、滤波）确定自己在哪、状态如何。</li>
</ol>
</blockquote>
<p><strong>SLAN</strong>：同步定位与地图构建。主要有：激光SLAN &amp; 视觉SLAN</p>
<h2 id="第一部分：硬件传感器"><a href="#第一部分：硬件传感器" class="headerlink" title="第一部分：硬件传感器"></a>第一部分：硬件传感器</h2><h3 id="1-激光雷达（激光SLAN）"><a href="#1-激光雷达（激光SLAN）" class="headerlink" title="1. 激光雷达（激光SLAN）"></a>1. 激光雷达（激光SLAN）</h3><ul>
<li><strong>原理</strong>：<strong>飞行时间法 (ToF)</strong>。发射激光束，计算往返时间测距。<ul>
<li>公式：$r = \frac{1}{2} c t$</li>
</ul>
</li>
<li><strong>特性</strong>：<ul>
<li>3D激光雷达采集到的信息叫做<strong>点云</strong>。将点云与全局地图进行迭代匹配（ICP）</li>
<li>✅ <strong>精度极高</strong>，测距远，分辨率高，隐蔽性好，抗干扰能力强</li>
<li>❌ <strong>成本高</strong>，易受<strong>雨雪雾</strong>天气干扰（噪声大）</li>
</ul>
</li>
<li><strong>考点</strong>：它是L4/L5级自动驾驶的核心传感器。</li>
</ul>
<h3 id="2-摄像头"><a href="#2-摄像头" class="headerlink" title="2. 摄像头"></a>2. 摄像头</h3><ul>
<li><strong>原理</strong>：被动接收环境光。</li>
<li><strong>分类与应用</strong> (🎯 <strong>学长考题重点</strong>)：<ul>
<li><strong>单目 (Monocular)</strong>：前视感知，识别人、车、车道线、红绿灯。</li>
<li><strong>双目 (Stereo)</strong>：通过视差原理测距。</li>
<li><strong>鱼眼 (Fisheye)</strong>：<strong>视野极大 (&gt;180°)</strong>，专门用于<strong>倒车影像、360°环视</strong>（图像有畸变）。</li>
</ul>
</li>
<li><strong>特性</strong>：<ul>
<li>✅ 纹理信息丰富，便宜。</li>
<li>❌ 受光照影响大（逆光、黑夜致盲），测距精度不如雷达。</li>
</ul>
</li>
</ul>
<h3 id="3-毫米波雷达"><a href="#3-毫米波雷达" class="headerlink" title="3. 毫米波雷达"></a>3. 毫米波雷达</h3><ul>
<li><strong>原理</strong>：多普勒效应。</li>
<li><strong>特性</strong>：<ul>
<li>✅ <strong>穿透力强</strong>（无视雨雪雾），<strong>测速</strong>特别准。</li>
<li>❌ 分辨率低（看不清物体轮廓，只能知道“有东西”）。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="第二部分：定位算法"><a href="#第二部分：定位算法" class="headerlink" title="第二部分：定位算法"></a>第二部分：定位算法</h2><h3 id="1-激光SLAN：ICP-vs-NDT-⭐⭐⭐-必考对比"><a href="#1-激光SLAN：ICP-vs-NDT-⭐⭐⭐-必考对比" class="headerlink" title="1. 激光SLAN：ICP vs NDT (⭐⭐⭐ 必考对比)"></a>1. 激光SLAN：ICP vs NDT (⭐⭐⭐ 必考对比)</h3><p><strong>（1）ICP</strong>：</p>
<p><img src="2025-12-27-12-37-38.png" alt=""><br><img src="2025-12-27-12-43-15.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">效果精确</td>
<td style="text-align:left">耗时大，效率低</td>
</tr>
<tr>
<td style="text-align:left">不必数据处理</td>
<td style="text-align:left">可能陷入局部最优</td>
</tr>
<tr>
<td style="text-align:left">较好收敛性</td>
<td style="text-align:left">需要噪声处理</td>
</tr>
</tbody>
</table>
</div>
<p><strong>（2）NDT</strong>：</p>
<p><img src="2025-12-27-12-55-49.png" alt=""><br><img src="2025-12-27-13-26-05.png" alt=""></p>
<p><strong>总结</strong>：<br><em>把“刚才看到的”（实时点云）和“记忆中的”（高精地图）拼在一起。</em></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left"><strong>ICP (迭代最近点)</strong></th>
<th style="text-align:left"><strong>NDT (正态分布变换)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>通俗理解</strong></td>
<td style="text-align:left"><strong>硬碰硬</strong>。每个点都去找地图上离它最近的点，强行拉近。</td>
<td style="text-align:left"><strong>概率流</strong>。把地图划成格子，看现在的点落在哪个概率最高的格子里。</td>
</tr>
<tr>
<td style="text-align:left"><strong>核心原理</strong></td>
<td style="text-align:left">最小化点对点欧氏距离。</td>
<td style="text-align:left">计算网格内的高斯概率密度函数 (PDF)。</td>
</tr>
<tr>
<td style="text-align:left"><strong>优点</strong></td>
<td style="text-align:left">精度极高（前提是初值给得好）。</td>
<td style="text-align:left"><strong>速度快</strong>，<strong>鲁棒性强</strong>（容错率高，初值差一点也能对上）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>缺点</strong></td>
<td style="text-align:left">慢，容易拼错（陷入局部最优）。</td>
<td style="text-align:left">精度略逊于精细的ICP。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-视觉SLAM"><a href="#2-视觉SLAM" class="headerlink" title="2. 视觉SLAM"></a>2. 视觉SLAM</h3><p><img src="2025-12-27-14-30-03.png" alt=""></p>
<ul>
<li><strong>视觉前端特征</strong> (🎯 <strong>填空题重点</strong>)：<ul>
<li><strong>角点 (Corner)</strong>：如 Harris, FAST。</li>
<li><strong>斑点 (Blob)</strong>：如 SIFT（最准最慢）。</li>
<li><strong>ORB特征</strong>：<strong>Oriented FAST</strong> (带方向角点) + <strong>BRIEF</strong> (二进制描述子)。<strong>速度极快</strong>，适合实时系统。</li>
</ul>
</li>
<li><strong>回环检测 (Loop Closure)</strong>：<ul>
<li>利用<strong>词袋模型 (BoW)</strong> 判断“我是否来过这里”，消除累积误差。</li>
</ul>
</li>
</ul>
<p><img src="2025-12-27-14-29-13.png" alt=""><br><img src="2025-12-27-14-29-25.png" alt=""></p>
<h2 id="第三部分：定位系统"><a href="#第三部分：定位系统" class="headerlink" title="第三部分：定位系统"></a>第三部分：定位系统</h2><h3 id="1-坐标系转换-⚠️-判断题陷阱"><a href="#1-坐标系转换-⚠️-判断题陷阱" class="headerlink" title="1. 坐标系转换 (⚠️ 判断题陷阱)"></a>1. 坐标系转换 (⚠️ 判断题陷阱)</h3><p><img src="2025-12-27-13-04-52.png" alt=""><br><img src="2025-12-27-13-05-50.png" alt=""></p>
<ul>
<li><strong>转换链条</strong>：<br>  <code>GPS输出的经纬度信息</code> $\rightarrow$ <code>ECEF (地心地固)</code> $\rightarrow$ <code>导航坐标系 (ENU/NED)</code> $\rightarrow$ <code>车体坐标系 (Body)</code></li>
<li><strong>关键点</strong>：<ul>
<li><strong>ENU</strong>：东北天坐标系（指北坐标系）。</li>
<li><strong>ECEF</strong>：随地球自转。</li>
<li><em>考题避坑</em>：通常<strong>不能</strong>直接从 ECEF 旋转得到车体坐标系，中间必须经过局部切平面（导航坐标系），否则无法描述“东南西北”。</li>
</ul>
</li>
</ul>
<h3 id="2-基于GPS-惯性组合（IMU）导航的定位系统"><a href="#2-基于GPS-惯性组合（IMU）导航的定位系统" class="headerlink" title="2. 基于GPS +惯性组合（IMU）导航的定位系统"></a>2. 基于GPS +惯性组合（IMU）导航的定位系统</h3><p><strong>（1）GPS</strong>：</p>
<p><img src="2025-12-27-13-27-04.png" alt=""></p>
<ul>
<li>RTK：载波相位动态实时差分，定位误差在城市中也可达到10～50m（一般GPS为10～100m）</li>
<li>差分GPS定位：通过在一个精确的已知位置（基准站）上安装GPS监测接收机，计算得到基准站与GPS卫星的距离，然后再根据误差修正结果，从而提高了定位精度。</li>
<li>伪距法定位：<ul>
<li>优点：速度快、无多值性问题、虽然精度低，但是满足大部分需求。 </li>
</ul>
</li>
</ul>
<p><img src="2025-12-27-13-32-18.png" alt=""></p>
<p><img src="2025-12-27-13-35-15.png" alt=""></p>
<p><strong>（2）惯性测量单元</strong>：</p>
<script type="math/tex; mode=display">
陀螺仪 + 加速度计</script><p><img src="2025-12-27-14-19-52.png" alt=""><br><img src="2025-12-27-14-21-05.png" alt=""></p>
<p><strong>总结</strong>：</p>
<ul>
<li><strong>GPS (全球定位系统)</strong>：提供<strong>绝对位置</strong>。<ul>
<li><em>缺点</em>：更新慢 (10Hz)，易被遮挡（隧道、高楼峡谷）。</li>
<li><em>RTK技术</em>：利用基准站差分，将精度提升至<strong>厘米级</strong>。</li>
</ul>
</li>
<li><strong>IMU (惯性测量单元)</strong>：提供<strong>相对推算</strong>。<ul>
<li><em>组成</em>：<strong>加速度计</strong>（测比力/加速度）+ <strong>陀螺仪</strong>（测角速度）。</li>
<li><em>原理</em>：航位推算。</li>
<li><em>缺点</em>：<strong>误差随时间累积</strong>（积分漂移）。</li>
</ul>
</li>
<li><strong>融合逻辑</strong>：<strong>GPS负责“纠偏”</strong>（消除累计误差），<strong>IMU负责“填空”</strong>（高频输出，保证信号不断）。</li>
</ul>
<p><strong>（3）惯性导航系统（INS）</strong>：<br><img src="2025-12-27-14-23-24.png" alt=""><br><img src="2025-12-27-14-23-36.png" alt=""><br><img src="2025-12-27-14-23-47.png" alt=""></p>
<h2 id="第四部分：卡尔曼滤波"><a href="#第四部分：卡尔曼滤波" class="headerlink" title="第四部分：卡尔曼滤波"></a>第四部分：卡尔曼滤波</h2><p>这一部分数学最难，重点掌握<strong>逻辑</strong>和<strong>公式</strong>。</p>
<ul>
<li><strong>卡尔曼滤波</strong>：是一种利用<strong>线性系统</strong>状态方程，通过系统输入输出观测数据，对系统状态进行最优估计的算法。</li>
<li><strong>基本思想</strong>：利用<code>前一时刻的估计值</code>和<code>现时刻的观测值</code>来更新对状态变量的估计，求出<code>现在时刻的估计值</code>。</li>
</ul>
<h3 id="1-卡尔曼滤波-KF-——-线性系统的最优估计"><a href="#1-卡尔曼滤波-KF-——-线性系统的最优估计" class="headerlink" title="1. 卡尔曼滤波 (KF) —— 线性系统的最优估计"></a>1. 卡尔曼滤波 (KF) —— 线性系统的最优估计</h3><ul>
<li><strong>核心逻辑</strong>：<strong>加权平均</strong>。<ul>
<li>模型算出一个值（预测），传感器测出一个值（测量）。</li>
<li><strong>谁方差小（谁靠谱），我就信谁多一点</strong>。</li>
</ul>
</li>
<li><strong>五大核心公式 (📝 建议默写)</strong>：<ol>
<li><strong>预测状态</strong>：$\hat{x}_k^- = A \hat{x}_{k-1} + B u_k$</li>
<li><strong>预测协方差</strong>：$P_k^- = A P_{k-1} A^T + Q$  <em>(Q: 模型噪声)</em></li>
<li><strong>卡尔曼增益</strong>：$K_k = P_k^- H^T (H P_k^- H^T + R)^{-1}$ <em>(R: 测量噪声)</em><ul>
<li><em>理解</em>：$R$ 越大（测量烂），$K$ 越小（不信测量）；$Q$ 越大（模型烂），$K$ 越大（信测量）。</li>
</ul>
</li>
<li><strong>状态更新</strong>：$\hat{x}_k = \hat{x}_k^- + K_k (z_k - H \hat{x}_k^-)$</li>
<li><strong>协方差更新</strong>：$P_k = (I - K_k H) P_k^-$</li>
</ol>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">实时性不能满足</td>
</tr>
<tr>
<td style="text-align:left">可靠性降低</td>
</tr>
<tr>
<td style="text-align:left">对于<strong>非线性</strong>系统效果不佳</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-进阶滤波：EKF-vs-UKF-⭐⭐-简答题"><a href="#2-进阶滤波：EKF-vs-UKF-⭐⭐-简答题" class="headerlink" title="2. 进阶滤波：EKF vs UKF (⭐⭐ 简答题)"></a>2. 进阶滤波：EKF vs UKF (⭐⭐ 简答题)</h3><p><em>解决非线性问题（如车在转弯、雷达测距是弧线）。</em></p>
<ul>
<li><strong>扩展卡尔曼滤波 (EKF)</strong>：<ul>
<li><strong>方法</strong>：<strong>线性化</strong>:用线性变换近似非线性变换。用<strong>泰勒级数展开</strong>（保留一阶导数），算出<strong>雅可比矩阵 (Jacobian)</strong>。</li>
<li><em>缺点</em>：雅可比矩阵难算，忽略高阶项有精度损失。</li>
<li><em>考点</em>：EKF近似的是<strong>非线性函数本身</strong>。</li>
</ul>
</li>
<li><strong>无损卡尔曼滤波 (UKF)</strong>：<ul>
<li><strong>方法</strong>：<strong>无损变换</strong>。选几个代表点（Sigma Points）去通过非线性函数，近似出新的<strong>概率分布</strong>。</li>
<li><em>优点</em>：不用算雅可比，精度通常更高。</li>
</ul>
</li>
</ul>
<h3 id="3-运动模型"><a href="#3-运动模型" class="headerlink" title="3. 运动模型"></a>3. 运动模型</h3><ul>
<li><strong>CTRV</strong> (Constant Turn Rate and Velocity)：<strong>恒定转弯率和速度</strong>。这是描述车辆运动最常用的模型。</li>
</ul>
<h3 id="4-传感器融合架构"><a href="#4-传感器融合架构" class="headerlink" title="4. 传感器融合架构"></a>4. 传感器融合架构</h3><ul>
<li><strong>顺序滤波 (Sequential Filtering)</strong>：<ul>
<li><strong>逻辑</strong>：一次预测，多次更新。</li>
<li><em>场景</em>：激光雷达和毫米波雷达的数据不同时到达。先用激光更新卡尔曼滤波，结果作为初值，马上再用毫米波更新。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="📝-考前抢分-CheckList-必背"><a href="#📝-考前抢分-CheckList-必背" class="headerlink" title="📝 考前抢分 CheckList (必背)"></a>📝 考前抢分 CheckList (必背)</h2><ol>
<li><strong>倒车影像</strong>用什么镜头？ $\rightarrow$ <strong>鱼眼 (Fisheye)</strong></li>
<li><strong>指北坐标系</strong>通常指什么？ $\rightarrow$ <strong>ENU (东北天)</strong></li>
<li><strong>ICP和NDT</strong>谁更快更鲁棒？ $\rightarrow$ <strong>NDT</strong></li>
<li><strong>EKF</strong>的核心数学操作是什么？ $\rightarrow$ <strong>泰勒展开 / 计算雅可比矩阵</strong></li>
<li><strong>SLAM</strong>常用视觉特征？ $\rightarrow$ <strong>ORB, SIFT, 角点</strong></li>
<li><strong>卡尔曼增益K</strong>变大说明什么？ $\rightarrow$ 说明更相信<strong>测量值</strong> (或者模型预测的不确定性Q变大了)。</li>
</ol>
<h1 id="第三章-深度学习和视觉感知"><a href="#第三章-深度学习和视觉感知" class="headerlink" title="第三章 深度学习和视觉感知"></a>第三章 深度学习和视觉感知</h1><h2 id="第一部分：路缘石检测"><a href="#第一部分：路缘石检测" class="headerlink" title="第一部分：路缘石检测"></a>第一部分：路缘石检测</h2><h3 id="1-ROI提取-感兴趣区域"><a href="#1-ROI提取-感兴趣区域" class="headerlink" title="1. ROI提取 (感兴趣区域)"></a>1. <strong>ROI提取 (感兴趣区域)</strong></h3><ul>
<li>获取原始点云，只保留车身周围一定范围内的点（去除无关背景）。</li>
</ul>
<h3 id="2-点集左右划分"><a href="#2-点集左右划分" class="headerlink" title="2. 点集左右划分"></a>2. <strong>点集左右划分</strong></h3><ul>
<li>将点云按照激光雷达的扫描线（车身前向射线）进行划分。</li>
</ul>
<p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" /></p>
<script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>

<style>
  .swiper {
    width: 100%;
    height: 500px; /* 如果图片觉得太小，可以把这里改成 500px 或 600px */
    border-radius: 8px; /* 圆角 */
  }
  .swiper-slide {
    text-align: center;
    background: #fff;
    display: flex;
    justify-content: center;
    align-items: center;
  }
  .swiper-slide img {
    width: 100%;
    height: 100%;
    object-fit: contain; /* 保持图片原有比例，不拉伸变形 */
  }
</style>

<div class="swiper mySwiper">
  <div class="swiper-wrapper">
    <div class="swiper-slide">
      <img src="2025-12-28-19-05-37.png" alt="Image 1">
    </div>
    <div class="swiper-slide">
      <img src="2025-12-28-19-05-49.png" alt="Image 2">
    </div>
    <div class="swiper-slide">
      <img src="2025-12-28-19-06-01.png" alt="Image 3">
    </div>
  </div>

  <div class="swiper-button-next"></div>
  <div class="swiper-button-prev"></div>
  <div class="swiper-pagination"></div>
</div>

<script>
  document.addEventListener("DOMContentLoaded", function() {
    var swiper = new Swiper(".mySwiper", {
      loop: true,               // 循环播放
      autoHeight: true,         // 自动适应高度
      navigation: {
        nextEl: ".swiper-button-next",
        prevEl: ".swiper-button-prev",
      },
      pagination: {
        el: ".swiper-pagination",
        clickable: true,        // 允许点击小圆点切换
      },
    });
  });
</script>

<h3 id="3-路缘石点提取"><a href="#3-路缘石点提取" class="headerlink" title="3. 路缘石点提取"></a>3. <strong>路缘石点提取</strong></h3><ul>
<li>在每一条射线上，利用几何特征（如高度突变、坡度变化）找到疑似路缘的点。</li>
<li>左右子点集划分：将找到的点分为“左侧路缘点集”和“右侧路缘点集”。</li>
</ul>
<p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" /></p>
<script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>

<style>
  .swiper {
    width: 100%;
    height: 450px; /* 如果图片较高，可以将这里改成 500px 或 600px */
    border-radius: 8px;
  }
  .swiper-slide {
    text-align: center;
    background: transparent;
    display: flex;
    justify-content: center;
    align-items: center;
  }
  .swiper-slide img {
    width: 100%;
    height: 100%;
    object-fit: contain; /* 保持图片完整显示，不裁剪 */
  }
</style>

<div class="swiper mySwiper">
  <div class="swiper-wrapper">
    <div class="swiper-slide">
      <img src="2025-12-28-19-09-06.png" alt="Image 1">
    </div>
    <div class="swiper-slide">
      <img src="2025-12-28-19-09-13.png" alt="Image 2">
    </div>
    <div class="swiper-slide">
      <img src="2025-12-28-19-09-23.png" alt="Image 3">
    </div>
    <div class="swiper-slide">
      <img src="2025-12-28-19-09-33.png" alt="Image 4">
    </div>
  </div>

  <div class="swiper-button-next"></div>
  <div class="swiper-button-prev"></div>
  <div class="swiper-pagination"></div>
</div>

<script>
  document.addEventListener("DOMContentLoaded", function() {
    var swiper = new Swiper(".mySwiper", {
      loop: true,               // 循环播放
      navigation: {
        nextEl: ".swiper-button-next",
        prevEl: ".swiper-button-prev",
      },
      pagination: {
        el: ".swiper-pagination",
        clickable: true,        // 允许点击圆点切换
      },
    });
  });
</script>

<h3 id="4-提取与拟合"><a href="#4-提取与拟合" class="headerlink" title="4. 提取与拟合"></a>4. <strong>提取与拟合</strong></h3><ul>
<li>利用最小二乘法或RANSAC算法，将点拟合成平滑的曲线（通常是三次样条曲线），得到最终的路缘线。</li>
</ul>
<p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" /></p>
<script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>

<style>
  .swiper {
    width: 100%;
    height: 450px; /* 高度可按需调整 */
    border-radius: 8px;
    margin-bottom: 20px; /* 增加一点底部间距 */
  }
  .swiper-slide {
    text-align: center;
    background: transparent;
    display: flex;
    justify-content: center;
    align-items: center;
  }
  .swiper-slide img {
    width: 100%;
    height: 100%;
    object-fit: contain;
  }
</style>

<div class="swiper mySwiper3">
  <div class="swiper-wrapper">
    <div class="swiper-slide"><img src="2025-12-28-19-11-56.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-12-11.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-12-28.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-12-37.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-12-47.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-12-59.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-13-10.png"></div>
  </div>

  <div class="swiper-button-next"></div>
  <div class="swiper-button-prev"></div>
  <div class="swiper-pagination"></div>
</div>

<script>
  document.addEventListener("DOMContentLoaded", function() {
    var swiper3 = new Swiper(".mySwiper3", {
      loop: true,
      navigation: {
        nextEl: ".mySwiper3 .swiper-button-next", // 指定只控制这一组的箭头
        prevEl: ".mySwiper3 .swiper-button-prev",
      },
      pagination: {
        el: ".mySwiper3 .swiper-pagination", // 指定只控制这一组的分页器
        clickable: true,
      },
    });
  });
</script>

<p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" /></p>
<script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>

<style>
  .swiper {
    width: 100%;
    height: 450px;
    border-radius: 8px;
    margin-bottom: 20px;
  }
  .swiper-slide {
    text-align: center;
    background: transparent;
    display: flex;
    justify-content: center;
    align-items: center;
  }
  .swiper-slide img {
    width: 100%;
    height: 100%;
    object-fit: contain;
  }
</style>

<div class="swiper mySwiper4">
  <div class="swiper-wrapper">
    <div class="swiper-slide"><img src="2025-12-28-19-13-46.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-13-57.png"></div>
    <div class="swiper-slide"><img src="2025-12-28-19-14-07.png"></div>
  </div>

  <div class="swiper-button-next"></div>
  <div class="swiper-button-prev"></div>
  <div class="swiper-pagination"></div>
</div>

<script>
  document.addEventListener("DOMContentLoaded", function() {
    var swiper4 = new Swiper(".mySwiper4", {
      loop: true,
      navigation: {
        nextEl: ".mySwiper4 .swiper-button-next", // 锁定只控制这一组
        prevEl: ".mySwiper4 .swiper-button-prev",
      },
      pagination: {
        el: ".mySwiper4 .swiper-pagination",
        clickable: true,
      },
    });
  });
</script>

<p><strong>补充</strong>：坐标系变换<br><img src="2025-12-28-19-15-03.png" alt=""></p>
<h2 id="第二部分：卷积神经网络（CNN）"><a href="#第二部分：卷积神经网络（CNN）" class="headerlink" title="第二部分：卷积神经网络（CNN）"></a>第二部分：卷积神经网络（CNN）</h2><ol>
<li><p><strong>卷积与前馈神经网络区别</strong>：</p>
<ul>
<li>用卷积运算代替矩阵乘法运算</li>
<li>传统全连接神经网络不足：<em>计算/储存要求高（参数大）、易过拟合</em></li>
<li><strong>CNN优势</strong>：<em>稀疏交互、权重共享、等变表示</em></li>
</ul>
</li>
<li><p><strong>CNN的三大特性</strong>：</p>
<ul>
<li><strong>稀疏交互</strong>：卷积核大小远小于输入图像，每个输出神经元只与局部输入连接（效率高）。</li>
<li><strong>参数共享</strong>：同一个卷积核在整张图上滑动，用的是同一组参数（储存空间少）。</li>
<li><strong>等变表示</strong>：输入平移，输出也跟着平移。</li>
</ul>
</li>
<li><p><strong>核心组件</strong>：</p>
<ul>
<li><strong>卷积层</strong>：提取特征。</li>
<li><strong>池化层</strong>：用相邻位置的<strong>总体统计特征</strong>来替换该位置的值，起到降维、减少参数、防止过拟合的作用。<em>（最大池化、平均池化）</em></li>
<li><strong>激活函数</strong>：引入非线性（如 ReLU）。</li>
</ul>
</li>
</ol>
<h2 id="第三部分：YOLO"><a href="#第三部分：YOLO" class="headerlink" title="第三部分：YOLO"></a>第三部分：YOLO</h2><p><img src="2025-12-28-20-29-46.png" alt=""></p>
<h3 id="1-YOLO的核心思想"><a href="#1-YOLO的核心思想" class="headerlink" title="1. YOLO的核心思想"></a>1. <strong>YOLO的核心思想</strong></h3><ul>
<li>不需要生成候选区域，直接把图像划分成 S×S 的网格，每个网格负责预测中心落在该格子里面的物体。</li>
<li>速度快，适合无人驾驶实时感知。</li>
</ul>
<h3 id="2-YOLOv3的特征提取"><a href="#2-YOLOv3的特征提取" class="headerlink" title="2. YOLOv3的特征提取"></a>2. <strong>YOLOv3的特征提取</strong></h3><p><strong>特点</strong>：</p>
<ul>
<li>有53个卷积层</li>
<li>输出图像尺寸：416x416x3 </li>
<li>连续使用卷积层和跳跃连接层</li>
</ul>
<p><strong>图像金字塔</strong>：<br><img src="2025-12-28-20-44-17.png" alt=""></p>
<ul>
<li><strong>图像金字塔-向下采样</strong>：删除图像偶数行列，大小变为四分之一。</li>
<li><strong>图像金字塔-向上采样</strong>：补充大量像素点0，大小变为四倍。<br><em>（注意：两种操作互相是<strong>不可逆</strong>的。）</em></li>
</ul>
<p><strong>多尺度特征预测</strong>：<br><img src="2025-12-28-20-52-14.png" alt=""><br><img src="2025-12-28-20-48-17.png" alt=""></p>
<p><strong>交并比</strong>：<br><img src="2025-12-28-20-56-57.png" alt=""></p>
<p><strong>边界框坐标计算</strong>：<br><img src="2025-12-28-20-59-15.png" alt=""></p>
<ul>
<li>$(b_x,b_y)$为<strong>中心坐标</strong>。</li>
<li>$b_w,b_h$进行了<strong>尺度缩放</strong>。</li>
<li><strong>Sigmoid函数作用</strong>：将$t_x,t_y$压缩到[0,1]区间内，<strong>确保预测边界在网格内，防止偏移过多</strong>。</li>
<li><strong>指数函数e的作用</strong>：确保<strong>缩放后的宽高为正</strong>。</li>
</ul>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
  <entry>
    <title>专业前沿讲座报告</title>
    <url>/2025/11/26/lecture-report/</url>
    <content><![CDATA[<blockquote>
<p><strong>课程信息</strong></p>
<ul>
<li><strong>课程名称：</strong> 专业前沿讲座</li>
<li><strong>学期：</strong> 2025—2026学年第 1 学期</li>
<li><strong>作者：</strong> 宋文韬 (智机试验2305 / 23013361)</li>
</ul>
</blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>当前，人工智能与机器人技术正经历高速发展，多智能体协同、复杂系统优化等成为研究热点。文章结合《专业前沿讲座》课程内容及相关文献研读，梳理与分析了多智能体感知决策、大规模全局优化、工业互联网信息融合与安全以及机器学习在科学探索中的前沿应用。通过对这些技术在原理、实例及前沿研究上的综合分析，本报告总结了当前人工智能技术发展的内在规律与主流趋势，进一步形成了对未来研究方向与科研方法的初步认识与思考。</p>
<p><strong>关键词：</strong> 人工智能与机器人、多智能体、全局优化、工业互联网、机器学习</p>
<hr>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>当今世界，新一轮科技革命和产业变革正在深入发展，人工智能与机器人技术已成为国际竞争的新高地，也是推动经济社会发展的核心引擎。从自动驾驶车队的协同作业到智慧工厂的精准制造，从复杂系统的调度优化到基础科学领域的突破性发现，智能科学技术正在以前所未有的速度重塑着我们对世界的认知与改造能力。作为信息学院的学生，置身于这一时代的浪潮中，不仅需要掌握扎实的工程基础，更需要具备广阔的国际视野，深刻理解技术演进的内在逻辑与未来趋势。</p>
<p>本报告将结合课程讲授内容与相关前沿文献的研读体会，分为三个部分展开：首先，探讨从单体智能向多智能体协同跨越的技术逻辑，以及大规模全局优化算法在其中的核心作用；其次，分析工业互联网背景下，信息融合技术如何赋能实体经济，以及随之而来的信息安全挑战与伦理责任；最后，展望机器学习在科学探索与增进民生福祉中的革命性应用。通过对这些前沿技术的综合阐述，本报告旨在总结当前人工智能技术发展的规律，并结合个人思考，形成对未来科研方向与职业规范的初步认识与体会。</p>
<h2 id="2-群体智能与复杂系统的优化决策"><a href="#2-群体智能与复杂系统的优化决策" class="headerlink" title="2. 群体智能与复杂系统的优化决策"></a>2. 群体智能与复杂系统的优化决策</h2><h3 id="2-1-多智能体系统的协同感知与决策"><a href="#2-1-多智能体系统的协同感知与决策" class="headerlink" title="2.1 多智能体系统的协同感知与决策"></a>2.1 多智能体系统的协同感知与决策</h3><p>在面对动态、非结构化或大规模任务环境如无人机集群协同、智慧交通调度等时，单体智能往往受限于感知范围与执行能力。多智能体系统（MAS）通过多个智能体间的交互与协作，能够涌现出超越个体之和的群体智能。</p>
<h4 id="2-1-1-分布式感知与博弈决策"><a href="#2-1-1-分布式感知与博弈决策" class="headerlink" title="2.1.1 分布式感知与博弈决策"></a>2.1.1 分布式感知与博弈决策</h4><p>智能感知与决策是多智能体系统的核心要素。与单体系统不同，多智能体面临着环境的“非平稳性”挑战——即一个智能体的策略更新会改变其他智能体面临的环境状态。根据 Zhang 等人（2021）的研究 [1]，解决这一问题的关键在于引入博弈论框架与多智能体强化学习（MARL）算法。通过建立纳什均衡（Nash Equilibrium）或相关均衡机制，智能体能够在竞争或合作中找到最优策略组合。</p>
<img src="/2025/11/26/lecture-report/1.png" class="" title="图 1 从单体决策到多智能体博弈的模型演变 [1]">
<blockquote>
<p><strong>图 1 说明：</strong> (a) 单体智能体面临的马尔可夫决策过程 (MDP)；(b) 多智能体系统中的马尔可夫博弈 (Markov Game) 模型，展示了多个智能体在共享环境中通过联合动作与环境交互并获取反馈的过程。这种从“中心化控制”向“分布式协同决策”的转变，是当前无人系统发展的必然趋势，也体现了智能时代的核心技术特征。</p>
</blockquote>
<h4 id="2-1-2-协同控制的实现路径"><a href="#2-1-2-协同控制的实现路径" class="headerlink" title="2.1.2 协同控制的实现路径"></a>2.1.2 协同控制的实现路径</h4><p>在具体的工程实践中，多智能体协同不仅需要算法支撑，还依赖于高效的通信拓扑与共识机制。通过对“智能感知与决策关键技术”的学习，我认识到，如何在大规模群体中实现低延迟的信息交互，并基于局部信息达成全局一致性，是目前工业界与学术界共同关注的热点。</p>
<h3 id="2-2-基于分解的大规模全局优化策略"><a href="#2-2-基于分解的大规模全局优化策略" class="headerlink" title="2.2 基于分解的大规模全局优化策略"></a>2.2 基于分解的大规模全局优化策略</h3><p>随着智能体数量的增加和系统精度的提升，优化问题的决策变量往往会膨胀至数千甚至上万维。课程第二章重点讲授了“基于分解的大规模全局优化新思考”，这为解决高维复杂系统的“维数灾难”提供了核心方法论。</p>
<h4 id="2-2-1-分解策略"><a href="#2-2-1-分解策略" class="headerlink" title="2.2.1 分解策略"></a>2.2.1 分解策略</h4><p>面对大规模全局优化（LSGO）问题，传统的进化算法往往因搜索空间过大而陷入局部最优。解决这一难题的关键在于“分解”。协同进化算法（CCEA）是处理此类问题的经典范式 [2]。其核心思想是将一个高维的大规模优化问题分解为若干个低维的子问题（Sub-components），针对每个子问题分别进行进化求解，最后协同组合成全局最优解。</p>
<h4 id="2-2-2-工程与科学思维的统一"><a href="#2-2-2-工程与科学思维的统一" class="headerlink" title="2.2.2 工程与科学思维的统一"></a>2.2.2 工程与科学思维的统一</h4><p>通过对“基于分解的大规模全局优化应用实例”的学习，我深刻体会到，优化不仅是数学计算，更是一种系统工程思维。无论是在工业制造的过程控制中，还是在物流网络的路径规划中，利用分解策略将复杂系统模块化、层次化，是提升系统运行效率、降低计算代价的根本途径。这不仅锻炼了我们解决复杂工程问题的能力，也加深了对“工程素养”这一思政要点的理解。</p>
<h2 id="3-工业互联背景下的信息融合与安全防线"><a href="#3-工业互联背景下的信息融合与安全防线" class="headerlink" title="3. 工业互联背景下的信息融合与安全防线"></a>3. 工业互联背景下的信息融合与安全防线</h2><h3 id="3-1-多源异构信息的深度融合"><a href="#3-1-多源异构信息的深度融合" class="headerlink" title="3.1 多源异构信息的深度融合"></a>3.1 多源异构信息的深度融合</h3><p>课程讲授中提到，大数据的处理分析与挖掘是理解智能时代的关键。通过学习“信息融合简介及应用”，我理解到信息融合不仅仅是数据的简单叠加，而是对多源信息进行多级别、多层次的处理。例如，在高端装备的运维中，需要将物理量与文本信息进行融合。这种融合往往需要结合“过程机理”与“大数据建模”。即利用深度学习算法挖掘数据特征的同时，必须引入物理机理模型作为约束，才能在工业场景下实现精准的状态监测与故障诊断。这不仅提升了系统的感知精度，也为辅助决策提供了可靠依据。</p>
<h3 id="3-2-工业控制系统的安全策略"><a href="#3-2-工业控制系统的安全策略" class="headerlink" title="3.2 工业控制系统的安全策略"></a>3.2 工业控制系统的安全策略</h3><p>随着信息技术与运营技术的深度融合，封闭的工业控制系统逐渐开放，随之而来的是暴露面的扩大和安全风险的剧增。课程强调了“信息安全算法”与“科学伦理”的重要性，这引发了我对工程师社会责任的深层思考。</p>
<h4 id="3-2-1-工业安全的特殊性和严峻挑战"><a href="#3-2-1-工业安全的特殊性和严峻挑战" class="headerlink" title="3.2.1 工业安全的特殊性和严峻挑战"></a>3.2.1 工业安全的特殊性和严峻挑战</h4><p>与传统互联网安全仅关注数据隐私不同，工业互联网安全直接关系到物理世界的安全。根据 Sadeghi 等人（2020）在《IEEE工业电子汇刊》上的分析，工业物联网（IIoT）面临着特有的安全与隐私挑战 [3]。一次针对化工控制系统的恶意攻击，不仅会导致商业机密的泄露，更可能引发设备损坏、环境污染甚至人员伤亡。因此，在设计智能系统时，我们必须遵循“Security by Design”（设计内建安全）的原则。不仅要研究异常检测与加密算法等技术防御手段，更要从系统架构层面考虑功能的安全性。</p>
<h4 id="3-2-2-工程伦理与社会责任"><a href="#3-2-2-工程伦理与社会责任" class="headerlink" title="3.2.2 工程伦理与社会责任"></a>3.2.2 工程伦理与社会责任</h4><p>我们不仅要追求算法的效率，更要时刻关注工程实践对公众安全、健康及环境的影响。在构建工业互联网平台时，必须自觉履行保护数据隐私、维护国家基础设施安全的社会责任，将科学伦理融入技术研发的每一个环节。这部分的学习使我对“职业规范”有了更具象的理解。</p>
<h2 id="4-从经典机器学习算法到科技前沿"><a href="#4-从经典机器学习算法到科技前沿" class="headerlink" title="4. 从经典机器学习算法到科技前沿"></a>4. 从经典机器学习算法到科技前沿</h2><h3 id="4-1-经典机器学习模型的科学适用性"><a href="#4-1-经典机器学习模型的科学适用性" class="headerlink" title="4.1 经典机器学习模型的科学适用性"></a>4.1 经典机器学习模型的科学适用性</h3><p>在本课程的学习过程中，我经历了一个由浅入深、从经典统计模型向深度神经网络进阶的认知过程。</p>
<p>最初，我从基于统计学习理论的支持向量机（SVM）入手，理解了如何通过数学变换处理小样本数据；随后，接触了以随机森林为代表的集成学习算法，领悟了通过“群体决策”降低模型方差的智慧；最后，探究了以卷积神经网络（CNN）为核心的深度学习架构，掌握了处理高维非结构化数据的关键技术。这一学习路径不仅夯实了我的算法基础，也让我深刻认识到不同模型在科学探索中独特的适用场景。</p>
<h4 id="4-1-1-支持向量机-SVM"><a href="#4-1-1-支持向量机-SVM" class="headerlink" title="4.1.1 支持向量机 (SVM)"></a>4.1.1 支持向量机 (SVM)</h4><p>作为基于统计学习理论的经典算法，SVM 在处理小样本、高维度的科学实验数据时表现出色。在课程讨论中，我们了解到在某些材料分类或生物标记物检测的场景下，由于实验数据获取昂贵且稀缺，深度学习难以收敛，而SVM 通过核函数将数据映射到高维空间寻找最优超平面的能力，使其依然具有不可替代的价值 [4]。</p>
<img src="/2025/11/26/lecture-report/2.png" class="" title="图 2 Kernel Function: 将低维数据映射到高维空间">
<h4 id="4-1-2-集成学习-Ensemble-Learning"><a href="#4-1-2-集成学习-Ensemble-Learning" class="headerlink" title="4.1.2 集成学习 (Ensemble Learning)"></a>4.1.2 集成学习 (Ensemble Learning)</h4><p>以随机森林（Random Forest）为代表的集成算法，通过构建多棵决策树来降低模型的方差。在涉及多变量耦合的复杂物理实验数据分析中，这类算法不仅能提供较高的预测精度，还能输出“特征重要性”排序 [5]。这种可解释性对于科学家理解哪些物理量对实验结果影响最大至关重要。</p>
<h4 id="4-1-3-深度神经网络-Deep-Learning"><a href="#4-1-3-深度神经网络-Deep-Learning" class="headerlink" title="4.1.3 深度神经网络 (Deep Learning)"></a>4.1.3 深度神经网络 (Deep Learning)</h4><p>从经典的感知机发展而来的深度学习算法是本课程课外自主学习的重点。特别是卷积神经网络（CNN），作为当前处理网格结构数据（如天文望远镜拍摄的星系图、电子显微镜下的材料微观结构）的主流架构，展现了强大的特征提取能力 [6]。该算法突破了传统方法的局限，能够自动从原始像素中学习到从边缘到纹理再到语义的高层特征，实现了端到端的智能感知。</p>
<img src="/2025/11/26/lecture-report/3.png" class="" title="图 3 深度卷积神经网络的典型架构">
<blockquote>
<p><strong>图 3 说明：</strong> 该图展示了网络处理图像的完整流程：输入图像经过多个卷积层（Convolutions）与池化层（Pooling）的交替处理，逐层提取从低级几何特征到高级语义概念的层级化表示，最终实现精准分类。</p>
</blockquote>
<h3 id="4-2-从传统科学到新的科学探索"><a href="#4-2-从传统科学到新的科学探索" class="headerlink" title="4.2 从传统科学到新的科学探索"></a>4.2 从传统科学到新的科学探索</h3><p>传统的科学研究模式往往依赖于“观察—假设—实验”的线性循环，但在面对高维非线性问题时，人类的认知直觉与传统计算模拟（如第一性原理计算）的效率均遭遇了显著瓶颈。</p>
<p>根据相关前沿学术综述研究 [7]，深度学习模型通过学习海量科学数据中的高维分布特征，能够构建出逼近自然界物理定律的代理模型（Surrogate Models）。这一过程并非简单的数据插值与拟合，而是利用神经网络强大的泛化能力，在高维参数空间中寻找隐藏的科学规律。例如，生成式人工智能（Generative AI）目前已能自动生成符合特定物理化学约束的新分子结构，这种技术成功实现了“假设生成—验证”闭环的自动化，将科学发现的周期大大缩短，确立了全新的科学探索范式。</p>
<img src="/2025/11/26/lecture-report/4.png" class="" title="图 4 人工智能驱动的科学发现闭环与应用版图">
<blockquote>
<p><strong>图 4 说明：</strong> 该图展示了AI for Science的核心范式：通过加速“观察—假设—实验”的迭代闭环，人工智能打破了学科壁垒，被广泛应用于天气预报、电池材料优化、核聚变控制及生物医疗序列建模等前沿领域，确立了区别于传统实验与理论计算的第四种科学范式。</p>
</blockquote>
<h3 id="4-3-机器学习应用趋势"><a href="#4-3-机器学习应用趋势" class="headerlink" title="4.3 机器学习应用趋势"></a>4.3 机器学习应用趋势</h3><p>结合当前的社会现实，我深刻感受到，人类的日常生活以及离不开智能技术的辅助。未来的智能技术将不仅仅追求算力的突破，更将致力于提升人类的生活质量，实现真正的“科技向善”。目前，机器学习已经渗透进人类生活的方方面面：</p>
<ul>
<li><strong>个性化推荐系统：</strong> 当我们打开购物软件或短视频平台时，背后的推荐算法（Recommendation Systems）正在利用机器学习技术分析我们的历史行为与偏好。它帮助我们在海量信息中快速筛选出感兴趣的内容，极大地降低了信息检索的成本，提升了生活效率。</li>
<li><strong>智能语音与交互：</strong> 从手机里的智能助手（如 Siri、小爱同学）到智能家居系统，自然语言处理（NLP）技术让机器能够“听懂”人类的指令。未来的智能家居将不再是简单的遥控，而是通过感知用户的习惯自动调节环境（如温度、灯光），提供更加人性化的居住体验。</li>
<li><strong>自动驾驶辅助：</strong> 在出行领域，计算机视觉技术正在让汽车变得更加智能。目前的 L2+ 级辅助驾驶功能已经能够实现车道保持、自适应巡航，有效降低了驾驶疲劳与交通事故的发生率。</li>
<li><strong>生成式协作：</strong> 同时，以 ChatGPT 为代表的生成式大模型（Generative AI）的兴起，标志着人工智能从“感知”走向了“生成”。在我们的日常学习与工作中，AI 正在成为最得力的工具。它不仅能帮助我们快速梳理文献、润色邮件、编写代码，还能在创意枯竭时提供灵感。这种人机协作的模式并没有取代人类，而是赋予了我们更强大的信息处理能力与创造力，让我们能够将精力集中在更具价值的逻辑思考与决策上。</li>
</ul>
<h2 id="5-结语与体会"><a href="#5-结语与体会" class="headerlink" title="5. 结语与体会"></a>5. 结语与体会</h2><p>通过本学期《专业前沿讲座》的学习，我完成了从人工智能基础概念到机器人工程前沿应用，再到机器学习赋能科学发现的知识图谱构建。课程讲授的多智能体协同、大规模全局优化、工业互联网信息安全以及机器学习科学应用等模块，并非孤立的技术点，而是构成智能系统的有机整体。</p>
<p>我深刻体会到，现代智能科学正朝着“群体化、复杂化、落地化”演进：多智能体解决了感知广度问题，分解优化策略突破了计算维度限制，而工业互联网与信息安全则保障了技术的稳健落地。这种从系统工程高度审视技术脉络的宏观视角，不仅极大增强了我的专业学习兴趣，更让我对未来的科研方向与职业规范有了清晰认识。</p>
<p>我认识到，单纯的算法优化已遭遇边际效应递减，真正的创新往往源于“AI+X”的跨学科交叉融合。无论是利用深度学习加速新材料研发，还是解决复杂的物流调度，都要求我们具备开阔的国际视野，打破学科壁垒，将智能技术作为解决国计民生核心难题的通用工具。同时，技术是把双刃剑，强大的算力必须配合正确的价值观。面对全球老龄化等严峻挑战，融合机器学习的智慧医疗与养老系统虽能解决社会痛点，但其底线必须建立在安全可控之上。作为未来的工程师，我们既要利用先进算法实现精准筛查与情感陪伴，更要时刻警惕隐私泄露与算法歧视等风险，坚定推动“负责任的人工智能”建设，确保技术在严守伦理规范的前提下真正赋能人类的美好生活，让科技的光辉温暖社会的每一个角落。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” <em>IEEE Transactions on Automatic Control</em>, vol. 66, no. [cite_start]12, pp. 5690–5708, 2021. [cite: 56, 57]</li>
<li>X. Li, K. Tang, M. N. Omidvar, Z. Yang, and K. Qin, “A survey on cooperative co-evolutionary algorithms,” <em>IEEE Transactions on Evolutionary Computation</em>, vol. 17, no. [cite_start]4, pp. 471–496, 2013. [cite: 58, 59]</li>
<li>A.-R. Sadeghi, C. Wachsmann, and M. Waidner, “Security and privacy in industrial internet of things: Current status and potential future challenges,” <em>ACM Transactions on Embedded Computing Systems</em>, vol. 14, no. [cite_start]4, pp. 76:1–76:27, 2015. [cite: 60, 61]</li>
<li>C. Cortes and V. Vapnik, “Support-vector networks,” <em>Machine Learning</em>, vol. 20, no. [cite_start]3, pp. 273–297, 1995. [cite: 62]</li>
<li>G. Biau, “Analysis of a random forests model,” <em>Journal of Machine Learning Research</em>, vol. [cite_start]13, pp. 1063–1095, 2012. [cite: 63]</li>
<li>Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em>Nature</em>, vol. 521, no. [cite_start]7553, pp. 436–444, 2015. [cite: 64]</li>
<li>H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac, A. Anandkumar, K. Bergen, C. P. Gomes, S. Ho, P. Kohli, J. Lasenby, J. Leskovec, T. Liu, A. Manrai, D. Marks, B. Ramsundar, L. Song, J. Sun, J. Tang, P. Veličković, M. Zitnik, and M. Zitnik, “Scientific discovery in the age of artificial intelligence,” <em>Nature</em>, vol. 620, no. [cite_start]7972, pp. 47–60, 2023. [cite: 65, 66]</li>
</ol>
<blockquote>
<p><strong>附件下载：</strong> <a href="report.docx">📄 点击下载完整 Word 课程报告</a></p>
</blockquote>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
</search>
