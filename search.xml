<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>模式识别与统计学习</title>
    <url>/2025/11/27/statistical-learning/</url>
    <content><![CDATA[<h1 id="第一章-统计学习方法概述"><a href="#第一章-统计学习方法概述" class="headerlink" title="第一章 统计学习方法概述"></a>第一章 统计学习方法概述</h1><blockquote>
<p>❝ 机器学习是一门人工智能的科学, 该领域的主要研究对象是人工智能, 特别是如何在经验学习中改善具体算法的性能。 ❞</p>
<p>—— 机器学习是对能通过经验自动改进的计算机算法的研究。</p>
</blockquote>
<h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><ul>
<li><strong>概念</strong>：计算机基于数据构建概率统计<strong>模型</strong>，运用<strong>模型</strong>对数据进行预测和分析。</li>
<li><strong>研究对象</strong>：数据、基本假设。</li>
<li><strong>目的</strong>：对数据（特别是未知数据）进行预测和分析。</li>
</ul>
<p><strong>🤔 为什么需要机器学习？</strong></p>
<ol>
<li><strong>数据量太大</strong>：人脑无法处理海量维度。</li>
<li><strong>知识盲区</strong>：人类无法解释专业知识，或专业知识本身不存在。</li>
</ol>
<hr>
<h3 id="发展历程与三要素"><a href="#发展历程与三要素" class="headerlink" title="发展历程与三要素"></a>发展历程与三要素</h3><p>模式识别与机器学习发展历程：<br><img src="2025-11-27-15-37-49.png" alt=""></p>
<h3 id="核心：统计学三要素"><a href="#核心：统计学三要素" class="headerlink" title="核心：统计学三要素"></a>核心：统计学三要素</h3><script type="math/tex; mode=display">\text{方法} = \text{模型} + \text{策略} + \text{算法}</script><p><strong>1. 模型 (Model)</strong></p>
<ul>
<li><strong>非概率模型</strong>：<ul>
<li>假设空间：决策函数 $F=\{f \mid Y=f(X)\}$</li>
<li>参数空间：$F=\{f \mid Y=f_\theta(X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
<li><strong>概率模型</strong>：<ul>
<li>假设空间：条件概率 $F=\{P \mid P(Y|X)\}$</li>
<li>参数空间：$F=\{P \mid P_\theta(Y|X), \theta \in \mathbb{R}^n\}$</li>
</ul>
</li>
</ul>
<p><strong>2. 策略 (Strategy)</strong></p>
<ul>
<li><strong>损失函数</strong>：度量<strong>一次</strong>预测的好坏。</li>
<li><strong>风险函数</strong>：度量平均意义下的好坏（损失函数的期望）。<script type="math/tex; mode=display">R(f)=E_P[L(Y,f(X))]=\int L(y,f(x))P(x,y)dxdy</script></li>
</ul>
<p><strong>3. 算法 (Algorithm)</strong></p>
<ul>
<li>求解得到最优模型（通常指最优化问题的求解算法）。</li>
</ul>
<hr>
<h3 id="分类体系"><a href="#分类体系" class="headerlink" title="分类体系"></a>分类体系</h3><ul>
<li><strong>学习分类</strong>：监督学习、无监督学习、强化学习 （具体概念）</li>
<li><strong>模型分类</strong>：(非)概率、(非)线性、(非)参数、生成/判别</li>
</ul>
<hr>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><strong>⚡️ 快速记忆口诀：</strong></p>
<blockquote>
<p><span style="background-color:#fff5f5; color:#ff5050; padding:2px 6px; border-radius:4px;">先看预测，再看结果</span></p>
<ul>
<li>预测为<strong>真</strong> $\rightarrow$ <strong>P</strong> (Positive)；预测为<strong>假</strong> $\rightarrow$ <strong>N</strong> (Negative)</li>
<li>预测<strong>正确</strong> $\rightarrow$ <strong>T</strong> (True)；预测<strong>错误</strong> $\rightarrow$ <strong>F</strong> (False)</li>
</ul>
</blockquote>
<p><strong>核心指标：</strong></p>
<ul>
<li><strong>精确率 (Precision)</strong>：$P=\frac{TP}{TP+FP}$ <em>（你认为是对的里面，有多少真是对的？）</em></li>
<li><strong>召回率 (Recall)</strong>：$R=\frac{TP}{TP+FN}$ <em>（把精确率的 FP 换成 FN）</em></li>
<li><strong>F1值</strong>：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}$</li>
</ul>
<hr>
<h2 id="过拟合-Overfitting"><a href="#过拟合-Overfitting" class="headerlink" title="过拟合 (Overfitting)"></a>过拟合 (Overfitting)</h2><p>在<strong>训练数据</strong>上表现很好（把噪声也学进去了），对于<strong>未知数据</strong>表现很差。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>正则化</li>
<li>增大数据集</li>
<li>早停机制</li>
<li>减少模型参数</li>
</ul>
<h2 id="欠拟合-Underfitting"><a href="#欠拟合-Underfitting" class="headerlink" title="欠拟合 (Underfitting)"></a>欠拟合 (Underfitting)</h2><p><strong>模型过于简单</strong>，未能学习到数据中的基本规律，导致在<strong>训练集和未知数据</strong>上表现均不理想。</p>
<p><strong>解决办法：</strong></p>
<ul>
<li>增加模型复杂度</li>
<li>增加特征数量</li>
<li>减少或移除正则化参数</li>
<li>延长训练时间/充分训练  </li>
</ul>
<h1 id="第二章-线性感知机"><a href="#第二章-线性感知机" class="headerlink" title="第二章 线性感知机"></a>第二章 线性感知机</h1><h2 id="1-超平面-Hyperplane"><a href="#1-超平面-Hyperplane" class="headerlink" title="1. 超平面 (Hyperplane)"></a>1. 超平面 (Hyperplane)</h2><h3 id="怎么理解？"><a href="#怎么理解？" class="headerlink" title="怎么理解？"></a>怎么理解？</h3><blockquote>
<p><strong>定义</strong>：在 $n$ 维空间中，将空间一分为二的 $n-1$ 维子空间。</p>
</blockquote>
<ul>
<li><strong>超平面方程</strong>：<script type="math/tex; mode=display">w \cdot x + b = 0</script></li>
</ul>
<h3 id="点到超平面的几何距离"><a href="#点到超平面的几何距离" class="headerlink" title="点到超平面的几何距离"></a>点到超平面的几何距离</h3><script type="math/tex; mode=display">d = \frac{|w \cdot x + b|}{\|w\|}</script><hr>
<h2 id="2-感知机模型-Perceptron"><a href="#2-感知机模型-Perceptron" class="headerlink" title="2. 感知机模型 (Perceptron)"></a>2. 感知机模型 (Perceptron)</h2><blockquote>
<p><strong>关键词</strong>：<code>线性二分模型、判别模型</code></p>
</blockquote>
<h3 id="1-模型"><a href="#1-模型" class="headerlink" title="(1) 模型"></a>(1) 模型</h3><ul>
<li><strong>输入空间</strong>：$x \in R^n$</li>
<li><strong>输出空间</strong>：$y \in \{+1, -1\}$</li>
<li><p><strong>假设空间</strong>：</p>
<script type="math/tex; mode=display">f(x) = sign(w \cdot x + b)</script><p><strong>符号函数 (Sign Function)</strong>：</p>
<script type="math/tex; mode=display">
sign(x) =
\begin{cases}
+1, & x \geq 0 \\
-1, & x < 0
\end{cases}</script></li>
<li><p><strong>模型参数</strong>：</p>
<ul>
<li>$w$：权值向量</li>
<li>$b$：偏置</li>
<li>$w \cdot x$：内积</li>
</ul>
</li>
</ul>
<h3 id="2-策略"><a href="#2-策略" class="headerlink" title="(2) 策略"></a>(2) 策略</h3><p>感知机的目标是<strong>最小化误分类点到超平面的总距离。</strong></p>
<ul>
<li><strong>损失函数</strong>（误分类点集合 $M$）：<script type="math/tex; mode=display">
  L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script>  <em>(注：由于 $\frac{1}{|w|}$ 不影响梯度方向，为了计算简便，在损失函数中省略)</em></li>
</ul>
<h3 id="3-算法"><a href="#3-算法" class="headerlink" title="(3) 算法"></a>(3) 算法</h3><p>转化为求解最优化问题：</p>
<script type="math/tex; mode=display">
\min_{w, b} L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)</script><hr>
<h2 id="3-算法实现形式"><a href="#3-算法实现形式" class="headerlink" title="3. 算法实现形式"></a>3. 算法实现形式</h2><h3 id="1-梯度下降法"><a href="#1-梯度下降法" class="headerlink" title="(1) 梯度下降法"></a>(1) 梯度下降法</h3><ul>
<li><strong>输入</strong>：训练数据集 $T = {(x_1, y_1), \dots, (x_N, y_N)}$，学习率 $\eta$ ($0 &lt; \eta \le 1$)</li>
<li><strong>输出</strong>：$w, b$</li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$w_0 \leftarrow 0, b_0 \leftarrow 0$，$\eta$看题目</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件 $y_i (w \cdot x_i + b) \le 0$，则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
w &\leftarrow w + \eta y_i x_i \\
b &\leftarrow b + \eta y_i
\end{aligned}</script></li>
<li><strong>循环</strong>：转至 (2)，直至训练集中没有误分类点。</li>
</ol>
<blockquote>
<p><strong>特点</strong>：直观，但如果数据量大或维度高，计算量大。</p>
</blockquote>
<h3 id="2-对偶形式"><a href="#2-对偶形式" class="headerlink" title="(2) 对偶形式"></a>(2) 对偶形式</h3><ul>
<li><strong>核心思想</strong>：将 $w$ 表示为实例 $x_i$ 和标记 $y_i$ 的线性组合。</li>
<li><strong>输出</strong>：$\alpha, b$</li>
<li><strong>假设空间变为</strong>：<script type="math/tex; mode=display">f(x) = sign\left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right)</script>  <em>(其中 $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_N)^T$)</em></li>
</ul>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：$\alpha \leftarrow 0, b \leftarrow 0$</li>
<li><strong>选取数据</strong>：$(x_i, y_i)$</li>
<li><strong>判断误分类</strong>：<br>如果满足条件：<script type="math/tex; mode=display">
y_i \left(\sum_{j=1}^{N} \alpha_j y_j (x_j \cdot x_i) + b\right) \le 0</script>则更新参数：<script type="math/tex; mode=display">
\begin{aligned}
\alpha_i &\leftarrow \alpha_i + \eta \\
b &\leftarrow b + \eta y_i
\end{aligned}</script>（$b$和梯度下降一样）</li>
<li><strong>循环</strong>：转至 (2) 直到无误分类数据。</li>
</ol>
<h4 id="💡-重点：如何计算判别式："><a href="#💡-重点：如何计算判别式：" class="headerlink" title="💡 重点：如何计算判别式："></a>💡 重点：如何计算判别式：</h4><p>在对偶形式中，样本之间的内积计算非常频繁，通常预先计算 <strong>Gram 矩阵</strong> 并存储：</p>
<script type="math/tex; mode=display">
G = [x_i \cdot x_j]_{N \times N} =
\begin{bmatrix}
x_1 \cdot x_1 & x_1 \cdot x_2 & \cdots & x_1 \cdot x_N \\
x_2 \cdot x_1 & x_2 \cdot x_2 & \cdots & x_2 \cdot x_N \\
\vdots & \vdots & \ddots & \vdots \\
x_N \cdot x_1 & x_N \cdot x_2 & \cdots & x_N \cdot x_N \\
\end{bmatrix}</script><p><strong>注意：$x_i$ 有几个，$\alpha_i$ 就有几个 $=0$（初始化）</strong><br><strong>做题技巧（表格法迭代）</strong>：<br><img src="2025-11-30-18-07-41.png" alt=""></p>
<hr>
<h2 id="4-感知机算法收敛性定理"><a href="#4-感知机算法收敛性定理" class="headerlink" title="4. 感知机算法收敛性定理"></a>4. 感知机算法收敛性定理</h2><ul>
<li><p><strong>收敛定理 (Novikoff)</strong>：<br>  误分类次数 $k$ 有上界。这意味着，<strong>只要训练数据集是线性可分的，感知机算法一定能收敛</strong>（在有限步内找到解）。</p>
</li>
<li><p><strong>局限性</strong>：</p>
<ol>
<li><strong>解不唯一</strong>：解依赖于初值的选择，也依赖于误分类点的选择顺序。</li>
<li><strong>震荡</strong>：如果数据集<strong>线性不可分</strong>，算法会一直震荡，无法做到完全正确分类。</li>
</ol>
</li>
<li><p><strong>下一步引申</strong>：<br>  为了得到<strong>唯一</strong>且<strong>最优</strong>的分离超平面，我们需要引入更强的约束 —— 这就是 <strong>SVM (支持向量机)</strong> 的由来。  </p>
</li>
</ul>
<h1 id="第三章-K-近邻"><a href="#第三章-K-近邻" class="headerlink" title="第三章 $K$近邻"></a>第三章 $K$近邻</h1><h2 id="1-算法简介"><a href="#1-算法简介" class="headerlink" title="1. 算法简介"></a>1. 算法简介</h2><blockquote>
<p><strong>关键词</strong>：<code>分类模型、回归模型、适用于(数值型、标称型)数据、1-NN训练错误率=0</code></p>
</blockquote>
<h3 id="算法特点及优缺点"><a href="#算法特点及优缺点" class="headerlink" title="算法特点及优缺点"></a>算法特点及优缺点</h3><h4 id="懒惰学习-Lazy-Learning"><a href="#懒惰学习-Lazy-Learning" class="headerlink" title="懒惰学习 (Lazy Learning)"></a>懒惰学习 (Lazy Learning)</h4><blockquote>
<p><strong>定义</strong>：<strong>无实质性模型训练</strong>，利用训练集数据对特征空间进行划分。测试时，要计算输入实例距每个样本的距离。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优点 (Pros)</th>
<th style="text-align:left">缺点 (Cons)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>精确度高</strong></td>
<td style="text-align:left"><strong>时间复杂度高</strong>（花时间）</td>
</tr>
<tr>
<td style="text-align:left"><strong>对异常值不敏感</strong></td>
<td style="text-align:left"><strong>空间复杂度高</strong>（耗内存）</td>
</tr>
<tr>
<td style="text-align:left"><strong>无数据输入假定</strong>（无训练阶段）</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
</div>
<h3 id="K-值的选择"><a href="#K-值的选择" class="headerlink" title="$K$ 值的选择"></a>$K$ 值的选择</h3><ul>
<li><p><strong>$K$ 值较小</strong> (模型复杂)</p>
<ul>
<li>近似误差（<em>偏差</em>）减小 $\rightarrow$ 估计误差（<em>方差</em>）增大（对噪声敏感）</li>
<li><strong>噪声敏感</strong></li>
<li>整体模型变得复杂，容易发生<strong>过拟合</strong></li>
</ul>
</li>
<li><p><strong>$K$ 值较大</strong> (模型简单)</p>
<ul>
<li>近似误差增大 $\rightarrow$ 估计误差减小</li>
<li>整体的模型变得简单，但容易发生<strong>欠拟合</strong></li>
</ul>
</li>
<li><p><strong>通过交叉验证选择 $K$ 值</strong></p>
<ul>
<li>模型准确率随 $K$ 值的变化<strong>非单调</strong></li>
<li>$K$ 一般低于训练集样本容量的平方根：$k &lt; \sqrt{N}$</li>
<li>经验上，$k$ 值一般不超过 20：$k &lt; 20$</li>
</ul>
</li>
</ul>
<h3 id="kNN-算法的应用挑战"><a href="#kNN-算法的应用挑战" class="headerlink" title="$kNN$ 算法的应用挑战"></a>$kNN$ 算法的应用挑战</h3><ul>
<li><strong>距离函数确定</strong><ul>
<li>距离度量方式众多</li>
</ul>
</li>
<li><strong>$k$ 值确定</strong><ul>
<li>$k$ 值对模型精度的影响<strong>非单调</strong></li>
<li>边界易出错 (软分类)</li>
</ul>
</li>
<li><strong>特征的选择</strong><ul>
<li>不同特征的影响程度不同</li>
</ul>
</li>
<li><strong>复杂度</strong><ul>
<li>需要计算测试例 $x’$ 与所有训练数据的距离</li>
<li>计算复杂度正比于训练样本容量</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-距离度量"><a href="#2-距离度量" class="headerlink" title="2. 距离度量"></a>2. 距离度量</h2><h3 id="L-p-距离"><a href="#L-p-距离" class="headerlink" title="$L_p$ 距离"></a>$L_p$ 距离</h3><ul>
<li><p><strong>欧氏距离 ($L_2$)</strong>：</p>
<script type="math/tex; mode=display">L_2(x_i, x_j) = \left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^2\right)^{\frac{1}{2}}</script></li>
<li><p><strong>曼哈顿距离 ($L_1$)</strong>：</p>
<script type="math/tex; mode=display">L_1(x_i, x_j) = \sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|</script></li>
<li><p><strong>切比雪夫距离 ($L_{\infty}$)</strong>：（取同维度坐标差最大者）</p>
<script type="math/tex; mode=display">L_{\infty}(x_i, x_j) = \max_{l} |x_i^{(l)} - x_j^{(l)}|</script></li>
</ul>
<h3 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h3><ul>
<li>了解即可</li>
</ul>
<h3 id="距离度量的性质"><a href="#距离度量的性质" class="headerlink" title="距离度量的性质"></a>距离度量的性质</h3><ul>
<li><strong>非负性 (non-negativity)</strong>:<script type="math/tex; mode=display">D(a, b) \ge 0</script></li>
<li><strong>自反性 (reflexivity)</strong>:<script type="math/tex; mode=display">D(a, b) = 0 \iff a = b</script></li>
<li><strong>对称性 (symmetry)</strong>:<script type="math/tex; mode=display">D(a, b) = D(b, a)</script></li>
<li><strong>三角不等式 (triangle inequality)</strong>:<script type="math/tex; mode=display">D(a, b) + D(b, c) \ge D(a, c)</script></li>
</ul>
<hr>
<h2 id="3-分类决策规则"><a href="#3-分类决策规则" class="headerlink" title="3. 分类决策规则"></a>3. 分类决策规则</h2><ul>
<li><strong>核心规则</strong>：多数表决规则。</li>
<li><strong>理论意义</strong>：等价于 $0-1$ 损失函数下的<strong>经验风险最小化</strong>。</li>
<li><strong>误分类概率</strong>：在一个领域中，与领域样本特征不同的总数 $\div$ 领域样本总数。</li>
</ul>
<hr>
<h2 id="4-kd-树"><a href="#4-kd-树" class="headerlink" title="4. $kd$ 树"></a>4. $kd$ 树</h2><h3 id="搜索方法"><a href="#搜索方法" class="headerlink" title="搜索方法"></a>搜索方法</h3><p><img src="2025-12-01-15-48-55.png" alt=""></p>
<p><em>(注：$kd$ 树是二叉树)</em>  </p>
<ol>
<li>先在 $x$ 轴找到中位数，垂直划分。</li>
<li>再在划分的两块区域里找 $y$ 轴中位数，继续划分。</li>
<li>直至划分完毕。</li>
</ol>
<h3 id="kd-树计算复杂度"><a href="#kd-树计算复杂度" class="headerlink" title="$kd$ 树计算复杂度"></a>$kd$ 树计算复杂度</h3><ul>
<li><strong>一般情况</strong>：时间复杂度为 $O(\log N)$</li>
<li><strong>最坏情况</strong>：时间复杂度 $O(N)$</li>
<li><strong>适用场景</strong>：更适用于<strong>训练实例维度远大于空间维度</strong>时的搜索。  </li>
</ul>
<h1 id="第四章-贝叶斯方法"><a href="#第四章-贝叶斯方法" class="headerlink" title="第四章 贝叶斯方法"></a>第四章 贝叶斯方法</h1><h2 id="1-概率统计基础知识"><a href="#1-概率统计基础知识" class="headerlink" title="1. 概率统计基础知识"></a>1. 概率统计基础知识</h2><h3 id="pmf-amp-pdf"><a href="#pmf-amp-pdf" class="headerlink" title="$pmf$ &amp; $pdf$"></a>$pmf$ &amp; $pdf$</h3><p><strong>概率质量函数（Probability mass function）</strong><br>定义：$P(X=x_i)$，即<strong>离散随机变量</strong>在各特定取值上的概率。</p>
<ul>
<li>$\sum_{i} P(X=x_i) = 1$</li>
<li>$P(X=x_i \cap X=x_j) = 0 \text{ if } i \neq j$</li>
<li>$P(X=x_i \cup X=x_j) = P(X=x_i) + P(X=x_j) \text{ if } i \neq j$</li>
<li>$P(X=x_1 \cup X=x_2 \cup \dots \cup X=x_k) = 1$</li>
</ul>
<p><strong>概率密度函数（Probability density function）</strong><br>定义：描述了<strong>连续随机变量</strong> $x$ 的概率密度。</p>
<ul>
<li>$f(x) \ge 0, \forall x$</li>
<li>$\int_{-\infty}^{+\infty} f(x) = 1$</li>
<li>真实概率通过 pdf 的积分得到</li>
<li>e.g. $X$ 取 0-1 的概率为：<script type="math/tex; mode=display">P(0 \le X \le 1) = \int_{0}^{1} f(x) dx</script></li>
</ul>
<h3 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h3><p>描述两个或多个随机变量<strong>同时</strong>发生的概率。</p>
<ul>
<li><strong>离散型</strong>：$P(X=x, Y=y)$<ul>
<li>性质：$\sum_x \sum_y P(X=x, Y=y) = 1$</li>
</ul>
</li>
<li><strong>连续型</strong>：$f(x, y)$<ul>
<li>性质：$\iint f(x, y) dx dy = 1$</li>
</ul>
</li>
<li><strong>边缘分布 (Marginalization)</strong>：从联合分布中求出单个变量的分布。<script type="math/tex; mode=display">P(X=x) = \sum_y P(X=x, Y=y)</script></li>
<li><strong>独立性</strong>：若 $X$ 与 $Y$ 相互独立，则：<script type="math/tex; mode=display">P(X, Y) = P(X)P(Y)</script></li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>定义：在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。</p>
<script type="math/tex; mode=display">P(A|B) = \frac{P(A, B)}{P(B)}</script><p>（其中 $P(A, B)$ 是联合概率，$P(B)$ 是边缘概率）</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>将联合概率分解为条件概率的乘积。<br>对于两个变量：</p>
<script type="math/tex; mode=display">P(A, B) = P(A|B)P(B) = P(B|A)P(A)</script><p>推广到 $n$ 个变量：</p>
<script type="math/tex; mode=display">P(X_1, X_2, \dots, X_n) = P(X_1) \prod_{i=2}^{n} P(X_i | X_1, \dots, X_{i-1})</script><h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>如果事件组 $B_1, B_2, \dots, B_n$ 构成样本空间的一个划分（互斥且完备），则对于任意事件 $A$：</p>
<script type="math/tex; mode=display">P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i)</script><ul>
<li><strong>直观理解</strong>：求 $A$ 发生的概率，等于把 $A$ 在各种不同情况（$B_i$）下发生的概率加权求和。</li>
</ul>
<h3 id="均值与方差"><a href="#均值与方差" class="headerlink" title="均值与方差"></a>均值与方差</h3><ul>
<li><strong>期望 (Expectation / Mean)</strong> $\mu$：<ul>
<li>离散：$E[X] = \sum x_i P(x_i)$</li>
<li>连续：$E[X] = \int_{-\infty}^{+\infty} x f(x) dx$</li>
<li>性质：$E[aX + b] = aE[X] + b$</li>
</ul>
</li>
<li><strong>方差 (Variance)</strong> $\sigma^2$：衡量数据的离散程度。<script type="math/tex; mode=display">Var(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2</script></li>
</ul>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>定义：衡量两个变量 $X$ 和 $Y$ 总体误差的期望，反映它们之间变化的<strong>方向</strong>。</p>
<script type="math/tex; mode=display">Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)]</script><ul>
<li><strong>$Cov &gt; 0$</strong>：正相关</li>
<li><strong>$Cov &lt; 0$</strong>：负相关</li>
<li><strong>$Cov = 0$</strong>：不相关（线性无关）</li>
</ul>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p><strong>皮尔逊相关系数 (Correlation Coefficient)</strong> $\rho$：<br>将协方差归一化，消除了量纲的影响，衡量线性相关的强度。</p>
<script type="math/tex; mode=display">\rho_{X,Y} = \frac{Cov(X, Y)}{\sigma_x \sigma_y}</script><ul>
<li>取值范围：$[-1, 1]$。<br>（$1$ 为完全正相关，$-1$ 为完全负相关，$0$ 为无线性相关）</li>
</ul>
<h2 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2. 朴素贝叶斯"></a>2. 朴素贝叶斯</h2><h3 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h3><ul>
<li><strong><code>贝叶斯公式 (Bayes&#39; Theorem)</code></strong>：<script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)P(A)}{P(B)}</script><ul>
<li>$P(A|B)$: 后验概率 (Posterior)</li>
<li>$P(B|A)$: 似然 (Likelihood)</li>
<li>$P(A)$: 先验概率 (Prior)</li>
<li>$P(B)$: 证据 (Evidence)</li>
</ul>
</li>
<li><strong>贝叶斯决策</strong>：<ul>
<li>已知<strong>类条件概率密度</strong>参数表达式和<strong>先验概率</strong></li>
<li>利用<strong>贝叶斯公式</strong>转换成<strong>后验概率</strong></li>
<li>根据<strong>后验概率</strong>大小进行决策分类</li>
</ul>
</li>
</ul>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>基于贝叶斯定理与特征条件独立假设的分类算法。</p>
<ul>
<li><strong>“朴素” (Naive) 的含义</strong>：假设样本的各个特征之间<strong>相互独立</strong>。<script type="math/tex; mode=display">P(x_1, x_2, \dots, x_n | y) = \prod_{i=1}^{n} P(x_i | y)</script></li>
</ul>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>给定输入特征向量 $\mathbf{x} = (x_1, x_2, \dots, x_n)$，预测类别 $y$。<br>我们只需最大化分子部分<code>(这个公式考试一定要写)</code>：</p>
<script type="math/tex; mode=display">\hat{y} = \arg\max_{y} P(y) \prod_{i=1}^{n} P(x_i | y)</script><ul>
<li>$P(y)$：先验概率，即类别的频率。</li>
<li>$P(x_i|y)$：条件概率（似然），即在某类中观察到该特征的概率。</li>
</ul>
<h3 id="朴素贝叶斯参数估计"><a href="#朴素贝叶斯参数估计" class="headerlink" title="朴素贝叶斯参数估计"></a>朴素贝叶斯参数估计</h3><ol>
<li><strong>极大似然估计 (MLE)</strong><br>直接使用频率来估计概率。<ul>
<li>$P(y = c_k) = \frac{N_{c_k}}{N}$</li>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik}}{N_{c_k}}$</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i}{N}$</li>
</ul>
</li>
<li><strong>贝叶斯估计 (Bayes)</strong><br>引入平滑项（如拉普拉斯平滑）解决零概率问题。<ul>
<li>$P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}$</li>
<li>其中 $\lambda \ge 0$ 为平滑系数（常取 1），$S_i$ 为特征 $x_i$ 可能取值的个数。</li>
<li>$\hat{\theta} = \frac{\sum_{i=1}^{N} x_i+\alpha}{N+\alpha+\beta}$ ($\hat{\theta}$ ~ $Beta(\alpha,\beta)$)</li>
</ul>
</li>
<li><strong>拉普拉斯平滑 (Laplace Smoothing)</strong><br>解决零概率问题（即某个特征在训练集中未出现导致概率为0）。<script type="math/tex; mode=display">P(x_i = a | y = c_k) = \frac{N_{ik} + \lambda}{N_{c_k} + \lambda S_i}</script><ul>
<li>$\lambda$: 平滑系数（通常取 1）。</li>
<li>$S_i$: 特征 $x_i$ 可能取值的个数。</li>
</ul>
</li>
</ol>
<h1 id="第五章-决策树"><a href="#第五章-决策树" class="headerlink" title="第五章 决策树"></a>第五章 决策树</h1><h2 id="1-决策树模型"><a href="#1-决策树模型" class="headerlink" title="1. 决策树模型"></a>1. 决策树模型</h2><h3 id="决策树的表示"><a href="#决策树的表示" class="headerlink" title="决策树的表示"></a>决策树的表示</h3><ul>
<li>一种描述对实例分类的 <strong>树形结构</strong>，包含：<ul>
<li><strong>根结点 (Root Node)</strong>：最上面的结点，是整个决策树的开始。</li>
<li><strong>内部结点 (Internal Node)</strong>：代表一个<strong>问题</strong>或者<strong>决策</strong>，通常对应待<strong>分类对象的属性</strong>。</li>
<li><strong>叶结点 (Leaf Node)</strong>：代表一种可能的<strong>分类结果</strong>。</li>
<li><strong>有向边</strong>：连接各个结点。</li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>可解释性强</strong>：推理过程容易理解，可以表示成 <code>If-Then</code> 形式。</li>
<li><strong>依赖属性取值</strong>：推理过程完全依赖于属性变量的取值特点。</li>
<li><strong>特征筛选</strong>：可自动忽略对目标变量<strong>没有贡献的属性变量</strong>，为判断属性重要性、减少变量数目提供参考。</li>
</ul>
<h3 id="学习目的"><a href="#学习目的" class="headerlink" title="学习目的"></a>学习目的</h3><ul>
<li><strong>理想的决策树</strong>（通常有以下3种目标）：<ol>
<li>叶结点数最少</li>
<li>叶结点深度最小</li>
<li>叶结点数最少且叶结点深度最小</li>
</ol>
</li>
<li><strong>现实困境</strong>：找到这种绝对最优的决策树是 <strong>NP难题</strong>。</li>
<li><strong>实际目标</strong>：找到 <strong>尽可能</strong> 趋向于最优的决策树。</li>
</ul>
<hr>
<h2 id="2-特征选择与信息熵"><a href="#2-特征选择与信息熵" class="headerlink" title="2. 特征选择与信息熵"></a>2. 特征选择与信息熵</h2><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h3><ul>
<li><strong>定义</strong>：度量样本集合 <strong>纯度 (Purity)</strong> 最常用的一种指标，代表随机变量<strong>不确定性的度量</strong>。<blockquote>
<p><strong>规律</strong>：熵越大，随机变量的不确定性就越大。</p>
</blockquote>
</li>
<li><strong>计算公式</strong>：<br>  设 $X$ 是一个取有限个值的离散随机变量，其概率分布为 $P(X=x_i) = p_i, \quad i=1,2,\dots,n$，则随机变量 $X$ 的熵 $H(X)$ 定义为：<script type="math/tex; mode=display">H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i</script>  <strong>注</strong>：若 $p_i=0$，则定义 $0 \log 0 = 0$。</li>
</ul>
<h3 id="条件熵-Conditional-Entropy"><a href="#条件熵-Conditional-Entropy" class="headerlink" title="条件熵 (Conditional Entropy)"></a>条件熵 (Conditional Entropy)</h3><ul>
<li><strong>定义</strong>：表示在已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。</li>
<li><strong>计算公式</strong>：<script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)</script>  其中 $p_i = P(X=x_i)$。</li>
</ul>
<h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益 (Information Gain)"></a>信息增益 (Information Gain)</h3><ul>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差。<script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A)</script></li>
<li><strong>物理意义</strong>：由于特征 $A$ 而使得对数据集 $D$ 的分类不确定性减少的程度。<strong>信息增益大的特征具有更强的分类能力。</strong></li>
</ul>
<h3 id="信息增益比-Information-Gain-Ratio"><a href="#信息增益比-Information-Gain-Ratio" class="headerlink" title="信息增益比 (Information Gain Ratio)"></a>信息增益比 (Information Gain Ratio)</h3><ul>
<li><strong>背景</strong>：信息增益倾向于选择 <strong>取值较多</strong> 的特征（例如唯一的 ID 号），这往往不是我们想要的。</li>
<li><strong>定义</strong>：特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>  其中，<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>   $n$ 是特征 $A$ 取值的个数。</li>
</ul>
<hr>
<h2 id="3-经典算法"><a href="#3-经典算法" class="headerlink" title="3. 经典算法"></a>3. 经典算法</h2><h3 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="$ID3$ 算法"></a>$ID3$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益</code>  </li>
</ul>
<p><strong>具体计算流程</strong>：（设$C_1+C_2=D$）</p>
<ol>
<li><p><strong>计算不考虑特征的信息熵</strong>：</p>
<script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k= -\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}</script><p>其中 $|D|$ 为样本总数，$|C_k|$ 为属于第 $k$ 类的样本数(eg: <em>买/不买</em>)。</p>
</li>
<li><p><strong>计算条件熵</strong>：</p>
<ul>
<li>先选择一个 <strong>特征 $A_i$</strong>，将数据集 $D$ 根据特征 $A_i$ 的<strong>不同取值</strong>划分为 $n$ 个子集 $D_1, D_2, \dots, D_n$。</li>
<li>计算特征 $A_i$ 下不同取值 $D_i$ 的信息熵 $H(D_i)$：<script type="math/tex; mode=display">H(D_i)= -\sum_{k=1}^{K} \frac{|C_k|}{|D_i|} \log_2 \frac{|C_k|}{|D_i|}</script>其中，$C_k$ 是在集合 $D_i$ 里的第 $k$ 种类别的数量 (eg: <em>买/不买</em>)。</li>
<li>计算条件熵 $H(D|A_i)$：<script type="math/tex; mode=display">H(D|A_i) = \sum_{i=1}^{n} p_i H(D_i)= \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)</script></li>
</ul>
</li>
<li><p><strong>计算信息增益</strong>：</p>
<script type="math/tex; mode=display">g(D, A_i) = H(D) - H(D|A_i)</script></li>
<li><p><strong>特征选择</strong>：<br>选取 <strong>信息增益最大</strong> 的特征$A_i$作为当前的分裂节点（根节点或内部节点）。</p>
</li>
<li><p><strong>递归构建</strong>：<br>对子节点重复以上步骤，以此类推，直至分类完全（或满足停止条件）。</p>
</li>
</ol>
<h3 id="C4-5-算法"><a href="#C4-5-算法" class="headerlink" title="$C4.5$ 算法"></a>$C4.5$ 算法</h3><ul>
<li><strong>测试属性</strong>：<code>信息增益比</code> （除<strong>选择特征的标准不同</strong>外，其余步骤与 $ID3$ <strong>一致</strong>）。</li>
<li>计算完<strong>信息增益</strong>后，计算<strong>信息增益比</strong>：<script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)}</script>其中，<strong>特征固有值</strong>：<script type="math/tex; mode=display">H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}</script>这里 $|D|$ 是样本总数，注意和 $H(D_i)$ 的计算区分。</li>
</ul>
<h3 id="过拟合-Overfitting-1"><a href="#过拟合-Overfitting-1" class="headerlink" title="过拟合 (Overfitting)"></a>过拟合 (Overfitting)</h3><ul>
<li><strong>现象</strong>：决策树算法增长树的每一个分支的深度，直到恰好能对训练样例比较完美地分类。</li>
<li><strong>原因</strong>：当数据中 <strong>有噪声</strong> 或 <strong>训练样例数量太少</strong>，以至于不能产生目标函数的有代表性的采样时，简单算法产生的树会过拟合训练样例。</li>
</ul>
<hr>
<h2 id="4-CART-算法-Classification-And-Regression-Tree"><a href="#4-CART-算法-Classification-And-Regression-Tree" class="headerlink" title="4. $CART$算法 (Classification And Regression Tree)"></a>4. $CART$算法 (Classification And Regression Tree)</h2><h3 id="基尼指数-Gini-Index"><a href="#基尼指数-Gini-Index" class="headerlink" title="基尼指数 (Gini Index)"></a>基尼指数 (Gini Index)</h3><ul>
<li><strong>定义</strong>：度量数据纯度的指标。<strong>基尼指数越小，模型的不确定性越小，纯度越高</strong>。</li>
<li><p><strong>计算公式</strong>：<br>  假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$，则概率分布的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K} p_k^2</script><p>  对于给定的样本集合 $D$，其基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D) = 1 - \sum_{k=1}^{K} \left(\frac{|C_k|}{|D|}\right)^2</script><p>  其中 $C_k$ 是 $D$ 中属于第 $k$ 类的样本子集。</p>
</li>
<li><p><strong>特征下的基尼指数</strong>：<br>  如果特征 $A$ 的取值将集合 $D$ 分割成 $D_1$ 和 $D_2$ 两部分（<strong>注意：CART 生成的是二叉树</strong>），则在特征 $A$ 的条件下，集合 $D$ 的基尼指数为：</p>
<script type="math/tex; mode=display">Gini(D, A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)</script></li>
<li><p><strong>选择标准</strong>：选择使得 <strong>基尼指数最小</strong> 的特征及其划分点作为最优特征和最优切分点。</p>
</li>
</ul>
<h3 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h3><ul>
<li><strong>回归树</strong>：使用 <strong>平方误差最小化</strong> 准则。</li>
<li><strong>分类树</strong>：使用 <strong>基尼指数最小化</strong> 准则。</li>
<li><strong>具体生成流程 (分类树)</strong>：<ol>
<li>对训练数据集 $D$ 的每个特征 $A$，以及该特征的每个可能取值 $a$，根据 $A=a$ 与 $A \neq a$ 将 $D$ 分割为 $D_1$ 和 $D_2$。</li>
<li>计算该切分下的 $Gini(D, A)$。</li>
<li>在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择 <strong>基尼指数最小</strong> 的特征及其切分点作为最优特征与最优切分点。</li>
<li>生成两个子结点，将训练数据集依特征分配到两个子结点中。</li>
<li>对子结点递归地调用上述步骤，直到满足停止条件（如结点样本数小于阈值、基尼指数小于阈值或没有更多特征）。</li>
</ol>
</li>
</ul>
<h3 id="决策树剪枝-Pruning"><a href="#决策树剪枝-Pruning" class="headerlink" title="决策树剪枝 (Pruning)"></a>决策树剪枝 (Pruning)</h3><ul>
<li><strong>目的</strong>：防止过拟合。</li>
<li><strong>手段</strong>：剪枝，限定叶节点最小样本数，强制决策树最大深度。</li>
</ul>
<h4 id="预剪枝-Pre-Pruning"><a href="#预剪枝-Pre-Pruning" class="headerlink" title="预剪枝 (Pre-Pruning)"></a>预剪枝 (Pre-Pruning)</h4><ul>
<li>在决策树生成过程中，对每个结点在 <strong>划分前先进行估计</strong>。</li>
<li>若当前结点的划分不能带来决策树 <strong>泛化性能提升</strong>，则停止划分，并标记当前结点为叶结点。</li>
</ul>
<h4 id="后剪枝-Post-Pruning"><a href="#后剪枝-Post-Pruning" class="headerlink" title="后剪枝 (Post-Pruning)"></a>后剪枝 (Post-Pruning)</h4><ul>
<li>从训练集先生成一棵 <strong>完整的决策树</strong>。</li>
<li>自底而上地考察非叶结点，若将该结点对应的子树替换为叶结点能带来决策树 <strong>泛化性能提升</strong>，则 <strong>将该子树替换为叶结点</strong>。</li>
<li>CART 常用 <strong>CCP (Cost-Complexity Pruning, 代价复杂度剪枝)</strong> 方法。</li>
</ul>
<blockquote>
<p><strong>对比</strong>：一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能优于预剪枝决策树。但训练时间开销比未剪枝决策树和预剪枝决策树要大得多。</p>
</blockquote>
<hr>
<h2 id="5-随机森林-Random-Forest"><a href="#5-随机森林-Random-Forest" class="headerlink" title="5. 随机森林 (Random Forest)"></a>5. 随机森林 (Random Forest)</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>一种基于 <strong>Bagging</strong>（装袋法）的集成学习方法。</li>
<li>通过构建多棵决策树，并将它们的预测结果进行汇总（分类问题用 <strong>多数投票</strong>，回归问题用 <strong>平均值</strong>）来输出最终结果。</li>
<li><strong>“随机”的含义</strong>：体现在 <strong>样本选择的随机性</strong> 和 <strong>特征选择的随机性</strong>。</li>
</ul>
<h3 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h3><ol>
<li><p><strong>Bootstrap 采样 (样本随机)</strong>：</p>
<ul>
<li>对于 $N$ 个样本的训练集，有放回地随机抽取 $N$ 次，得到一个新的训练集。</li>
<li>未被抽到的样本称为 <strong>袋外数据 (Out-of-Bag, OOB)</strong>，可用于验证模型泛化能力。</li>
<li>重复上述步骤 $k$ 次，生成 $k$ 个独立的训练集。</li>
</ul>
</li>
<li><p><strong>特征随机选择 (特征随机)</strong>：</p>
<ul>
<li>在构建每棵树的每个分裂结点时，不是从所有 $M$ 个特征中选择最优特征，而是随机从 $M$ 个特征中选取 $m$ 个特征 ($m \ll M$) 作为一个子集。</li>
<li>从这 $m$ 个特征中选择最优的分裂属性。</li>
</ul>
</li>
<li><p><strong>生成决策树</strong>：</p>
<ul>
<li>利用上述采样和特征子集，完全生长决策树（通常 <strong>不剪枝</strong>）。</li>
</ul>
</li>
<li><p><strong>集成输出</strong>：</p>
<ul>
<li><strong>分类</strong>：所有树投票，票数最多的类别为最终结果。</li>
<li><strong>回归</strong>：所有树预测值的简单算术平均。</li>
</ul>
</li>
</ol>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>准确率高</strong>：在许多数据集上表现良好，抗过拟合能力强（因为引入了双重随机性）。</li>
<li><strong>并行处理</strong>：每棵树互不依赖，可以并行训练，速度快。</li>
<li><strong>高维数据友好</strong>：能够处理具有成千上万个特征的输入矩阵，而无需进行特征降维。</li>
<li><strong>自带评估</strong>：可以使用 OOB 数据进行内部评估，无需额外的验证集。</li>
</ul>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
  <entry>
    <title>专业前沿讲座报告</title>
    <url>/2025/11/26/lecture-report/</url>
    <content><![CDATA[<blockquote>
<p><strong>课程信息</strong></p>
<ul>
<li><strong>课程名称：</strong> 专业前沿讲座</li>
<li><strong>学期：</strong> 2025—2026学年第 1 学期</li>
<li><strong>作者：</strong> 宋文韬 (智机试验2305 / 23013361)</li>
</ul>
</blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>当前，人工智能与机器人技术正经历高速发展，多智能体协同、复杂系统优化等成为研究热点。文章结合《专业前沿讲座》课程内容及相关文献研读，梳理与分析了多智能体感知决策、大规模全局优化、工业互联网信息融合与安全以及机器学习在科学探索中的前沿应用。通过对这些技术在原理、实例及前沿研究上的综合分析，本报告总结了当前人工智能技术发展的内在规律与主流趋势，进一步形成了对未来研究方向与科研方法的初步认识与思考。</p>
<p><strong>关键词：</strong> 人工智能与机器人、多智能体、全局优化、工业互联网、机器学习</p>
<hr>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>当今世界，新一轮科技革命和产业变革正在深入发展，人工智能与机器人技术已成为国际竞争的新高地，也是推动经济社会发展的核心引擎。从自动驾驶车队的协同作业到智慧工厂的精准制造，从复杂系统的调度优化到基础科学领域的突破性发现，智能科学技术正在以前所未有的速度重塑着我们对世界的认知与改造能力。作为信息学院的学生，置身于这一时代的浪潮中，不仅需要掌握扎实的工程基础，更需要具备广阔的国际视野，深刻理解技术演进的内在逻辑与未来趋势。</p>
<p>本报告将结合课程讲授内容与相关前沿文献的研读体会，分为三个部分展开：首先，探讨从单体智能向多智能体协同跨越的技术逻辑，以及大规模全局优化算法在其中的核心作用；其次，分析工业互联网背景下，信息融合技术如何赋能实体经济，以及随之而来的信息安全挑战与伦理责任；最后，展望机器学习在科学探索与增进民生福祉中的革命性应用。通过对这些前沿技术的综合阐述，本报告旨在总结当前人工智能技术发展的规律，并结合个人思考，形成对未来科研方向与职业规范的初步认识与体会。</p>
<h2 id="2-群体智能与复杂系统的优化决策"><a href="#2-群体智能与复杂系统的优化决策" class="headerlink" title="2. 群体智能与复杂系统的优化决策"></a>2. 群体智能与复杂系统的优化决策</h2><h3 id="2-1-多智能体系统的协同感知与决策"><a href="#2-1-多智能体系统的协同感知与决策" class="headerlink" title="2.1 多智能体系统的协同感知与决策"></a>2.1 多智能体系统的协同感知与决策</h3><p>在面对动态、非结构化或大规模任务环境如无人机集群协同、智慧交通调度等时，单体智能往往受限于感知范围与执行能力。多智能体系统（MAS）通过多个智能体间的交互与协作，能够涌现出超越个体之和的群体智能。</p>
<h4 id="2-1-1-分布式感知与博弈决策"><a href="#2-1-1-分布式感知与博弈决策" class="headerlink" title="2.1.1 分布式感知与博弈决策"></a>2.1.1 分布式感知与博弈决策</h4><p>智能感知与决策是多智能体系统的核心要素。与单体系统不同，多智能体面临着环境的“非平稳性”挑战——即一个智能体的策略更新会改变其他智能体面临的环境状态。根据 Zhang 等人（2021）的研究 [1]，解决这一问题的关键在于引入博弈论框架与多智能体强化学习（MARL）算法。通过建立纳什均衡（Nash Equilibrium）或相关均衡机制，智能体能够在竞争或合作中找到最优策略组合。</p>
<img src="/2025/11/26/lecture-report/1.png" class="" title="图 1 从单体决策到多智能体博弈的模型演变 [1]">
<blockquote>
<p><strong>图 1 说明：</strong> (a) 单体智能体面临的马尔可夫决策过程 (MDP)；(b) 多智能体系统中的马尔可夫博弈 (Markov Game) 模型，展示了多个智能体在共享环境中通过联合动作与环境交互并获取反馈的过程。这种从“中心化控制”向“分布式协同决策”的转变，是当前无人系统发展的必然趋势，也体现了智能时代的核心技术特征。</p>
</blockquote>
<h4 id="2-1-2-协同控制的实现路径"><a href="#2-1-2-协同控制的实现路径" class="headerlink" title="2.1.2 协同控制的实现路径"></a>2.1.2 协同控制的实现路径</h4><p>在具体的工程实践中，多智能体协同不仅需要算法支撑，还依赖于高效的通信拓扑与共识机制。通过对“智能感知与决策关键技术”的学习，我认识到，如何在大规模群体中实现低延迟的信息交互，并基于局部信息达成全局一致性，是目前工业界与学术界共同关注的热点。</p>
<h3 id="2-2-基于分解的大规模全局优化策略"><a href="#2-2-基于分解的大规模全局优化策略" class="headerlink" title="2.2 基于分解的大规模全局优化策略"></a>2.2 基于分解的大规模全局优化策略</h3><p>随着智能体数量的增加和系统精度的提升，优化问题的决策变量往往会膨胀至数千甚至上万维。课程第二章重点讲授了“基于分解的大规模全局优化新思考”，这为解决高维复杂系统的“维数灾难”提供了核心方法论。</p>
<h4 id="2-2-1-分解策略"><a href="#2-2-1-分解策略" class="headerlink" title="2.2.1 分解策略"></a>2.2.1 分解策略</h4><p>面对大规模全局优化（LSGO）问题，传统的进化算法往往因搜索空间过大而陷入局部最优。解决这一难题的关键在于“分解”。协同进化算法（CCEA）是处理此类问题的经典范式 [2]。其核心思想是将一个高维的大规模优化问题分解为若干个低维的子问题（Sub-components），针对每个子问题分别进行进化求解，最后协同组合成全局最优解。</p>
<h4 id="2-2-2-工程与科学思维的统一"><a href="#2-2-2-工程与科学思维的统一" class="headerlink" title="2.2.2 工程与科学思维的统一"></a>2.2.2 工程与科学思维的统一</h4><p>通过对“基于分解的大规模全局优化应用实例”的学习，我深刻体会到，优化不仅是数学计算，更是一种系统工程思维。无论是在工业制造的过程控制中，还是在物流网络的路径规划中，利用分解策略将复杂系统模块化、层次化，是提升系统运行效率、降低计算代价的根本途径。这不仅锻炼了我们解决复杂工程问题的能力，也加深了对“工程素养”这一思政要点的理解。</p>
<h2 id="3-工业互联背景下的信息融合与安全防线"><a href="#3-工业互联背景下的信息融合与安全防线" class="headerlink" title="3. 工业互联背景下的信息融合与安全防线"></a>3. 工业互联背景下的信息融合与安全防线</h2><h3 id="3-1-多源异构信息的深度融合"><a href="#3-1-多源异构信息的深度融合" class="headerlink" title="3.1 多源异构信息的深度融合"></a>3.1 多源异构信息的深度融合</h3><p>课程讲授中提到，大数据的处理分析与挖掘是理解智能时代的关键。通过学习“信息融合简介及应用”，我理解到信息融合不仅仅是数据的简单叠加，而是对多源信息进行多级别、多层次的处理。例如，在高端装备的运维中，需要将物理量与文本信息进行融合。这种融合往往需要结合“过程机理”与“大数据建模”。即利用深度学习算法挖掘数据特征的同时，必须引入物理机理模型作为约束，才能在工业场景下实现精准的状态监测与故障诊断。这不仅提升了系统的感知精度，也为辅助决策提供了可靠依据。</p>
<h3 id="3-2-工业控制系统的安全策略"><a href="#3-2-工业控制系统的安全策略" class="headerlink" title="3.2 工业控制系统的安全策略"></a>3.2 工业控制系统的安全策略</h3><p>随着信息技术与运营技术的深度融合，封闭的工业控制系统逐渐开放，随之而来的是暴露面的扩大和安全风险的剧增。课程强调了“信息安全算法”与“科学伦理”的重要性，这引发了我对工程师社会责任的深层思考。</p>
<h4 id="3-2-1-工业安全的特殊性和严峻挑战"><a href="#3-2-1-工业安全的特殊性和严峻挑战" class="headerlink" title="3.2.1 工业安全的特殊性和严峻挑战"></a>3.2.1 工业安全的特殊性和严峻挑战</h4><p>与传统互联网安全仅关注数据隐私不同，工业互联网安全直接关系到物理世界的安全。根据 Sadeghi 等人（2020）在《IEEE工业电子汇刊》上的分析，工业物联网（IIoT）面临着特有的安全与隐私挑战 [3]。一次针对化工控制系统的恶意攻击，不仅会导致商业机密的泄露，更可能引发设备损坏、环境污染甚至人员伤亡。因此，在设计智能系统时，我们必须遵循“Security by Design”（设计内建安全）的原则。不仅要研究异常检测与加密算法等技术防御手段，更要从系统架构层面考虑功能的安全性。</p>
<h4 id="3-2-2-工程伦理与社会责任"><a href="#3-2-2-工程伦理与社会责任" class="headerlink" title="3.2.2 工程伦理与社会责任"></a>3.2.2 工程伦理与社会责任</h4><p>我们不仅要追求算法的效率，更要时刻关注工程实践对公众安全、健康及环境的影响。在构建工业互联网平台时，必须自觉履行保护数据隐私、维护国家基础设施安全的社会责任，将科学伦理融入技术研发的每一个环节。这部分的学习使我对“职业规范”有了更具象的理解。</p>
<h2 id="4-从经典机器学习算法到科技前沿"><a href="#4-从经典机器学习算法到科技前沿" class="headerlink" title="4. 从经典机器学习算法到科技前沿"></a>4. 从经典机器学习算法到科技前沿</h2><h3 id="4-1-经典机器学习模型的科学适用性"><a href="#4-1-经典机器学习模型的科学适用性" class="headerlink" title="4.1 经典机器学习模型的科学适用性"></a>4.1 经典机器学习模型的科学适用性</h3><p>在本课程的学习过程中，我经历了一个由浅入深、从经典统计模型向深度神经网络进阶的认知过程。</p>
<p>最初，我从基于统计学习理论的支持向量机（SVM）入手，理解了如何通过数学变换处理小样本数据；随后，接触了以随机森林为代表的集成学习算法，领悟了通过“群体决策”降低模型方差的智慧；最后，探究了以卷积神经网络（CNN）为核心的深度学习架构，掌握了处理高维非结构化数据的关键技术。这一学习路径不仅夯实了我的算法基础，也让我深刻认识到不同模型在科学探索中独特的适用场景。</p>
<h4 id="4-1-1-支持向量机-SVM"><a href="#4-1-1-支持向量机-SVM" class="headerlink" title="4.1.1 支持向量机 (SVM)"></a>4.1.1 支持向量机 (SVM)</h4><p>作为基于统计学习理论的经典算法，SVM 在处理小样本、高维度的科学实验数据时表现出色。在课程讨论中，我们了解到在某些材料分类或生物标记物检测的场景下，由于实验数据获取昂贵且稀缺，深度学习难以收敛，而SVM 通过核函数将数据映射到高维空间寻找最优超平面的能力，使其依然具有不可替代的价值 [4]。</p>
<img src="/2025/11/26/lecture-report/2.png" class="" title="图 2 Kernel Function: 将低维数据映射到高维空间">
<h4 id="4-1-2-集成学习-Ensemble-Learning"><a href="#4-1-2-集成学习-Ensemble-Learning" class="headerlink" title="4.1.2 集成学习 (Ensemble Learning)"></a>4.1.2 集成学习 (Ensemble Learning)</h4><p>以随机森林（Random Forest）为代表的集成算法，通过构建多棵决策树来降低模型的方差。在涉及多变量耦合的复杂物理实验数据分析中，这类算法不仅能提供较高的预测精度，还能输出“特征重要性”排序 [5]。这种可解释性对于科学家理解哪些物理量对实验结果影响最大至关重要。</p>
<h4 id="4-1-3-深度神经网络-Deep-Learning"><a href="#4-1-3-深度神经网络-Deep-Learning" class="headerlink" title="4.1.3 深度神经网络 (Deep Learning)"></a>4.1.3 深度神经网络 (Deep Learning)</h4><p>从经典的感知机发展而来的深度学习算法是本课程课外自主学习的重点。特别是卷积神经网络（CNN），作为当前处理网格结构数据（如天文望远镜拍摄的星系图、电子显微镜下的材料微观结构）的主流架构，展现了强大的特征提取能力 [6]。该算法突破了传统方法的局限，能够自动从原始像素中学习到从边缘到纹理再到语义的高层特征，实现了端到端的智能感知。</p>
<img src="/2025/11/26/lecture-report/3.png" class="" title="图 3 深度卷积神经网络的典型架构">
<blockquote>
<p><strong>图 3 说明：</strong> 该图展示了网络处理图像的完整流程：输入图像经过多个卷积层（Convolutions）与池化层（Pooling）的交替处理，逐层提取从低级几何特征到高级语义概念的层级化表示，最终实现精准分类。</p>
</blockquote>
<h3 id="4-2-从传统科学到新的科学探索"><a href="#4-2-从传统科学到新的科学探索" class="headerlink" title="4.2 从传统科学到新的科学探索"></a>4.2 从传统科学到新的科学探索</h3><p>传统的科学研究模式往往依赖于“观察—假设—实验”的线性循环，但在面对高维非线性问题时，人类的认知直觉与传统计算模拟（如第一性原理计算）的效率均遭遇了显著瓶颈。</p>
<p>根据相关前沿学术综述研究 [7]，深度学习模型通过学习海量科学数据中的高维分布特征，能够构建出逼近自然界物理定律的代理模型（Surrogate Models）。这一过程并非简单的数据插值与拟合，而是利用神经网络强大的泛化能力，在高维参数空间中寻找隐藏的科学规律。例如，生成式人工智能（Generative AI）目前已能自动生成符合特定物理化学约束的新分子结构，这种技术成功实现了“假设生成—验证”闭环的自动化，将科学发现的周期大大缩短，确立了全新的科学探索范式。</p>
<img src="/2025/11/26/lecture-report/4.png" class="" title="图 4 人工智能驱动的科学发现闭环与应用版图">
<blockquote>
<p><strong>图 4 说明：</strong> 该图展示了AI for Science的核心范式：通过加速“观察—假设—实验”的迭代闭环，人工智能打破了学科壁垒，被广泛应用于天气预报、电池材料优化、核聚变控制及生物医疗序列建模等前沿领域，确立了区别于传统实验与理论计算的第四种科学范式。</p>
</blockquote>
<h3 id="4-3-机器学习应用趋势"><a href="#4-3-机器学习应用趋势" class="headerlink" title="4.3 机器学习应用趋势"></a>4.3 机器学习应用趋势</h3><p>结合当前的社会现实，我深刻感受到，人类的日常生活以及离不开智能技术的辅助。未来的智能技术将不仅仅追求算力的突破，更将致力于提升人类的生活质量，实现真正的“科技向善”。目前，机器学习已经渗透进人类生活的方方面面：</p>
<ul>
<li><strong>个性化推荐系统：</strong> 当我们打开购物软件或短视频平台时，背后的推荐算法（Recommendation Systems）正在利用机器学习技术分析我们的历史行为与偏好。它帮助我们在海量信息中快速筛选出感兴趣的内容，极大地降低了信息检索的成本，提升了生活效率。</li>
<li><strong>智能语音与交互：</strong> 从手机里的智能助手（如 Siri、小爱同学）到智能家居系统，自然语言处理（NLP）技术让机器能够“听懂”人类的指令。未来的智能家居将不再是简单的遥控，而是通过感知用户的习惯自动调节环境（如温度、灯光），提供更加人性化的居住体验。</li>
<li><strong>自动驾驶辅助：</strong> 在出行领域，计算机视觉技术正在让汽车变得更加智能。目前的 L2+ 级辅助驾驶功能已经能够实现车道保持、自适应巡航，有效降低了驾驶疲劳与交通事故的发生率。</li>
<li><strong>生成式协作：</strong> 同时，以 ChatGPT 为代表的生成式大模型（Generative AI）的兴起，标志着人工智能从“感知”走向了“生成”。在我们的日常学习与工作中，AI 正在成为最得力的工具。它不仅能帮助我们快速梳理文献、润色邮件、编写代码，还能在创意枯竭时提供灵感。这种人机协作的模式并没有取代人类，而是赋予了我们更强大的信息处理能力与创造力，让我们能够将精力集中在更具价值的逻辑思考与决策上。</li>
</ul>
<h2 id="5-结语与体会"><a href="#5-结语与体会" class="headerlink" title="5. 结语与体会"></a>5. 结语与体会</h2><p>通过本学期《专业前沿讲座》的学习，我完成了从人工智能基础概念到机器人工程前沿应用，再到机器学习赋能科学发现的知识图谱构建。课程讲授的多智能体协同、大规模全局优化、工业互联网信息安全以及机器学习科学应用等模块，并非孤立的技术点，而是构成智能系统的有机整体。</p>
<p>我深刻体会到，现代智能科学正朝着“群体化、复杂化、落地化”演进：多智能体解决了感知广度问题，分解优化策略突破了计算维度限制，而工业互联网与信息安全则保障了技术的稳健落地。这种从系统工程高度审视技术脉络的宏观视角，不仅极大增强了我的专业学习兴趣，更让我对未来的科研方向与职业规范有了清晰认识。</p>
<p>我认识到，单纯的算法优化已遭遇边际效应递减，真正的创新往往源于“AI+X”的跨学科交叉融合。无论是利用深度学习加速新材料研发，还是解决复杂的物流调度，都要求我们具备开阔的国际视野，打破学科壁垒，将智能技术作为解决国计民生核心难题的通用工具。同时，技术是把双刃剑，强大的算力必须配合正确的价值观。面对全球老龄化等严峻挑战，融合机器学习的智慧医疗与养老系统虽能解决社会痛点，但其底线必须建立在安全可控之上。作为未来的工程师，我们既要利用先进算法实现精准筛查与情感陪伴，更要时刻警惕隐私泄露与算法歧视等风险，坚定推动“负责任的人工智能”建设，确保技术在严守伦理规范的前提下真正赋能人类的美好生活，让科技的光辉温暖社会的每一个角落。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” <em>IEEE Transactions on Automatic Control</em>, vol. 66, no. [cite_start]12, pp. 5690–5708, 2021. [cite: 56, 57]</li>
<li>X. Li, K. Tang, M. N. Omidvar, Z. Yang, and K. Qin, “A survey on cooperative co-evolutionary algorithms,” <em>IEEE Transactions on Evolutionary Computation</em>, vol. 17, no. [cite_start]4, pp. 471–496, 2013. [cite: 58, 59]</li>
<li>A.-R. Sadeghi, C. Wachsmann, and M. Waidner, “Security and privacy in industrial internet of things: Current status and potential future challenges,” <em>ACM Transactions on Embedded Computing Systems</em>, vol. 14, no. [cite_start]4, pp. 76:1–76:27, 2015. [cite: 60, 61]</li>
<li>C. Cortes and V. Vapnik, “Support-vector networks,” <em>Machine Learning</em>, vol. 20, no. [cite_start]3, pp. 273–297, 1995. [cite: 62]</li>
<li>G. Biau, “Analysis of a random forests model,” <em>Journal of Machine Learning Research</em>, vol. [cite_start]13, pp. 1063–1095, 2012. [cite: 63]</li>
<li>Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em>Nature</em>, vol. 521, no. [cite_start]7553, pp. 436–444, 2015. [cite: 64]</li>
<li>H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac, A. Anandkumar, K. Bergen, C. P. Gomes, S. Ho, P. Kohli, J. Lasenby, J. Leskovec, T. Liu, A. Manrai, D. Marks, B. Ramsundar, L. Song, J. Sun, J. Tang, P. Veličković, M. Zitnik, and M. Zitnik, “Scientific discovery in the age of artificial intelligence,” <em>Nature</em>, vol. 620, no. [cite_start]7972, pp. 47–60, 2023. [cite: 65, 66]</li>
</ol>
<blockquote>
<p><strong>附件下载：</strong> <a href="report.docx">📄 点击下载完整 Word 课程报告</a></p>
</blockquote>
]]></content>
      <categories>
        <category>华理</category>
        <category>大三上</category>
      </categories>
  </entry>
</search>
